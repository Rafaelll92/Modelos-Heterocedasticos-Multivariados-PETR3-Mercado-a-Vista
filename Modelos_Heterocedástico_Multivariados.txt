########################/                     \#################################    
#                                                                              #
#                                                                              #
#     *Rentabilidade passada não conresponde a rentabilidades fututuras        #
#     *As estimações nesse trabalho podem diferir bastante com a realidade     #
#     *Para propósitos educacionais, apenas                                    #
#                                                                              #
#                                                                              #
########################/                     \#################################



#Chamando as bibliotecas

from torch.autograd import Variable
import torch.nn.functional as F
from datetime import datetime
import math
import numbers
import time
import torch
import sklearn
import torch.nn as nn
from functools import reduce
import seaborn as sns
import numpy as np
import pystan
from numpy import cumsum, log, polyfit, sqrt, std, subtract
from numpy.random import randn
import matplotlib.pyplot as plt
import statsmodels.api as sm
from torch.optim.optimizer import Optimizer, required
import copy
from sklearn.metrics import mean_absolute_error
from sklearn import preprocessing
from scipy.stats import ks_2samp
import os
import sys
from scipy import signal
from scipy.optimize import minimize
import psutil
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from tabulate import tabulate
from statsmodels.tsa.stattools import adfuller
from scipy import stats
from scipy.stats import kstest
from statsmodels.stats.diagnostic  import lilliefors
from scipy.stats import shapiro
from scipy.stats import normaltest
from torch.nn.modules.utils import _pair
import datetime
import pandas as pd 
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from torchviz import make_dot
from sklearn.linear_model import LinearRegression
from torch.optim.optimizer import Optimizer, required
import copy
from scipy.stats import norm
import sys
from torch.nn.modules.utils import _pair
import librosa
import librosa.display
from distfit import distfit
from statsmodels.tsa.stattools import adfuller, kpss


#Parte 1 - Estatísticas Descritivas 


PETR3_SA = pd.read_csv(r"C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3.SA.csv", parse_dates=['Date'])
PETR3_SA.index = PETR3_SA["Date"]
PETR3_SA_Close = PETR3_SA.loc[:,[ 'Close']]
PETR3_SA_Volume = PETR3_SA.loc[:,[ 'Volume']]
start_date = pd.to_datetime(min(PETR3_SA.index))
end_date = pd.to_datetime(max(PETR3_SA.index))                   
PETR3_SA["Date"] = pd.to_datetime(PETR3_SA.index) 
new_df = (PETR3_SA['Date']>= start_date) & (PETR3_SA['Date']<= end_date)
df1 = PETR3_SA.loc[new_df]
stock_data = df1.set_index('Date')
top_plt = plt.subplot2grid((5,4), (0, 0), rowspan=3, colspan=4)
top_plt.plot(stock_data.index, stock_data["Close"])
plt.title('Dados Históricos Petrobras Ações Ordinárias - PETR3_SA 2020-10-07 00:00:00 até 2023-10-06 00:00:00')
bottom_plt = plt.subplot2grid((5,4), (3,0), rowspan=1, colspan=4)
bottom_plt.bar(stock_data.index, stock_data['Volume'])
plt.title('PETR3_SA . Volume', y=-0.60)
plt.gcf().set_size_inches(1,8)
plt.show()

#Figura1.1a

######################/                                                       \##############################
#   Muitos indicadores macroeconômicos estão atrelados ao crescimento populacional, que é exponencial,
#   e, portanto, têm uma tendência exponencial. Portanto, o processo antes da modelagem com
#   métodos Lineares geralmente é:
#
#   Cole, T. J., & Altman, D. G. (2017). 
#   Statistics Notes: Percentage differences, symmetry, and natural logarithms. BMJ, 358(August), j3683. 
#   https://doi.org/20.1136/bmj.j3683
#
#
#   *   Obter logaritímicos para obter uma série com uma tendência Linear
#   *   Obter a diferença para obter uma série estacionária 
#
######################/                                                       \##############################


##################################################################################################
#      Diferenciação fracionária para remover a tendência Linear enquanto preserva  memória 
#                    - Tentar obter periodograma com autocoreelação com decaímento senoidal-
#
#      [DE PRADO, 2017]  M.L de Prado (2017) Advances in Financial Machine Learning.
#      1st, Edition. Wiley, 2017.
#
##################################################################################################


def getWeights(d,lags):
     # return the weights from the series expansion of the differencing operator
     # for real orders d and up to lags coefficients
     w=[1]
     for k in range(1,lags):
         w.append(-w[-1]*((d-k+1))/k)
     w=np.array(w).reshape(-1,1)
     return w
	 

def plotWeights(dRange, lags, numberPlots):
     weights=pd.DataFrame(np.zeros((lags, numberPlots)))
     interval=np.linspace(dRange[0],dRange[1],numberPlots)
     for i, diff_order in enumerate(interval):
         weights[i]=getWeights(diff_order,lags)
     weights.columns = [round(x,2) for x in interval]
     fig=weights.plot(figsize=(15,6))
     plt.legend(title='Order of differencing')
     plt.title('Lag coefficients for various orders of differencing')
     plt.xlabel('lag coefficients')
     plt.grid(False)
     plt.show()
	 

def cutoff_find(order,cutoff,start_lags): #order is our dearest d, cutoff is 1e-5 for us, and start lags is an initial amount of lags in which the loop will start, this can be set to high values in order to speed up the function
     val=np.inf
     lags=start_lags
     while abs(val)>cutoff:
         w=getWeights(order, lags)
         val=w[len(w)-1]
         lags+=1
     return lags
	 

def ts_differencing_tau(series, order, tau):
     # return the time series resulting from (fractional) differencing
     lag_cutoff=(cutoff_find(order,tau,1)) #finding lag cutoff with tau
     weights=getWeights(order, lag_cutoff)
     res=0
     for k in range(lag_cutoff):
         res += weights[k]*series.shift(k).fillna(0)
     return res[lag_cutoff:]


###########################################################################################
# Séries mariores podem permitir um limite menor para preservar memória no periodograma.  #
# De maneira geral, quanto maior o limite de diferenciação "d", mais curta a série fica.  #
# No caso o limite, aqui denominado de "tau, foi de 1e-2                                  #
###########################################################################################

possible_d=np.divide(range(1,200),200)
tau=1e-2
log_adf_stat_holder=[None]*len(possible_d)

for i in range(len(possible_d)):
    log_adf_stat_holder[i]=adfuller(ts_differencing_tau(np.log(pd.DataFrame(PETR3_SA_Close)) ,possible_d[i],tau))[1]

#Plotando o auto-correlograma para as defasagens "d" 

plt.plot(possible_d,log_adf_stat_holder)
plt.axhline(y=0.000000000001,color='r')
plt.ylabel('P-valor para teste ADF por ordem de diferenciação na série logarítmica de fechamento PETR3_SA 1 Dia')
plt.show()

#Figura1.1b

d=1.00
PETR3_SA_Close_fractional_difference  = (ts_differencing_tau(np.log(PETR3_SA_Close), d,tau )) 
PETR3_SA_Close_fractional_difference = PETR3_SA_Close_fractional_difference.fillna(method='ffill')  
pd.DataFrame(PETR3_SA_Close_fractional_difference).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_fractional_difference.csv', index=None)
PETR3_SA_Close_fractional_difference = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_fractional_difference.csv")

def nan_helper(y):
   """
      Função para limpar valores com NaNs.
   Inpute:
      - y, vetor em 1d  com possivéis NaNs 
   Output:
      - NaNs.Valores com NaNs.
      - Função com índices = index(NaNs), 
	    para converter índicies de NaNs para valores interpolados
   Exemplo:
      # interpolação linaer de  NaNs
      nans, x= nan_helper(y)
      y[nans]= np.interp(x(nans), x(~nans), y[~nans])
   """
   return np.isnan(y), lambda z: z.nonzero()[0]

nans, x= nan_helper(PETR3_SA_Close_fractional_difference.values)
PETR3_SA_Close_fractional_difference.values[nans]= np.interp(x(nans), x(~nans), PETR3_SA_Close_fractional_difference.values[~nans])
check_nan_in_df = PETR3_SA_Close_fractional_difference.isnull().values.any()
print (check_nan_in_df)
#False


###############################
#                             #                           
#   Filtro de Kalman Linear   #
#                             #                         
###############################


def matrix_to_symmetric_matrix(x):
  return 0.5 * (x + np.transpose(x))

Linear_Kalman_Filter = """
/*
estimado com :
  - Filtro de Covarância ( sem raíz quadrada ou processamento sequencial
  - paramêtros invariantes ao tempo
  - valores inicias , onde as y observações são supostas serem normalmente distribuidas

*/
data {
    int<lower=1>  n; // Numero de observações na Amostragem 
    int p; // numero de variavéis observadas 
    int m; // numero de estados 
    int r; // tamanho das equações dos erros; r >= m; R: m x r, Q: r x r
    real dt;       // intervalo de tempo.A defasagem "d" definida na etapa anterior
    vector[p] y[n]; // As observações em si
    matrix[p, m] Z; // Equação das Observações 
    // equação dos sistemas
    matrix[m, m] Ac; // coeficientes
    matrix[m, r] R; //  erros do sistema  
    // valores iniciais para os estados iniciais da média x e covariância P
    vector[m] x_1;
    cov_matrix[m] P_1; 
}

parameters {
  // mensuração dos erros 
  vector<lower=0.0>[p] h;
  vector<lower=0.0>[r] q;
}

transformed parameters {
  matrix[p, p] H;
  matrix[r, r] Q;
  vector[m] x[n+1];
  matrix[m,m] P[n+1];
  matrix[m, m] A;
  real llik_obs[n];
  real lp=0;
  // Ruído de covariância 
  H = diag_matrix(h);
  Q = diag_matrix(q);
  //matrix[m, m] A;
  // Tempo continuo para discreto
  //A = matrix_exp(Ac*dt);
  A = (Ac*dt);
  for (i in 1:n) {
    vector[p] v;
    matrix[p, p] F;
    matrix[p, p] Finv;
    matrix[m, p] K; 
    matrix[m, m] I = diag_matrix(rep_vector(1, m));  
    // Valores iniciais 
    x[1] = x_1;  //     Valores para os estados da média
    P[1] = P_1;  //     Valores para a matriz dos estados da covariância
    //valores iniciais declarados ( a priori) //
    x[i] = A*x[i];  //  prever os estados que mensuram a média
    P[i] = (A*P[i])*A' + (R*Q)*R';    // prever os estados que mensuram a matriz de covariância 
    // mensurações  //
    v = y[i] - Z * x[i];      // Inovação Innovation ou residuos  de pré-ajuste de medição
    F = (Z  * P[i]) * Z' + H; // covariância de saída
    Finv = inverse(F);
    K = (P[i] * Z' )* Finv;   // Ganho de Kalman 
    /* Atuaização do sistema em si  (posterior) -  Predição  */ 
    P[i+1] = (I-K*Z)*P[i]*(I - K*Z)' + K*H*K';   // atualizar a matriz de estados da  covariância com o Ganho de Kalman 
    x[i+1] = x[i] + K * v; //  atualizar os estados de média  com o Ganho de Kalman 
    // Verossimelhança 
    llik_obs[i] = -0.5 * (1 * log(2 * pi()) + log(determinant(F)) + v' * Finv * v);
    //llik_obs[i] = -( multi_normal_lpdf(y[i]| Z * x[i], F));                    
    }
    lp += lp + sum(llik_obs);
}              
"""

Linear_Kalman_Filter = pystan.StanModel(model_code= Linear_Kalman_Filter)

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min  

data_Linear_Kalman_Filter = ({
         'n' : len(PETR3_SA_Volume),
         'p' : 1,
         'm' : 2,
         'r' : 2,
         'dt' : 1,
         'y': pd.DataFrame(PETR3_SA_Volume).values,
         'Z' : np.matrix('1 0'),
         'Ac' :  np.matrix('0 1; 1 0'), 
         'R' : np.matrix('1 0; 0 1'),
         'x_1' : np.asarray([[pd.DataFrame(PETR3_SA_Volume).iloc[0],pd.DataFrame(PETR3_SA_Volume).iloc[0]]]).ravel(),
         'P_1' : matrix_to_symmetric_matrix(np.matrix('1 0; 0 1'))
         })

control = {}
control['max_treedepth'] = 1
control['adapt_delta'] = 1

fit_Linear_Kalman_Filter = Linear_Kalman_Filter.sampling(data=data_Linear_Kalman_Filter, iter=3500,  chains=3, warmup=1200 , thin=1, seed=101, n_jobs = -1,  control=control)

parameters_Linear_Kalman_Filter = fit_Linear_Kalman_Filter.extract(permuted=True)
parameters_Linear_Kalman_Filter.keys()

PETR3_SA_Volume_Linear_filtred = parameters_Linear_Kalman_Filter['x']
PETR3_SA_Volume_Linear_filtred = PETR3_SA_Volume_Linear_filtred.mean(axis=0)
PETR3_SA_Volume_Linear_filtred = ((PETR3_SA_Volume_Linear_filtred[:,0] + PETR3_SA_Volume_Linear_filtred[:,1])/2)

pd.DataFrame(PETR3_SA_Volume_Linear_filtred).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Volume_Linear_filtred.csv', index=None)
PETR3_SA_Volume_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Volume_Linear_filtred.csv")

plt.plot(PETR3_SA_Volume_Linear_filtred)
plt.legend(['Petrobrás Ações Ordinárias % D1 , Volume Filtrado de PETR3_SA de 2020-10-07 00:00:00 até 2023-10-06 00:00:00'])
plt.show()

#Figura1.1c


def nan_helper(y):
   """
      Função para limpar valores com NaNs.
   Inpute:
      - y, vetor em 1d  com possivéis NaNs 
   Output:
      - NaNs.Valores com NaNs.
      - Função com índices = index(NaNs), 
	    para converter índicies de NaNs para valores interpolados
   Exemplo:
      # interpolação linaer de  NaNs
      nans, x= nan_helper(y)
      y[nans]= np.interp(x(nans), x(~nans), y[~nans])
   """
   return np.isnan(y), lambda z: z.nonzero()[0]


nans, x= nan_helper(PETR3_SA_Volume_Linear_filtred.values)
PETR3_SA_Volume_Linear_filtred.values[nans]= np.interp(x(nans), x(~nans), PETR3_SA_Volume_Linear_filtred.values[~nans])
check_nan_in_df = PETR3_SA_Volume_Linear_filtred.isnull().values.any()
print (check_nan_in_df)
#False

sns.distplot(PETR3_SA_Volume_Linear_filtred, hist=True, kde=True, rug=False)
plt.legend(labels=['Petrobrás Ações Ordinárias % D1 , \Histograma para o Volume Filtrado de PETR3_SA de 2020-10-07 00:00:00 até 2023-10-06 00:00:00 '])
plt.show()

# Instanciar o objeto, a função,  distfit
dist = distfit()

# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(PETR3_SA_Volume_Linear_filtred.values)

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name score                loc  ... bootstrap_score bootstrap_pass    color
0         beta   0.0     7017548.852388  ...               0           None  #e41a1c
1     dweibull   0.0    16625448.062599  ...               0           None  #e41a1c
2         norm   0.0    18678620.398915  ...               0           None  #377eb8
3     loggamma   0.0 -2169964493.336449  ...               0           None  #4daf4a
4        expon   0.0     7138599.637751  ...               0           None  #984ea3
5            t   0.0      -40195.391671  ...               0           None  #ff7f00
6      uniform   0.0     7138599.637751  ...               0           None  #ffff33
7       pareto   0.0      -17145.969959  ...               0           None  #a65628
8      lognorm   0.0     7138599.637751  ...               0           None  #f781bf
9   genextreme   0.0     7138600.080917  ...               0           None  #999999
10       gamma   0.0   110591150.911431  ...               0           None  #999999

# Plot resultados
dist.plot()
plt.show()

#Figura1.1d


for i in range(len(PETR3_SA_Volume_Linear_filtred) - len(PETR3_SA_Close_fractional_difference))  :
         data_Linear_Kalman_Filter = ({
          'n' : len(PETR3_SA_Close_fractional_difference),
          'p' : 1,
          'm' : 2,    
          'r' : 2,
          'dt' : 1,
          'y':  pd.DataFrame(PETR3_SA_Close_fractional_difference),
          'Z' : np.matrix('1 0'),
          'Ac' :  np.matrix('0 1; 1 0'),
          'R' : np.matrix('1 0; 0 1'),
          'x_1' : np.asarray([[pd.DataFrame(PETR3_SA_Close_fractional_difference).iloc[0],pd.DataFrame(PETR3_SA_Close_fractional_difference).iloc[0]]]).ravel(),
          'P_1' : matrix_to_symmetric_matrix(np.matrix('1 0; 0 1'))
          })
         fit_Linear_Kalman_Filter = Linear_Kalman_Filter.sampling(data=data_Linear_Kalman_Filter, iter=3500,  chains=3, warmup=1200 , thin=1, seed=101, n_jobs = -1,  control=control)
         parameters_Linear_Kalman_Filter = fit_Linear_Kalman_Filter.extract(permuted=True)
         PETR3_SA_Close_fractional_difference_Linear_filtred = parameters_Linear_Kalman_Filter['x'].mean(axis=0)
         PETR3_SA_Close_fractional_difference = np.append(PETR3_SA_Close_fractional_difference, PETR3_SA_Close_fractional_difference_Linear_filtred[-1:])
         PETR3_SA_Close_fractional_difference = pd.DataFrame(PETR3_SA_Close_fractional_difference)
		 
		 
pd.DataFrame(pd.DataFrame(PETR3_SA_Close_fractional_difference)).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_fractional_difference_Linear_filtred.csv', index=None)
PETR3_SA_Close_fractional_difference_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_fractional_difference_Linear_filtred.csv")
PETR3_SA_Close_fractional_difference_Linear_filtred = PETR3_SA_Close_fractional_difference_Linear_filtred[-len(PETR3_SA_Volume_Linear_filtred):]
sns.distplot(PETR3_SA_Close_fractional_difference_Linear_filtred, hist=True, kde=True, rug=False)
plt.legend(labels=['Petrobrás Ações Ordinárias % D1 , \Histograma para o Preço de Fechamento Filtrado de PETR3_SA de PETR3_SA de 2020-10-07 00:00:00 até 2023-10-06 00:00:00  '])
plt.show() 

#Figura1.1e

def nan_helper(y):
   """
      Função para limpar valores com NaNs.
   Inpute:
      - y, vetor em 1d  com possivéis NaNs 
   Output:
      - NaNs.Valores com NaNs.
      - Função com índices = index(NaNs), 
	    para converter índicies de NaNs para valores interpolados
   Exemplo:
      # interpolação linaer de  NaNs
      nans, x= nan_helper(y)
      y[nans]= np.interp(x(nans), x(~nans), y[~nans])
   """
   return np.isnan(y), lambda z: z.nonzero()[0]
   

nans, x= nan_helper(PETR3_SA_Close_fractional_difference_Linear_filtred.values)
PETR3_SA_Close_fractional_difference_Linear_filtred.values[nans]= np.interp(x(nans), x(~nans), PETR3_SA_Close_fractional_difference_Linear_filtred.values[~nans])
check_nan_in_df = PETR3_SA_Close_fractional_difference_Linear_filtred.isnull().values.any()
print (check_nan_in_df)
#False 

# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(PETR3_SA_Close_fractional_difference_Linear_filtred.values)

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name        score           loc  ... bootstrap_score bootstrap_pass    color
0            t    27.182551      0.002054  ...               0           None  #e41a1c
1     dweibull    48.597999      0.001195  ...               0           None  #e41a1c
2     loggamma    123.54552     -0.131193  ...               0           None  #377eb8
3         beta     135.5376 -38992.899184  ...               0           None  #4daf4a
4         norm   146.760645      0.000844  ...               0           None  #984ea3
5      lognorm   147.904074      -24.5767  ...               0           None  #ff7f00
6        gamma   229.393499     -0.387716  ...               0           None  #ffff33
7      uniform  1660.004112     -0.229158  ...               0           None  #a65628
8        expon  1971.820721     -0.229158  ...               0           None  #f781bf
9   genextreme  1986.635063     -0.017151  ...               0           None  #999999
10      pareto  2028.151977     -0.760411  ...               0           None  #999999


# Plot resultados
dist.plot()
plt.show()

#Figura1.1f

########################
#                      #
#    Testes de         #
#         Normalidade  #
#                      # 
########################

t1 = stats.kstest(PETR3_SA_Close_fractional_difference_Linear_filtred, 'norm') # KS
t2 = lilliefors(PETR3_SA_Close_fractional_difference_Linear_filtred, dist='norm', pvalmethod='approx') # Lilliefors
t3 = stats.shapiro(PETR3_SA_Close_fractional_difference_Linear_filtred) # Shapiro-Wilk
t4 = normaltest(PETR3_SA_Close_fractional_difference_Linear_filtred) # Shapiro-Francia


pd.set_option('display.width', 1000)
pd.set_option('colheader_justify', 'center')

columns=['kstest', 'lilliefors', 'Shapiro-Wilk', 'Shapiro-Francia'] 
lines=['statistics', ' p-values']

df = pd.DataFrame({'kstest': t1,'lilliefors': t2,'Shapiro-Wilk': t3,'Shapiro-Francia': t4,},lines, columns)

               kstest             Shapiro-Francia
statistics   5.906269e-01        [266.91026262405114]
 p-values   8.380711e-249    [1.0994434328579063e-58]

[2 rows x 4 columns]

# ADF Test
result = adfuller(PETR3_SA_Close_fractional_difference_Linear_filtred.values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
#ADF Statistic: -20.62490417371537

print(f'p-value: {result[1]}')
#p-value:  0.0 < 0.05 ( Reject the null hypotesis that seris is  non-statioNarxy )

for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

  
Critial Values:
   1%, -3.4391580196774494
Critial Values:
   5%, -2.8654273226340554
Critial Values:
   10%, -2.5688400274762397


# KPSS Test
result = kpss(PETR3_SA_Close_fractional_difference_Linear_filtred.values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
KPSS Statistic:   0.060768 > 0.05 ( Rejeitar a hipotese nula que a série não é estacionária)

print('p-value: %f' % result[1])
# p-value: 0.100000


for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')
		
Critial Values:
   10%, 0.347
Critial Values:
   5%, 0.463
Critial Values:
   2.5%, 0.574
Critial Values:
   1%, 0.739
  
   
names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']

for i, x in enumerate(names):
   PETR3_SA[x] = (PETR3_SA.index.get_level_values(0).weekday == i).astype(int)

dates = PETR3_SA[['Date','Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']][:-30]
dates = dates.append(pd.DataFrame({'Date': pd.date_range(start=dates.Date.iloc[-1], periods= 31, freq='d')}))
dates.index = dates["Date"]

for i, x in enumerate(names):
   dates[x] = (dates.index.get_level_values(0).weekday == i).astype(int)

pd.DataFrame(dates).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\dates.csv', index=None)
dates = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/dates.csv")
  
############################################################################
# Particionando os dados entre treino e teste.Para séries temporais,       #
#      devido a interdependência e autocorrelação,                         #
#          foi colocado apenas as últimas 30 obs para a partição de teste. #
############################################################################

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min  

PETR3_SA_Close_fractional_difference_Linear_filtred = function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred,-1,+1)
PETR3_SA_Close_fractional_difference_Linear_filtred_train  =  PETR3_SA_Close_fractional_difference_Linear_filtred[:-30]
PETR3_SA_Close_fractional_difference_Linear_filtred_test   =  PETR3_SA_Close_fractional_difference_Linear_filtred[-30:]

PETR3_SA_Volume_Linear_filtred_train =  PETR3_SA_Volume_Linear_filtred[:-30]  
PETR3_SA_Volume_Linear_filtred_test  =  PETR3_SA_Volume_Linear_filtred[-30:]

sm.graphics.tsa.plot_pacf((PETR3_SA_Close_fractional_difference_Linear_filtred_train), lags=50, method="ywm") # p = 1
sm.graphics.tsa.plot_acf((PETR3_SA_Close_fractional_difference_Linear_filtred_train), lags=50)  # q = 1
plt.show()

Figure_1-1f_pacf_PETR3_LOG_DIFF_LINEAR_FILTRED
#############################/                                               \##################################
#                                                      
#           Espectograma   
#
#    Wolf,Peter 2.5: Noise Modeling - White, Pink, and Brown Noise, Pops and Crackles
#
#   https://eng.libretexts.org/Bookshelves/
#	Industrial_and_Systems_Engineering/Chemical_Process_Dynamics_and_Controls_(Woolf)
#	/02%3A_Modeling_Basics/
#	2.05%3A_Noise_modeling-
#    _more_detailed_information_on_noise_modeling-_white%2C_pink%2C_and_brown_noise%2C_pops_and_crackles                            
#                                                      
#############################/                                                \#################################

# Obtendo a taxa de amostragem através das duas primeiras observaçõs dos log retornos da série para a amostragem de treino
inter_sample_time = PETR3_SA_Close_fractional_difference_Linear_filtred_train.values[1] - PETR3_SA_Close_fractional_difference_Linear_filtred_train.values[0]
print('inter_sample_time =', inter_sample_time, 'd')
#inter_sample_time = [0.19442186] d


# Checar se tem o mesmo tamanho 
print('Mesmo tamanho?:', np.allclose(PETR3_SA_Close_fractional_difference_Linear_filtred_train, inter_sample_time))
#Mesmo tamanho?: False.Não é um Ruído Branco Uniforme (Infinito) 

print('n_samples =', len(PETR3_SA_Close_fractional_difference_Linear_filtred_train))
#n_samples = 717
n_samples = len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)
sampling_freq =  1 / inter_sample_time

# Obtendo as frequências do espectograma
f = np.fft.fftfreq(len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)) * sampling_freq

# Obtendo a desnidade do espectograma
psd = np.abs(np.fft.fft(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values))**2 / len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)

# Gráfico
plt.plot(f[f>=0], psd[f>=0])
plt.xlabel('freq (Hz)')
plt.ylabel('PSD')
plt.margins(0.02)
plt.show()

Figure_1-1g


frequency = np.log(abs(f.flatten()))
frequency[np.isneginf(frequency)] = np.mean(frequency)
frequency = pd.DataFrame(frequency)
frequency = frequency.replace([np.inf, -np.inf], 0).fillna(0)
psd = np.log(abs(psd.flatten())).reshape(-1, 1)
psd[np.isneginf(psd)] = np.mean(psd)
psd = pd.DataFrame(psd)
psd = psd.replace([np.inf, -np.inf], 0).fillna(0)
reg = LinearRegression().fit(frequency,psd)
reg.coef_
array([[-0.01664891]])
Algo entre um ruído azul e branco, mas muito perto que a amostragem de treino de PETR3.SA é um ruído branco.

#Parte 2 Modelagem

SVVol_MomentumLSTM = """
/*

//[NGUYEN   ET AL, 2019] Nguyen, Trong-Nghia & Tran, Minh-Ngoc & Gunawan, David & Kohn, Robert. (2019). A long short-term memory stochastic volatility model. 

//[TAYLOR 1986] Taylor, S. (1986). Modelling Financial Time Series. John Wiley, Chichester.

//[TAYLOR 1982] Taylor, S. J. (1982). Financial returns modelled by the product of two stochastic processes a study of daily sugar prices 1961-79. In Anderson, O. D., editor, Time Series Analysis: Theory and Practice, page 203-226. Amsterdam: North-Holland.

//[NGUYEN   ET AL, 2020] Nguyen. Tam M, Baraniuk .. Richard G,   Bertozzi. Andrea L, Osher. Stanley J, Wang, Bao. (2020). MomentumRNN: Integrating Momentum into Recurrent Neural Networks .. Advances in Neural Information Processing Systems (NeurIPS) 2020
//https://doi.org/20.48550/arXiv.2006.06919


//[HOCHREITER & SCHMIDHUBER, 1997] Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). 
//Long Short-term Memory. Neural computation. 9. 1735-80. 20.1162/neco.1997.9.8.1735. 

//[BETANCOURT , 2017] Betancourt. M (2017) , A Conceptual Introduction to Hamiltonian Monte Carlo, 2017. 
//https://doi.org/20.48550/arXiv.1701.02434.

//[ABANTO, 2020] Carlos A. Abanto-Valle, Helio S. Migon & Hedibert F. Lopes, Bayesian modeling of financial returns: A relationship between volatility and trading volume (2020) , https://hedibert.org/wp-content/uploads/2013/12/abanto-migon-lopes-2010.pdf

/*

/*----------------------- Data --------------------------*/
data {
   int<lower=1> N;
   int<lower=1> p;
   int<lower=1> T_forecast;
   real<lower=0> momentum;
   real<lower=0> epsilon;
   matrix [N+T_forecast,5] date;
   vector[N] y;
   int v[N];
}

parameters {
   vector [N] n;
   vector <lower=-1,upper=1> [N] thnv_d;
   vector <lower=-1,upper=1> [N] thnw_d;
   vector <lower=-1,upper=1> [N] thnv_i;
   vector <lower=-1,upper=1> [N] thnw_i;
   vector <lower=-1,upper=1> [N] thetv_o;
   vector <lower=-1,upper=1> [N] thnw_o;
   vector <lower=-1,upper=1> [N] thnv_f;
   vector <lower=-1,upper=1> [N] thnw_f;
   vector <lower=-1,upper=1> [N] thnb_d;
   vector <lower=-1,upper=1> [N] thnb_i;
   vector <lower=-1,upper=1> [N] thnb_o;
   vector <lower=-1,upper=1> [N] thnb_f;
   real<lower=0> sigma;         // white noise shock scale
   real<lower=-1.0, upper=1.0> monday0;
   real<lower=-1.0, upper=1.0> thuesday0;
   real<lower=-1.0, upper=1.0> wednesday0;
   real<lower=-1.0, upper=1.0> thursday0;
   real<lower=-1.0, upper=1.0> friday0;  
   real v_g_o_1;
   real v_n_d_1;     
   real v_g_i_1;
   real v_g_f_1;
   real m0;
   real m1;
   real h0;
   real beta0;
   real beta1;
   real mu;
   real phi0[p];
   real beta;
}   

transformed parameters {
  vector <lower=0.0> [N] lambda;
  vector <lower=-1, upper=1 > [p] phi_00;    // ar parameters
  vector [N] z;
  vector [N] s;
  vector <lower=-1.0, upper=1.0> [N] h;
  vector [N] C;
  vector <lower=-1.0, upper=1.0> [N] n_d;
  vector <lower= 0.0, upper=1.0> [N] g_i;
  vector <lower= 0.0, upper=1.0> [N] g_o;
  vector <lower= 0.0, upper=1.0> [N] g_f;
  vector [N] v_g_o;
  vector [N] v_n_d;
  vector [N] v_g_i;
  vector [N] v_g_f;
  real C_1;
  real h_1;
  for( i in 1:p){
     phi_00[i] = 2*fabs(phi0[i]) - 1;
  }
  for(j in 1:5){
    z[1] =  mu + n[1] + monday0*date[1,j] + thuesday0*date[1,j] + wednesday0*date[1,j] + thursday0*date[1,j] + friday0*date[1,j];
  }
  C_1 = 0.0;
  h_1 = 0.0;
  v_g_o[1] = v_g_o_1;
  v_n_d[1] = v_n_d_1;
  v_g_i[1] = v_g_i_1;
  v_g_f[1] = v_g_f_1;
  v_g_o[1] = (momentum * v_g_o[1] + epsilon*thetv_o[1]*n[1]);
  v_n_d[1] = (momentum * v_n_d[1] + epsilon*thnv_d[1]*n[1]);
  v_g_i[1] = (momentum * v_g_i[1] + epsilon*thnv_i[1]*n[1]);
  v_g_f[1] = (momentum * v_g_f[1] + epsilon*thnv_f[1]*n[1]);
  g_o[1]  = inv_logit(v_g_o[1] + thnw_o[1]*h_1 + thnb_o[1]);
  n_d[1]  = tanh(v_n_d[1]+ thnw_d[1]*h_1 + thnb_d[1]);
  g_i[1]  = inv_logit(v_g_i[1] + thnw_i[1]*h_1 + thnb_i[1]);
  g_f[1]  = inv_logit(v_g_f[1] + thnw_f[1]*h_1+ thnb_f[1]);
  C[1] =   g_i[1] * n_d[1]  + g_f[1] *C_1;
  h[1] =   g_o[1] * tanh(C[1]); 
  for(j in 1:5){
    lambda[1] = m0 + m1*exp(z[1])  + monday0*date[1,j] + thuesday0*date[1,j] + wednesday0*date[1,j] + thursday0*date[1,j] + friday0*date[1,j];
  }
  for (t in 2:N){
    v_g_o[t] = (momentum * v_g_o[t-1] + epsilon*thetv_o[t]*n[t-1]);
	v_g_o[t] = v_g_o[t] + momentum*(v_g_o[t] - v_g_o[t-1]);
    v_n_d[t] = (momentum * v_n_d[t-1] + epsilon*thnv_d[t]*n[t-1]);
	v_n_d[t] = v_n_d[t] + momentum*(v_n_d[t] - v_n_d[t-1]);
    v_g_i[t] = (momentum * v_g_i[t-1] + epsilon*thnv_i[t]*n[t-1]);
	v_g_i[t] = v_g_i[t] + momentum*(v_g_i[t] - v_g_i[t-1]);
    v_g_f[t] = (momentum * v_g_f[t-1] + epsilon*thnv_f[t]*n[t-1]);
	v_g_f[t] = v_g_f[t] + momentum*(v_g_f[t] - v_g_f[t-1]);
    g_o[t]  = inv_logit(v_g_o[t] + thnw_o[t]*h[t-1] + thnb_o[t]);
	n_d[t]  = tanh(v_n_d[t] + thnw_d[t]*h[t-1] + thnb_d[t]);
    g_i[t]  = inv_logit(v_g_i[t] + thnw_i[t]*h[t-1] + thnb_i[t]);
	g_f[t]  = inv_logit(v_g_f[t] + thnw_f[t]*h[t-1] + thnb_f[t]);
    C[t] =   g_i[t] *n_d[t]  + g_f[t] *C[max(t-1,1)];
    h[t] =   g_o[t] * tanh(C[t]); 
	if(p > 0) for(i in 1:min(t-1,p)){ 
	  z[t] =  n[t] + phi_00[i]*(z[t-i]);
	}
    for(j in 1:5){
	 lambda[t] = m0 + m1*exp(z[t])+  monday0*date[t,j] + thuesday0*date[t,j] + wednesday0*date[t,j] + thursday0*date[t,j] + friday0*date[t,j];
	   }
	}
}


model {
   beta0 ~ normal(0,0.01);
   sigma ~ inv_gamma(2.5,0.25);
   beta1 ~ inv_gamma(2.5,0.25);
   mu ~ cauchy(0, 10);
   phi0 ~ beta(20,1.5);
   m0 ~ gamma(0.08,0.1);
   m1 ~ gamma(1,10);
   n[1] ~  normal(beta0, sigma);
   for (t in 2:N){
    for(j in 1:5){
       n[t] ~   normal(beta0 + beta1*h[t] +  monday0*date[t,j] + thuesday0*date[t,j] + wednesday0*date[t,j] + thursday0*date[t,j] + friday0*date[t,j] ,sigma);	  	  
     }
   }
   for( i in 1:N){
     thetv_o[i] ~ normal(0, 0.1);
     thnw_o[i]  ~ normal(0, 0.1);
     thnb_o[i]  ~ normal(0, 0.1);
     thnv_d[i]  ~ normal(0, 0.1);
     thnw_d[i]  ~ normal(0, 0.1);
     thnb_d[i]  ~ normal(0, 0.1);
     thnv_i[i]  ~ normal(0, 0.1);
     thnw_i[i]  ~ normal(0, 0.1);
     thnb_i[i]  ~ normal(0, 0.1);
     thnv_f[i]  ~ normal(0, 0.1);
     thnw_f[i]  ~ normal(0, 0.1);
     thnb_f[i]  ~ normal(0, 0.1);
	 y[i] ~  normal(0,exp(z[i]/2));
     v[i] ~  poisson(lambda[i]);
	 }
}


generated quantities{
   vector [N+T_forecast] n_new;
   int  v_new[N+T_forecast];
   vector <lower=0.0> [N+T_forecast] lambda_new;
   vector <lower=-1.0, upper=1.0> [N+T_forecast] h_new;
   vector [N+T_forecast] z_new;
   vector [N+T_forecast] C_new;
   vector <lower=-1.0, upper=1.0>  [N+T_forecast] n_d_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_i_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_o_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_f_new;
   vector [N+T_forecast] v_g_o_new;
   vector [N+T_forecast] v_n_d_new;
   vector [N+T_forecast] v_g_i_new;
   vector [N+T_forecast] v_g_f_new;
   v_g_o_new[1:N] = v_g_o;
   v_new[1:N] = v;
   lambda_new[1:N] = lambda;
   v_n_d_new[1:N] = v_n_d;
   v_g_i_new[1:N] = v_g_i;
   v_g_f_new[1:N] = v_g_f;
   n_new[1:N] = n;
   z_new[1:N] = z;
   g_o_new[1:N] = g_o;
   n_d_new[1:N] = n_d;
   g_i_new[1:N] = g_i;
   g_f_new[1:N] = g_f;
   C_new[1:N] = C;
   h_new[1:N] = h;
   for( t in (N+1):(N+T_forecast)){
	 v_g_o_new[t] = (momentum * v_g_o_new[t-1] + epsilon*mean(thetv_o)*n_new[t-1]);
     v_g_o_new[t] = v_g_o_new[t] + momentum*(v_g_o_new[t] - v_g_o_new[t-1]);
     v_n_d_new[t] = (momentum * v_n_d_new[t-1] + epsilon*mean(thnv_d)*n_new[t-1]);
	 v_n_d_new[t] = v_n_d_new[t] + momentum*(v_n_d_new[t] - v_n_d_new[t-1]);
     v_g_i_new[t] = (momentum * v_g_i_new[t-1] + epsilon*mean(thnv_i)*n_new[t-1]);
	 v_g_i_new[t] = v_g_i_new[t] + momentum*(v_g_i_new[t] - v_g_i_new[t-1]);
     v_g_f_new[t] = (momentum * v_g_f_new[t-1] + epsilon*mean(thnv_f)*n_new[t-1]);
     v_g_f_new[t] = v_g_f_new[t] + momentum*(v_g_f_new[t] - v_g_f_new[t-1]);
	 g_o_new[t] = inv_logit(v_g_o_new[t] + mean(thnw_o)*h_new[t-1] + mean(thnb_o));
     n_d_new[t] = tanh(v_n_d_new[t]+ mean(thnw_d)*h_new[t-1] + mean(thnb_d));
	 g_i_new[t] = inv_logit(v_g_i_new[t] + mean(thnw_i)*h_new[t-1] + mean(thnb_i));
     g_f_new[t] = inv_logit(v_g_f_new[t] + mean(thnw_f)*h_new[t-1] + mean(thnb_f));
     C_new[t] =   g_i_new[t] *n_d_new[t]  + g_f_new[t] *C_new[max(t-1,1)]; 
     h_new[t] =   g_o_new[t] * tanh(C_new[t]);
	 for(j in 1:5){
       n_new[t] =  normal_rng(beta0 + beta1*h_new[t]  +  monday0*date[t,j] + thuesday0*date[t,j] + wednesday0*date[t,j] + thursday0*date[t,j] + friday0*date[t,j] ,sigma);	  	  
     }
	 for(i in 1:p){
       z_new[t] =  n_new[t] + phi_00[i]*(z_new[t-i]);
     }
	 for(j in 1:5){
       lambda_new[t] = m0 + m1*exp(z_new[t]) +  monday0*date[t,j] + thuesday0*date[t,j] + wednesday0*date[t,j] + thursday0*date[t,j] + friday0*date[t,j];
	 }
     v_new[t] =   poisson_rng(lambda_new[t]);		
     }
}



"""   

SVVol_MomentumLSTM  = pystan.StanModel(model_code=SVVol_MomentumLSTM)

data  = ({'N' : len(PETR3_SA_Close_fractional_difference_Linear_filtred_train),
          'p' : 1,
          'T_forecast' : len(PETR3_SA_Close_fractional_difference_Linear_filtred_test),
		  'momentum': 0.90,   
		  'epsilon' : 0.90,
		  'date': dates[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']].values.astype(np.int16),
          'y':  np.array(PETR3_SA_Close_fractional_difference_Linear_filtred_train, dtype=np.float16).flatten() - np.mean(np.array(PETR3_SA_Close_fractional_difference_Linear_filtred_train, dtype=np.float16)).flatten(),
		  'v':  np.array(PETR3_SA_Volume_Linear_filtred_train/1000000).astype(np.int16).flatten()
		 }) 
		 
control = {}
control['max_treedepth'] = 1
control['adapt_delta'] = 1

fit_SVVol_MomentumLSTM = SVVol_MomentumLSTM.sampling(data=data, iter=3500, chains=3, warmup=1200 ,thin=1, seed=101, n_jobs = -1, control=control)
		 
parameters_SVVol_MomentumLSTM = fit_SVVol_MomentumLSTM.extract(permuted=True)
parameters_SVVol_MomentumLSTM.keys()

odict_keys(['n', 'thnv_d', 'thnw_d', 'thnv_i', 'thnw_i', 'thetv_o', 'thnw_o', 'thnv_f', 'thnw_f', 'thnb_d', 'thnb_i', 't
hnb_o', 'thnb_f', 'sigma', 'monday0', 'thuesday0', 'wednesday0', 'thursday0', 'friday0', 'v_g_o_1', 'v_n_d_1', 'v_g_i_1'
, 'v_g_f_1', 'm0', 'm1', 'h0', 'beta0', 'beta1', 'mu', 'phi0', 'beta', 'lambda', 'phi_00', 'z', 's', 'h', 'C', 'n_d', 'g
_i', 'g_o', 'g_f', 'v_g_o', 'v_n_d', 'v_g_i', 'v_g_f', 'n_new', 'v_new', 'lambda_new', 'h_new', 'z_new', 'C_new', 'n_d_n
ew', 'g_i_new', 'g_o_new', 'g_f_new', 'v_g_o_new', 'v_n_d_new', 'v_g_i_new', 'v_g_f_new', 'lp__'])


h_predict_all = np.exp(parameters_SVVol_MomentumLSTM['z_new']/2)
h_predict = np.exp(parameters_SVVol_MomentumLSTM['z_new'].mean(axis=0)/2)

pd.DataFrame(pd.DataFrame(h_predict_all)).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\volatility_PETR3_SA_Close_SV_MomentumLSTM_all.csv', index=None)
pd.DataFrame(pd.DataFrame(h_predict)).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\volatility_PETR3_SA_Close_SV_MomentumLSTM.csv', index=None)

volatility_PETR3_SA_Close_SV_MomentumLSTM = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volatility_PETR3_SA_Close_SV_MomentumLSTM.csv")
volatility_PETR3_SA_Close_SV_MomentumLSTM_all = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volatility_PETR3_SA_Close_SV_MomentumLSTM_all.csv")

mean = np.mean(volatility_PETR3_SA_Close_SV_MomentumLSTM_all, axis=0) 
std = np.std(volatility_PETR3_SA_Close_SV_MomentumLSTM_all, axis=0)
Credibility_intervals = 1.960 * (std/np.sqrt(len(volatility_PETR3_SA_Close_SV_MomentumLSTM)))
upper_bands = mean + Credibility_intervals
lower_bands = mean - Credibility_intervals

plt.plot(function_to_scale_data(volatility_PETR3_SA_Close_SV_MomentumLSTM.values,0,1), label = "Amostra de teste prevista")
plt.title('Volatilidades para os log-retornos a 1.0 da PETR3_SA para os dados de treino ')
plt.axvline(x = len(PETR3_SA_Close_fractional_difference_Linear_filtred_train[-len(PETR3_SA_Close_fractional_difference_Linear_filtred_train):]), color = 'black', label = 'Volatilidade Extrapolado')
plt.plot(function_to_scale_data(upper_bands.values,0,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.values,0,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figura2.1.a

sm.graphics.tsa.plot_pacf((volatility_PETR3_SA_Close_SV_MomentumLSTM.values), lags=50, method="ywm") #1
plt.show()

#Figure_2-1a_pacf_volatility..png

v_predict_all = parameters_SVVol_MomentumLSTM['v_new']
v_predict = parameters_SVVol_MomentumLSTM['v_new'].mean(axis=0)

pd.DataFrame(pd.DataFrame(v_predict_all)).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\volume_PETR3_SA_Close_SV_MomentumLSTM_all.csv', index=None)
pd.DataFrame(pd.DataFrame(v_predict)).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\volume_PETR3_SA_Close_SV_MomentumLSTM.csv', index=None)

volume_PETR3_SA_Close_SV_MomentumLSTM = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volume_PETR3_SA_Close_SV_MomentumLSTM.csv")
volume_PETR3_SA_Close_SV_MomentumLSTM_all = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volume_PETR3_SA_Close_SV_MomentumLSTM_all.csv")

mean = np.mean(volume_PETR3_SA_Close_SV_MomentumLSTM_all, axis=0) 
std = np.std(volume_PETR3_SA_Close_SV_MomentumLSTM_all, axis=0)
Credibility_intervals = 1.960 * (std/np.sqrt(len(volume_PETR3_SA_Close_SV_MomentumLSTM)))
upper_bands = mean + Credibility_intervals
lower_bands = mean - Credibility_intervals

plt.plot(volume_PETR3_SA_Close_SV_MomentumLSTM.values, label = "Amostra de teste prevista")
plt.title('Volatilidades para os log-retornos a 1.0 da PETR3_SA para os dados de treino ')
plt.axvline(x = len(PETR3_SA_Close_fractional_difference_Linear_filtred_train[-len(PETR3_SA_Close_fractional_difference_Linear_filtred_train):]), color = 'black', label = 'volume extrapolado')
plt.plot(upper_bands.values , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(lower_bands.values , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figura2.1.b

sm.graphics.tsa.plot_pacf((volume_PETR3_SA_Close_SV_MomentumLSTM.values), lags=50, method="ywm") #1
plt.show()

#Figure_2-1b_pacf_volume..png

#########*/                                                     *\###########
#
#      */\*ar Aproximações Numéricas   
#                  recursivas via State Space HMC  */\*
#      Linear Auto-Regression *ar* 
#      Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X
#
#########*/                                                      *\###########


###############################################################################################################
#                                                                                                             #
#           Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X                            #
#                                                                           Mean Time Series Model            #
#                                                                                                             #
###############################################################################################################


Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X = """


//[NGUYEN   ET AL, 2019] Nguyen, Trong-Nghia & Tran, Minh-Ngoc & Gunawan, David & kohn, Robert. (2019). A long short-term memory stochastic volatility model. 

//[NGUYEN   ET AL, 2020] Nguyen. Tam M, Baraniuk .. Richard G,   Bertozzi. Andrea L, Osher. Stanley J, Wang, Bao. (2020). MomentumRNN: Integrating Momentum into Recurrent Neural Networks. Advances in Neural Information Processing Systems (NeurIPS) 2020
//https://doi.org/20.48550/arXiv.2006.06919

//[DEY & SALEM ,, 2017] Dey.R & Salem.F.M ( (2017)   Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks, 2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)
//DOI: 20.1209/MWSCAS.2017.8053243.

//[FERRARA ET GUEGAN, 2000] Ferrara, L & D. Guegan (2000) ,,Forecasting Financial Times Series with Generalized Long Memory Processes.   Advances in Quantitative Asset Management (pp.319-342)
//DOI:20.02007/978-1-4620-4389-3_14.

//[CHUNG ET AL., 2014] Junyoung Chung, Caglar Gulcehre, kyungHyun Cho, Yoshua Bengio (2014) Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. 2014 Deep Learning and Representation Learning Workshop ,,   	
//https://doi.org/20.48550/arXiv.1412.3555

//[HOCHREITER & SCHMIDHUBER, 1997] Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). Long Short-term Memory. Neural computation. 9. 1735-80. 20.1162/neco.1997.9.8.1735. 

//[SCHUSTER & PALIWAL, 1997] Schuster, Mike & Paliwal, kuldip. (1997). Bidirectional recurrent neural networks. Signal Processing IEEE Transactions on. 45. 2673 - 2681. 20.1209/78.650093. 

//[BOX & JENkIS, 1976] Box, G. E. P and Jenkins, G.M., (1976). “Time series analysis: Forecasting and control,” Holden-Day, San Francisco.

//[GLOSTEN, JAGANNATHAN & RUNKLE, 1993] Lawrence R. Glosten, Ravi Jagannathan, David   E. Runkle (1993) On the Relation between the Expected Value and the Volatility of the Nominal Excess Return on Stocks. The Journal of Finance ,, Volume 48 Issue 5 ,, 1779-1801,   https://doi.org/20.1111/j.2040-6261.1993.tb05128.x

//[HIDAYANA, 2021] Hidayana, R.A, Sukuno, Napitupulu,N.(2021) FARMA-GJR-GARCH Model for Determining Value-at-Risk and Back testing of Some Stock Returns   Proceedings of the Second Asia Pacific International Conference on Industrial Engineering and Operations Management Surakarta, 
//Indonesia, September 14-16, 2021

//[RAMACHANDRAN ET AL. 2017] Ramachandran, Prajit and Zoph, Barret and Le, Quoc V. (2017) ,
//Swish: a Self-Gated Activation Function,  https://arxiv.org/abs/1720.05941

//[AGARAP, 2018] Agarap, Abien Fred( 2018) Deep Learning using Rectified Linear Units (ReLU) 
//https://arxiv.org/abs/1803.08375

//https://discourse.mc-stan.org/t/chains-not-mixing-for-gegenbauer-time-series/12437 Gegenbauer implementation Stan
//https://groups.google.com/g/stan-users/c/BTdmJlXnk-Q FARMA-GARCH implementation Stan HMC
//https://discourse.mc-stan.org/t/gjr-garch-model-in-stan/3590  GJR-GRACH implementation Stan HMC

//[Damiano , Peterson et Weylandt, 2017] https://luisdamiano.github.io/stancon18/hmm_stan_tutorial.pdf  
A Tutorial on Hidden Markov Models using Stan
Luis Damiano (Universidad Nacional de Rosario), Brian Peterson (University of
Washington), Michael Weylandt (Rice University)
2017-12-15

//[BETANCOURT , 2017] Betancourt. M (2017) , A Conceptual Introduction to Hamiltonian Monte Carlo, 2017. 
//https://doi.org/20.48550/arXiv.1701.02434.



functions {
  real swish(real x) {
   return x*inv_logit(x*0.000001);
  }
  real ReLU(real x) {
   return fmax(x,0.0);
  }
}


data {
  // Hiperparametros
  int<lower=0> k1;      // numero de frequencias
  int<lower=0> N;        // numero de linhas da matriz 
  int<lower=0> p;        // numero de coeficientes autoregressivos
  int<lower=0> p1;        // numero de coeficientes autoregressivos Garch
  int<lower=0> p2;        // numero de coeficientes autoregressivos exogenous
  int<lower=0> q;        // numero de coeficientes médias móveis
  int<lower=0> k;        // número de coeficientes Gegenbauer 
  int<lower=0> T_forecast;
  real<lower=0.0> momentum;
  real<lower=0.0> epsilon;
  // data
  vector[N] y;       // resultado fracionado da série temporal *processo arfima*
  vector[N+T_forecast] x; // Regressor Exogeno.Sua própria volatilidade extrapolada
  vector[N+T_forecast] x1; // Regressor Exogeno.Sua próprio volume extrapolado
}

parameters{
  real<lower=0.0,upper=0.5> f_1;
  real<lower=0.0,upper=0.5> d_1;
  real <lower=- 1,upper = 1> alfa1 [ k1 ] ;  // sine parameters
  real <lower=- 1,upper = 1> beta1 [ k1 ] ;  // cossine parameters
  // HMM Transition Probabilities Probabilidades de Transição
  // Parametrizar pela probabilidade de permanecer no estado que se está no presente t
  real<lower=0, upper=1> p_remain[2];
  real<lower=0> sigma0;
  real  v_g_f_gru1;
  real  v_h_gru1;  
  real 	v_g_o_forward1;
  real  v_n_d_forward1;
  real  v_g_i_forward1;
  real  v_g_f_forward1;
  real  v_g_o_backward1;
  real  v_n_d_backward1;
  real  v_g_i_backward1;
  real  v_g_f_backward1;
  real <lower=-1,upper=1> phi0 [p];   // ar parametros
  real <lower=-1,upper=1> theta0 [q]; // ma parametros
  real <lower=0> omega [2];   ///                  
  real <lower=0,upper=1>  alpha [p,2];   //Não pode ser negativo para garantir variância positiva
  real <lower=0,upper=1>  psi [p,2] ;      //Não pode ser negativo para garantir variância positiva
  real <lower=0,upper=1>  beta [q,2] ;      //Limite superior para garantir a estacionalidade
  real <lower=0,upper=1>  phix [p1,2];   //   garchx ar parametro
  real <lower=0,upper=1>  phix2 [p2,2];   //  garchx ar parametro
  real mu0;  // parametro de localização
  vector <lower=-1,upper=1> [N] thnv_d_forward;
  vector <lower=-1,upper=1> [N] thnw_d_forward;
  vector <lower=-1,upper=1> [N] thnv_i_forward;
  vector <lower=-1,upper=1> [N]thnw_i_forward;
  vector <lower=-1,upper=1> [N]thetv_o_forward;
  vector <lower=-1,upper=1> [N]thnw_o_forward;
  vector <lower=-1,upper=1> [N]thnv_f_forward;
  vector <lower=-1,upper=1> [N]thnw_f_forward;
  vector <lower=-1,upper=1> [N]thnb_d_forward;
  vector <lower=-1,upper=1> [N]thnb_i_forward;
  vector <lower=-1,upper=1> [N]thnb_o_forward;
  vector <lower=-1,upper=1> [N]thnb_f_forward;
  vector <lower=-1,upper=1> [N]thetv_o_backward;
  vector <lower=-1,upper=1> [N]thnw_o_backward;
  vector <lower=-1,upper=1> [N]thnb_o_backward;
  vector <lower=-1,upper=1> [N]thnv_d_backward;
  vector <lower=-1,upper=1> [N]thnw_d_backward;
  vector <lower=-1,upper=1> [N]thnb_d_backward;
  vector <lower=-1,upper=1> [N]thnv_i_backward;
  vector <lower=-1,upper=1> [N]thnw_i_backward;
  vector <lower=-1,upper=1> [N]thnb_i_backward;
  vector <lower=-1,upper=1> [N]thnv_f_backward;
  vector <lower=-1,upper=1> [N]thnw_f_backward;
  vector <lower=-1,upper=1> [N]thnb_f_backward;
  vector <lower=-1,upper=1> [N] thnv_f_gru;
  vector <lower=-1,upper=1> [N] thnw_f_gru;
  vector <lower=-1,upper=1> [N] thnb_f_gru;
  vector <lower=-1,upper=1> [N] thnw_h_gru;
  vector <lower=-1,upper=1> [N] thnb_h_gru;
  vector <lower=-1,upper=1> [N] theta_h_gru;
}

transformed parameters{
	// Probabilidades de trasição entre os regimes (alta e baixa volatilidade)
	matrix[2, 2] A;
	vector[2] probability[N];
	// HMM Paramêtros       
    vector [2] log_alpha[N]; // Probabilidades acumuladas dos estados não-normalizadas 	
    vector [N] v_g_o_forward;
    vector [N] v_n_d_forward;
    vector [N] v_g_i_forward;
    vector [N] v_g_f_forward;
	vector [N] v_g_o_backward;
    vector [N] v_n_d_backward;
    vector [N] v_g_i_backward;
    vector [N] v_g_f_backward;
	vector [N] v_g_f_gru;
	vector [N] v_h_gru;
    vector <lower= -1.0, upper=1.0> [N] h_forward;
    vector [N] C_forward;
    vector <lower=-1.0, upper=1.0> [N] n_d_forward;
    vector <lower=0.0, upper=1.0>  [N] g_i_forward;
    vector <lower=0.0, upper=1.0>  [N] g_o_forward;
    vector <lower=0.0, upper=1.0>  [N] g_f_forward;
    vector <lower=-1.0, upper=1.0> [N] h_backward ;
    vector [N] C_backward;
    vector <lower=-1.0, upper=1.0> [N] n_d_backward;
    vector <lower= 0.0, upper=1.0> [N] g_i_backward;
    vector <lower= 0.0, upper=1.0> [N] g_o_backward;
    vector <lower= 0.0, upper=1.0> [N] g_f_backward;
    vector <lower=-1.0, upper=1.0> [N] h_concatenate;
	vector <lower= 0.0, upper=1.0> [N] g_f_gru;
	vector <lower=-1.0, upper=1.0> [N] h_gru;
	vector [k1] alfa11;  //  sine frequencies parameters
    vector [k1] beta11; //   cossine frequencies parameters
    real h_forward_0;
    real C_forward_0;
    real h_backward_0;
    real C_backward_0;
	real h_0;
    //***********************************************
    //          Paramêtros do modelo
    //***********************************************
    // parametros dos coeficientes
    vector <lower=0> [2] sigma[N];
    vector <lower=-1, upper=1 > [p] phi ;    // ar parameters
    vector <lower=-1, upper=1 > [q] theta;   // ma parameters
    // Temporal mean while residuals 
    vector  [N] mu;         // Mean Parameter
    vector  [N] Epsilon;     // residual parameter
	real g_1[k+1];
    real u_1;
	real accumulator[2];
	A[1, 1] =  p_remain[1];
	A[1, 2] = 1 - p_remain[1];
	A[2, 1] = 1 - p_remain[2];
	A[2, 2] = p_remain[2]; 
    u_1=cos(2.0*pi()*f_1);
    g_1[1] = 1.0;            // this while next few lines to derive the Ggbr Coefficients.
    g_1[2] = 2.0*u_1*d_1;
    for (j in 3:(k+1)) {
      g_1[j]=(2.0*u_1*((j-1)+d_1-1.0)*g_1[j-1]-((j-1)+2.0*d_1-2.0)*g_1[j-2])/(j-1);
    }
    h_forward_0 = 0.0;
    C_forward_0 = 0.0;
	v_g_o_forward[1] = v_g_o_forward1;
    v_n_d_forward[1] = v_n_d_forward1;
    v_g_i_forward[1] = v_g_i_forward1;
    v_g_f_forward[1] = v_g_f_forward1;
    v_g_o_forward[1] = (momentum * v_g_o_forward[1] + epsilon*thetv_o_forward[1]*h_forward_0);
    v_n_d_forward[1] = (momentum * v_n_d_forward[1] + epsilon*thnv_d_forward[1]*h_forward_0);
    v_g_i_forward[1] = (momentum * v_g_i_forward[1] + epsilon*thnv_i_forward[1]*h_forward_0);
    v_g_f_forward[1] = (momentum * v_g_f_forward[1] + epsilon*thnv_f_forward[1]*h_forward_0);
    g_o_forward[1]  = inv_logit(v_g_o_forward[1] + thnw_o_forward[1]*y[1] + thnb_o_forward[1]);
    n_d_forward[1]  = tanh(v_n_d_forward[1] + thnw_d_forward[1]*y[1] + thnb_d_forward[1]);
    g_i_forward[1]  = inv_logit(v_g_i_forward[1] + thnw_i_forward[1]*y[1] + thnb_i_forward[1]);
    g_f_forward[1]  = inv_logit(v_g_f_forward[1] + thnw_f_forward[1]*y[1] + thnb_f_forward[1]);
    C_forward[1] =   g_i_forward[1] *n_d_forward[1]  + g_f_forward[1] *C_forward_0;
    h_forward[1] =   g_o_forward[1]* tanh(C_forward[1]); 
    for (l in 2:N){
	 v_g_o_forward[l] = (momentum * v_g_o_forward[l-1] + epsilon*thetv_o_forward[l]*h_forward[max(l-1,1)]);
	 v_g_o_forward[l] = v_g_o_forward[l] + momentum*(v_g_o_forward[l] - v_g_o_forward[l-1]);
     v_n_d_forward[l] = (momentum * v_n_d_forward[l-1] + epsilon*thnv_d_forward[l]*h_forward[max(l-1,1)]);
	 v_n_d_forward[l] = v_n_d_forward[l] + momentum*(v_n_d_forward[l] - v_n_d_forward[l-1]);
     v_g_i_forward[l] = (momentum * v_g_i_forward[l-1] + epsilon*thnv_i_forward[l]*h_forward[max(l-1,1)]);
	 v_g_i_forward[l] = v_g_i_forward[l] + momentum*(v_g_i_forward[l] - v_g_i_forward[l-1]);
     v_g_f_forward[l] = (momentum * v_g_f_forward[l-1] + epsilon*thnv_f_forward[l]*h_forward[max(l-1,1)]);
     v_g_f_forward[l] = v_g_f_forward[l] + momentum*(v_g_f_forward[l] - v_g_f_forward[l-1]);
	 g_o_forward[l]  = inv_logit(v_g_o_forward[l] + thnw_o_forward[l]*y[max(l-1,1)] + thnb_o_forward[l]);
     n_d_forward[l]  = tanh(v_n_d_forward[l] + thnw_d_forward[l]*y[max(l-1,1)] + thnb_d_forward[l]);
     g_i_forward[l]  = inv_logit(v_g_i_forward[l] + thnw_i_forward[l]*y[max(l-1,1)] + thnb_i_forward[l]);
     g_f_forward[l]  = inv_logit(v_g_f_forward[l] + thnw_f_forward[l]*y[max(l-1,1)] + thnb_f_forward[l]);
     C_forward[l] =   g_i_forward[l]*n_d_forward[l]  + g_f_forward[l]*C_forward[max(l-1,1)];
     h_forward[l] =   g_o_forward[l]*tanh(C_forward[l]); 
     }
     h_backward_0 = 0.0;
     C_backward_0 = 0.0;
	 v_g_o_backward[1] = v_g_o_backward1;
     v_n_d_backward[1] = v_n_d_backward1;
     v_g_i_backward[1] = v_g_i_backward1;
     v_g_f_backward[1] = v_g_f_backward1;
     v_g_o_backward[1] = (momentum * v_g_o_backward[1] + epsilon*thetv_o_backward[1]*h_backward_0);
     v_n_d_backward[1] = (momentum * v_n_d_backward[1] + epsilon*thnv_d_backward[1]*h_backward_0);
     v_g_i_backward[1] = (momentum * v_g_i_backward[1] + epsilon*thnv_i_backward[1]*h_backward_0);
     v_g_f_backward[1] = (momentum * v_g_f_backward[1] + epsilon*thnv_f_backward[1]*h_backward_0);
     g_o_backward[1]  = inv_logit(v_g_o_backward[1] + thnw_o_backward[1]*y[N] + thnb_o_backward[1]);
     n_d_backward[1]  = tanh(v_n_d_backward[1]+ thnw_d_backward[1]*y[N] + thnb_d_backward[1]);
     g_i_backward[1]  = inv_logit(v_g_i_backward[1] + thnw_i_backward[1]*y[N] + thnb_i_backward[1]);
     g_f_backward[1]  = inv_logit(v_g_f_backward[1]+ thnw_f_backward[1]*y[N] + thnb_f_backward[1]);
     C_backward[1] =   g_i_backward[1]*n_d_backward[1]  + g_f_backward[1]*C_backward_0; 
     h_backward[1] =   g_o_backward[1]*tanh(C_backward[1]);
     for (t in 2:N){
	      v_g_o_backward[t] = (momentum * v_g_o_backward[t-1] + epsilon*thetv_o_backward[t]*h_backward[max(t-1,1)]);
		  v_g_o_backward[t] = v_g_o_backward[t] + momentum*(v_g_o_backward[t] - v_g_o_backward[t-1]);
          v_n_d_backward[t] = (momentum * v_n_d_backward[t-1] + epsilon*thnv_d_backward[t]*h_backward[max(t-1,1)]);
          v_n_d_backward[t] = v_n_d_backward[t] + momentum*(v_n_d_backward[t] - v_n_d_backward[t-1]);
		  v_g_i_backward[t] = (momentum * v_g_i_backward[t-1] + epsilon*thnv_i_backward[t]*h_backward[max(t-1,1)]);
          v_g_i_backward[t] = v_g_i_backward[t] + momentum*(v_g_i_backward[t] - v_g_i_backward[t-1]);
		  v_g_f_backward[t] = (momentum * v_g_f_backward[t-1] + epsilon*thnv_f_backward[t]*h_backward[max(t-1,1)]);
          v_g_f_backward[t] = v_g_f_backward[t] + momentum*(v_g_f_backward[t] - v_g_f_backward[t-1]);
		  g_o_backward[t]  = inv_logit(v_g_o_backward[t] + thnw_o_backward[t]*y[max(N-t,1)]  + thnb_o_backward[t]);
          n_d_backward[t]  = tanh(v_n_d_backward[t]  + thnw_d_backward[t]*y[max(N-t,1)]  + thnb_d_backward[t]);
          g_i_backward[t]  = inv_logit(v_g_i_backward[t]  + thnw_i_backward[t]*y[max(N-t,1)]  + thnb_i_backward[t]);
          g_f_backward[t]  = inv_logit(v_g_f_backward[t]  + thnw_f_backward[t]*y[max(N-t,1)]  + thnb_f_backward[t]);
          C_backward[t] =    g_i_backward[t]*n_d_backward[t]  + g_f_backward[t]*C_backward[max(t-1,1)] ; 
          h_backward[t] =    g_o_backward[t]*tanh(C_backward[t]);
    }
    h_concatenate[1] = (h_forward[1] +h_backward[N])/2;
    //***********************************************
    //       Coeficientes de  Transformação
    //***********************************************
    for( i in 1:p){
     phi[i] = 2*fabs(phi0[i])-1;
    }
    for(i in 1:q){
     theta[i] =2*fabs(theta0[i])-1;
    }
	if (k1 > 0)
	for(i in 1:k1){
        alfa11[i] = 2*fabs(alfa1[i])-1;
	}
    for(i in 1:k1){
        beta11[i] = 2*fabs(beta1[i])-1;
	}
    //***********************************************
    //       FARMA_MS_GJR_GARCH_X estimação
    //***********************************************
    mu[1] = swish(mu0  + phi[1]*h_concatenate[1]);    // assume err[0] == 0
    Epsilon[1] = y[1] - mu[1];
	h_0 = 0.0;
	v_g_f_gru[1] = v_g_f_gru1;
	v_h_gru[1] =   v_h_gru1;
	v_g_f_gru[1] =  (momentum * v_g_f_gru[1] + epsilon*thnw_f_gru[1]*h_0);
    g_f_gru[1]   =  inv_logit(v_g_f_gru[1] + thnv_f_gru[1]*Epsilon[1] +  thnb_f_gru[1]);
	v_h_gru[1]   =  (momentum *v_h_gru[1] + epsilon*thnw_h_gru[1]*(g_f_gru[1] + h_0));
	h_gru[1]     =   tanh(v_h_gru[1] + theta_h_gru[1]*Epsilon[1]  + thnb_h_gru[1]);
	h_gru[1]     =  (1- g_f_gru[1])*h_0 +  g_f_gru[1] *h_gru[1];
	for(ii in 1:2){
	    if (h_gru[1] > 0){
	      sigma[1,ii] = ReLU(sqrt(omega[ii] + (alpha[1,ii] + psi[1,ii]*(h_gru[1])*(pow(h_gru[1],2))) + beta[1,ii]*pow(sigma0,2)+ phix[1,ii]*x[1]+phix2[1,ii]*x1[1]));
		}
		else{
		   sigma[1,ii] = ReLU(sqrt(omega[ii] + (alpha[1,ii] + psi[1,ii]*(pow(h_gru[1],2))) + beta[1,ii]*pow(sigma0,2)+ phix[1,ii]*x[1]+phix2[1,ii]*x1[1]));
        }
	}
    // HMM Componente
	// ------------------	
	// Calcular log p(etado em t = j | histórico até t) recursivamente
	// Propriedade de Markov permite fazer atualizações de uma etapa por vez		
	// Assumir distribuições iguais iniciais entre os dois estados 
	// Um modelo melhor nos permitiria ponderar por distribuição estacionária HMM
	log_alpha[1, 1] = log(0.5) + normal_lpdf(Epsilon[1] | 0, sigma[1, 1]);
	log_alpha[1, 2] = log(0.5) + normal_lpdf(Epsilon[1] | 0, sigma[1, 2]);
	probability[1] = softmax(log_alpha[1]);
	for(t in 2:N){
	 for(ii in 1:2){
      if (p > 0) for(i in 1:min(t-1,p)){
       if(q > 0) for(j in 1:min(t-1,q)) {
		if(p1 > 0) for(l in 1:min(t-1,p1)){
		 if(p2 >0) for (n in 1:min(t-1,p2)){
		  v_g_f_gru[t] = (momentum * v_g_f_gru[t-1] + epsilon*thnw_f_gru[t]*h_gru[t-1]);
		  v_g_f_gru[t] = v_g_f_gru[t] + momentum*(v_g_f_gru[t] - v_g_f_gru[t-1]);
		  g_f_gru[t] = inv_logit(v_g_f_gru[t] + thnv_f_gru[t]*Epsilon[t-1] + thnb_f_gru[t]);
		  v_h_gru[t] =  (momentum *v_h_gru[t-1] + epsilon*thnw_h_gru[t]*(g_f_gru[t] + h_gru[t-1]));
		  v_h_gru[t] = v_h_gru[t] + momentum*(v_h_gru[t] - v_h_gru[t-1]);
	      h_gru[t] =  tanh(v_h_gru[t] + theta_h_gru[t]*Epsilon[t-1] + thnb_h_gru[t]);
	      h_gru[t] = (1- g_f_gru[t])*h_gru[t] +  g_f_gru[t] *h_gru[t];
          if (h_gru[t - i] > 0) {
               sigma[t,ii] = ReLU(sqrt(omega[ii] + (alpha[i,ii] + psi[i,ii]*(h_gru[t - i])*(pow(h_gru[t - i],2))) + beta[j,ii]*pow(sigma[t-j,ii],2)+ phix[l,ii]*x[t-l] + phix2[n,ii]*x1[t-n]));
               } 
		  else{
               sigma[t,ii] = ReLU(sqrt(omega[ii] + (alpha[i,ii] + psi[i,ii]*(pow(h_gru[t - i],2))) + beta[j,ii]*pow(sigma[t-j,ii],2) + phix[l,ii]*x[t-l] + phix2[n,ii]*x1[t-n]));
               }
              }
            }
	      }
	   }
	}
    h_concatenate[t] =  (h_forward[t] +h_backward[max(N-t,1)])/2;
	// FARMA Estimation
	if(k1 > 0) for (n in 1:min(t-1,k1)){
    //Gegenbauer estimação  
     if(k > 0) for (m in 1:min(t-1,k)){
      //  ar Estimation
      if(p > 0) for (i in 1:min(t-1,p)){
        if(q > 0) for(j in 1:min(t-1,q)) {
            mu[t] = swish(mu0 + h_concatenate[t-i]*phi[i]  + g_1[m+1]*Epsilon[t-m] + Epsilon[t-j]*theta[j] + (alfa11[k]*sin((2*pi()*n/N)*t) + beta11[k]*cos((2*pi()*n/N)*t))*mu[t- i]);
	      }
	    } 
	  }
	}
	else {
	     if(k > 0) for (m in 1:min(t-1,k)){
         //  ar Estimation
           if(p > 0) for (i in 1:min(t-1,p)){
              if(q > 0) for(j in 1:min(t-1,q)) {
                 mu[t] = swish(mu0 + h_concatenate[t-i]*phi[i]  + g_1[m+1]*Epsilon[t-m] + Epsilon[t-j]*theta[j]);
			 }
	      }
	   }
   }
   Epsilon[t] = y[t] - mu[t]; 
	for(j in 1:2) { // Current state
	  for(i in 1:2) { // Previous state
	   accumulator[i] = log_alpha[t-1, i] + // Probability from previous obs
	                    log(A[i, j]) + // Transition probability
	                     //(Local) likelihood / evidence for given state
	                     normal_lpdf(Epsilon[t] | 0, sigma[t-1, i]);	
       }							   
	   log_alpha[t, j] = log_sum_exp(accumulator);
	  }
	 probability[t] = softmax(log_alpha[t]);
	 }
}

model {
  d_1   ~ uniform(0.0, 0.5);
  f_1   ~ uniform(0.0, 0.5);
  // HMM components        
  p_remain ~ beta(3.0, 1);  // Forma pouco informativa de dizer que os estados estão meio fixos
  mu0 ~ normal(0, 10);    // priores 
  phi0 ~ normal(0,2);
  theta0 ~ normal(0,2);
  sigma0 ~ cauchy(0, 5); // expected uncond. variance of ts is 1.0
  for(i in 1:N){  
      theta_h_gru[i] ~  normal(0, 0.1); 
      thnv_f_gru[i] ~  normal(0, 0.1); 
      thnw_f_gru[i]~   normal(0, 0.1); 
      thnb_f_gru[i]~   normal(0, (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));  
      thnw_h_gru[i]~   normal(0, 0.1); 
      thnb_h_gru[i]~  normal(0, (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2])); 
      thetv_o_forward[i] ~   normal(0, 0.1);
      thnw_o_forward[i]  ~  normal(0, 0.1);
      thnb_o_forward[i]  ~ normal(0,(sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thnv_d_forward[i]  ~ normal(0, 0.1);
      thnw_d_forward[i]  ~ normal(0, 0.1);
      thnb_d_forward[i]  ~ normal(0,  (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thnv_i_forward[i]   ~ normal(0, 0.1);
      thnw_i_forward[i]   ~ normal(0, 0.1);
      thnb_i_forward[i]   ~ normal(0, (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thnv_f_forward[i]   ~ normal(0, 0.1);
      thnw_f_forward[i]   ~ normal(0, 0.1);
      thnb_f_forward[i]   ~ normal(0, (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thetv_o_backward[i] ~ normal(0, 0.1);
      thnw_o_backward[i]  ~ normal(0, 0.1);
      thnb_o_backward[i]  ~ normal(0, (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thnv_d_backward[i]  ~ normal(0, 0.1);
      thnw_d_backward[i]  ~ normal(0, 0.1);
      thnb_d_backward[i]  ~ normal(0,(sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thnv_i_backward[i]  ~ normal(0, 0.1);
      thnw_i_backward[i]  ~ normal(0, 0.1);
      thnb_i_backward[i]  ~ normal(0,(sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
      thnv_f_backward[i]  ~ normal(0, 0.1);
      thnw_f_backward[i]  ~ normal(0, 0.1);
      thnb_f_backward[i]  ~ normal(0, (sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2]));
		// Likelihood	
      Epsilon[i] ~ normal(0,(sigma[i,1]*probability[i,1]+sigma[i,2]*probability[i,2])); // FARMA likehood
   }
}

generated quantities{
    vector [N+T_forecast] nu_predict;
    vector [N+T_forecast] residuals;
	vector [N+T_forecast] v_g_f_gru_new;
	vector [N+T_forecast] v_h_gru_new;
	vector [N+T_forecast] v_g_o_forward_new;
    vector [N+T_forecast] v_n_d_forward_new;
    vector [N+T_forecast] v_g_i_forward_new;
    vector [N+T_forecast] v_g_f_forward_new;
	vector [N+T_forecast] v_g_o_backward_new;
    vector [N+T_forecast] v_n_d_backward_new;
    vector [N+T_forecast] v_g_i_backward_new;
    vector [N+T_forecast] v_g_f_backward_new;
    vector <lower=0> [2] sigma_new[N+T_forecast];
    vector  <lower= 0.0, upper=1.0>  [N+T_forecast] g_o_forward_new ;
    vector  <lower=-1.0, upper=1.0>  [N+T_forecast] n_d_forward_new ;
    vector  <lower= 0.0, upper=1.0>  [N+T_forecast] g_i_forward_new ;
    vector  <lower=-1.0, upper=1.0>  [N+T_forecast] g_f_forward_new ;
    vector  [N+T_forecast] C_forward_new ;
    vector  <lower=-1.0, upper=1.0>  [N+T_forecast] h_forward_new ;
    vector  <lower= 0.0, upper=1.0>  [N+T_forecast] g_o_backward_new ;
    vector  <lower=-1.0, upper=1.0>  [N+T_forecast] n_d_backward_new ;
    vector  <lower= 0.0, upper=1.0>  [N+T_forecast] g_i_backward_new ;
    vector  <lower= 0.0, upper=1.0>  [N+T_forecast] g_f_backward_new ;
    vector  [N+T_forecast] C_backward_new ;
    vector  <lower=-1.0, upper=1.0>  [N+T_forecast] h_backward_new ;
    vector  <lower=-1.0, upper=1.0>  [N+T_forecast] h_concatenate_new;
	vector  <lower= 0.0, upper=1.0>  [N+T_forecast] g_f_gru_new;
	vector  <lower=-1.0, upper=1.0>  [N+T_forecast] h_gru_new;
	g_f_gru_new[1:N] = g_f_gru;
	h_gru_new[1:N] = h_gru;
	v_g_f_gru_new[1:N] = v_g_f_gru;
	v_h_gru_new[1:N] = v_h_gru;
    g_o_forward_new[1:N] = g_o_forward;
    n_d_forward_new[1:N] = n_d_forward;
    g_i_forward_new[1:N] = g_i_forward;
    g_f_forward_new[1:N] = g_f_forward;
    C_forward_new[1:N] = C_forward;
    h_forward_new[1:N] = h_forward;
	v_g_o_forward_new[1:N] = v_g_o_forward;
	v_n_d_forward_new[1:N] = v_n_d_forward;
	v_g_i_forward_new[1:N] = v_g_i_forward;
	v_g_f_forward_new[1:N] = v_g_f_forward;
    g_o_backward_new[1:N] = g_o_backward;
    n_d_backward_new[1:N] = n_d_backward;
    g_i_backward_new[1:N] = g_i_backward;
    g_f_backward_new[1:N] = g_f_backward;
    C_backward_new[1:N] =   C_backward;
    h_backward_new[1:N] =   h_backward;
	v_g_o_backward_new[1:N] = v_g_o_backward;
	v_n_d_backward_new[1:N] = v_n_d_backward;
	v_g_i_backward_new[1:N] = v_g_i_backward;
	v_g_f_backward_new[1:N] = v_g_f_backward;
    h_concatenate_new[1:N] = h_concatenate;
    nu_predict[1:N] = mu ;
    residuals[1:N] = Epsilon ;
    sigma_new[1:N] = sigma ;
    for( t in (N+1):(N+T_forecast)) {
	  for(ii in 1:2){
        for(i in 1:p){
          for(j in 1:q){
		    for(l in 1:p1) {
			  for(n in 1:p2) {
		       v_g_f_gru_new[t] = (momentum * v_g_f_gru_new[t-1] + epsilon*mean(thnw_f_gru)*h_gru_new[t-1]);
		       v_g_f_gru_new[t] = v_g_f_gru_new[t] + momentum*(v_g_f_gru_new[t] - v_g_f_gru_new[t-1]);
			   g_f_gru_new[t] = inv_logit(v_g_f_gru_new[t] + mean(thnv_f_gru)*residuals[t-1] + mean(thnb_f_gru));
			   v_h_gru_new[t] =  (momentum *v_h_gru_new[t-1] + epsilon*mean(thnw_h_gru)*(g_f_gru_new[t] + h_gru_new[t-1]));
	           v_h_gru_new[t] = v_h_gru_new[t] + momentum*(v_h_gru_new[t] - v_h_gru_new[t-1]);
			   h_gru_new[t] =  tanh(v_h_gru_new[t] + mean(theta_h_gru)*residuals[t-1] + mean(thnb_h_gru));
	           h_gru_new[t]   = (1- g_f_gru_new[t])*h_gru_new[t] +  g_f_gru_new[t] *h_gru_new[t];
			   if (h_gru_new[t - i] > 0) {
                 sigma_new[t,ii] = ReLU(sqrt(omega[ii] + (alpha[i,ii] + psi[i,ii]*(h_gru_new[t - i]*pow(h_gru_new[t - i],2))) + 
				                        beta[j,ii]*pow(sigma_new[t-j,ii],2)+phix[l,ii]*x[t-l] + phix2[n,ii]*x1[t-n]));
               }
			   else {
                 sigma_new[t,ii] = ReLU(sqrt(omega[ii] + (alpha[i,ii] + psi[i,ii]*pow(h_gru_new[t - i],2)) +
                       				 beta[j,ii]*pow(sigma_new[t-j,ii],2)+phix[l,ii]*x[t-l] + phix2[n,ii]*x1[t-n]));
                     }
                   }
		        }
		     }
		  }
	   }	  
       residuals[t] = normal_rng(0, (mean(probability[,1])*sigma_new[t,1]+mean(probability[,2])*sigma_new[t,2]));
	   v_g_o_forward_new[t] = (momentum * v_g_o_forward_new[t-1] + epsilon*mean(thetv_o_forward)*h_forward_new[t-1]);
	   v_g_o_forward_new[t] = v_g_o_forward_new[t] + momentum*(v_g_o_forward_new[t] - v_g_o_forward_new[t-1]);
       v_n_d_forward_new[t] = (momentum * v_n_d_forward_new[t-1] + epsilon*mean(thnv_d_forward)*h_forward_new[t-1]);
       v_n_d_forward_new[t] = v_n_d_forward_new[t] + momentum*(v_n_d_forward_new[t] - v_n_d_forward_new[t-1]);
	   v_g_i_forward_new[t] = (momentum * v_g_i_forward_new[t-1] + epsilon*mean(thnv_i_forward)*h_forward_new[t-1]);
       v_g_i_forward_new[t] = v_g_i_forward_new[t] + momentum*(v_g_i_forward_new[t] - v_g_i_forward_new[t-1]);
	   v_g_f_forward_new[t] = (momentum * v_g_f_forward_new[t-1] + epsilon*mean(thnv_f_forward)*h_forward_new[t-1]);
       v_g_f_forward_new[t] = v_g_f_forward_new[t] + momentum*(v_g_f_forward_new[t] - v_g_f_forward_new[t-1]);
	   g_o_forward_new[t]  = inv_logit(v_g_o_forward_new[t] + mean(thnw_o_forward)*nu_predict[t-1] + mean(thnb_o_forward));
       n_d_forward_new[t]  = tanh(v_n_d_forward_new[t] + mean(thnw_d_forward)*nu_predict[t-1] +  mean(thnb_d_forward));
       g_i_forward_new[t]  = inv_logit(v_g_i_forward_new[t] + mean(thnw_i_forward)*nu_predict[t-1] + mean(thnb_i_forward));
       g_f_forward_new[t]  = inv_logit(v_g_f_forward_new[t] + mean(thnw_f_forward)*nu_predict[t-1] + mean(thnb_f_forward));
       C_forward_new[t] =   g_i_forward_new[t]*n_d_forward_new[t]  + g_f_forward_new[t]*C_forward_new[t-1];
       h_forward_new[t] =   g_o_forward_new[t]*tanh(C_forward_new[t]); 
	   v_g_o_backward_new[t] = (momentum * v_g_o_backward_new[t-1] + epsilon*mean(thetv_o_backward)*h_backward_new[t-1]);
       v_g_o_backward_new[t] = v_g_o_backward_new[t] + momentum*(v_g_o_backward_new[t] - v_g_o_backward_new[t-1]);
	   v_n_d_backward_new[t] = (momentum * v_n_d_backward_new[t-1] + epsilon*mean(thnv_d_backward)*h_backward_new[t-1]);
       v_n_d_backward_new[t] = v_n_d_backward_new[t] + momentum*(v_n_d_backward_new[t] - v_n_d_backward_new[t-1]);
	   v_g_i_backward_new[t] = (momentum * v_g_i_backward_new[t-1] + epsilon*mean(thnv_i_backward)*h_backward_new[t-1]);
       v_g_i_backward_new[t] = v_g_i_backward_new[t] + momentum*(v_g_i_backward_new[t] - v_g_i_backward_new[t-1]);
	   v_g_f_backward_new[t] = (momentum * v_g_f_backward_new[t-1] + epsilon*mean(thnv_f_backward)*h_backward_new[t-1]);
       v_g_f_backward_new[t] = v_g_f_backward_new[t] + momentum*(v_g_f_backward_new[t] - v_g_f_backward_new[t-1]);
	   g_o_backward_new[t]  = inv_logit(v_g_o_backward_new[t] + mean(thnw_o_backward)*nu_predict[t-1]  + mean(thnb_o_backward));
       n_d_backward_new[t]  = tanh(v_n_d_backward_new[t]  + mean(thnw_d_backward)*nu_predict[t-1] + mean(thnb_d_backward));
       g_i_backward_new[t]  = inv_logit(v_g_i_backward_new[t]  + mean(thnw_i_backward)*nu_predict[t-1]   + mean(thnb_i_backward));
       g_f_backward_new[t]  = inv_logit(v_g_f_backward_new[t]  + mean(thnw_f_backward)*nu_predict[t-1]  + mean(thnb_f_backward));
       C_backward_new[t] =   g_i_backward_new[t]*n_d_backward_new[t]  + g_f_backward_new[t]*C_backward_new[t-1] ; 
       h_backward_new[t] =   g_o_backward_new[t]*tanh(C_backward_new[t]);
       h_concatenate_new[t] = (h_forward_new[t] + h_backward_new[t])/2; 
	   if(k1 > 0) for (n in 1:k1){
          for (m in 1:k){
            for(i in 1:p) {
		      for(j in 1:q) {
                      nu_predict[t] = swish(mu0  +  h_concatenate_new[t-i]*phi[i]  + g_1[m+1]*residuals[t-m] + residuals[t-j]*theta[j] + 
					  (alfa11[k]*sin((2*pi()*n/N)*t) + beta11[k]*cos((2*pi()*n/N)*t))*nu_predict[t-i]) ;
	              }
	          }	
          }
	   }
	   else { 
	      for (m in 1:k){
            for(i in 1:p) {
		      for(j in 1:q) {
                nu_predict[t] = swish(mu0  +  h_concatenate_new[t-i]*phi[i]  + g_1[m+1]*residuals[t-m] + residuals[t-j]*theta[j]);
	                  }
	              }
             }		
         }
    }
}

"""
Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X= pystan.StanModel(model_code = Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X)

# Estimação da densidade espectral de potência usando um periodograma
f, Pxx = signal.periodogram(PETR3_SA_Close_Linear_filtred_train.values)

top_3_periods = {}

# Obtendo índices para os 3 valores  mais altos
top3_freq_indices = np.flip(np.argsort(Pxx), 0)[0:3]

# Usando os índices da etapa anterior para obter 3 frequências com maior potência
freqs = f[top3_freq_indices]

# Usar os mesmos índices para obter as ptências também
power = Pxx[top3_freq_indices]

# O período é calculado como o inverso da frequência 
periods = 1 / np.array(freqs)

# preencher o "dicionário" com os  valores calculados

top_3_periods['period1'] = periods[0]
top_3_periods['freq1'] = freqs[0]
top_3_periods['power1'] = power[0]

top_3_periods['period2'] = periods[1]
top_3_periods['freq2'] = freqs[1]
top_3_periods['power2'] = power[1]

top_3_periods['period3'] = periods[2]
top_3_periods['freq3'] = freqs[2]
top_3_periods['power3'] = power[2]

top_3_periods

{'period1': array([inf]), 'freq1': array([0.]), 'power1': array([[0.]]),
 'period2': array([inf]), 'freq2': array([0.]), 'power2': array([[0.]]),
 'period3': array([inf]), 'freq3': array([0.]), 'power3': array([[0.]])}

data = ({ 'k1': 0, 
          'N' : len(PETR3_SA_Close_fractional_difference_Linear_filtred_train),
          'p' : 1,
		  'p1': 1,
		  'p2': 1,
          'q' : 1,
          'k' : 7,
          'T_forecast':len(PETR3_SA_Close_fractional_difference_Linear_filtred_test),
		  'momentum' : 0.90,
		  'epsilon' :  0.90,
          'y':    function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.flatten(),0,1).astype('float16'),
		  'x':    function_to_scale_data(volatility_PETR3_SA_Close_SV_MomentumLSTM.values.flatten(),0,1).astype('float16'),
		  'x1':   function_to_scale_data(volume_PETR3_SA_Close_SV_MomentumLSTM.values.flatten(),0,1).astype('float16')
	   })
	   

control = {}
control['max_treedepth'] = 1
control['adapt_delta'] = 1


fit_Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X =  Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X.sampling(data=data, iter=3500, chains=3, warmup=1200 , thin=1, seed=101, n_jobs = -1,  control=control)

[distfit] >WARNING> Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.
To run all diagnostics call pystan.check_hmc_diagnostics(fit)
[distfit] >WARNING> 6900 of 6900 iterations saturated the maximum tree depth of 1 (100 %)
[distfit] >WARNING> Run again with max_treedepth larger than 1 to avoid saturation
[distfit] >WARNING> Chain 1: E-BFMI = 0.119
[distfit] >WARNING> Chain 3: E-BFMI = 0.147
[distfit] >WARNING> E-BFMI below 0.2 indicates you may need to reparameterize your model

parameters = fit_Bidirectional_MomentumLSTM_Gegenbauer_FARMA_MomentumGRU_MS_GJR_GARCH_X.extract(permuted=True)
parameters.keys()

odict_keys(['f_1', 'd_1', 'p_remain', 'sigma0', 'v_g_f_gru1', 'v_h_gru1', 'v_g_o_forward1', 'v_n_d_forward1', 'v_g_i_for
ward1', 'v_g_f_forward1', 'v_g_o_backward1', 'v_n_d_backward1', 'v_g_i_backward1', 'v_g_f_backward1', 'phi0', 'theta0',
'omega', 'alpha', 'psi', 'beta', 'phix', 'phix2', 'mu0', 'thnv_d_forward', 'thnw_d_forward', 'thnv_i_forward', 'thnw_i_f
orward', 'thetv_o_forward', 'thnw_o_forward', 'thnv_f_forward', 'thnw_f_forward', 'thnb_d_forward', 'thnb_i_forward', 't
hnb_o_forward', 'thnb_f_forward', 'thetv_o_backward', 'thnw_o_backward', 'thnb_o_backward', 'thnv_d_backward', 'thnw_d_b
ackward', 'thnb_d_backward', 'thnv_i_backward', 'thnw_i_backward', 'thnb_i_backward', 'thnv_f_backward', 'thnw_f_backwar
d', 'thnb_f_backward', 'thnv_f_gru', 'thnw_f_gru', 'thnb_f_gru', 'thnw_h_gru', 'thnb_h_gru', 'theta_h_gru', 'A', 'probab
ility', 'log_alpha', 'v_g_o_forward', 'v_n_d_forward', 'v_g_i_forward', 'v_g_f_forward', 'v_g_o_backward', 'v_n_d_backwa
rd', 'v_g_i_backward', 'v_g_f_backward', 'v_g_f_gru', 'v_h_gru', 'h_forward', 'C_forward', 'n_d_forward', 'g_i_forward',
 'g_o_forward', 'g_f_forward', 'h_backward', 'C_backward', 'n_d_backward', 'g_i_backward', 'g_o_backward', 'g_f_backward
', 'h_concatenate', 'g_f_gru', 'h_gru', 'h_forward_0', 'C_forward_0', 'h_backward_0', 'C_backward_0', 'h_0', 'sigma', 'p
hi', 'theta', 'mu', 'Epsilon', 'g_1', 'u_1', 'accumulator', 'nu_predict', 'residuals', 'v_g_f_gru_new', 'v_h_gru_new', '
v_g_o_forward_new', 'v_n_d_forward_new', 'v_g_i_forward_new', 'v_g_f_forward_new', 'v_g_o_backward_new', 'v_n_d_backward
_new', 'v_g_i_backward_new', 'v_g_f_backward_new', 'sigma_new', 'g_o_forward_new', 'n_d_forward_new', 'g_i_forward_new',
 'g_f_forward_new', 'C_forward_new', 'h_forward_new', 'g_o_backward_new', 'n_d_backward_new', 'g_i_backward_new', 'g_f_b
ackward_new', 'C_backward_new', 'h_backward_new', 'h_concatenate_new', 'g_f_gru_new', 'h_gru_new', 'lp__'])

nu_predict = parameters['nu_predict'].mean(axis=0)
nu_predict_all = parameters['nu_predict']

pd.DataFrame(nu_predict_all).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_non_Linear2_all.csv', index=None)

PETR3_SA_Close_forecasted_nonLinear_all2 = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_non_Linear2_all.csv")
PETR3_SA_Close_forecasted_nonLinear_all2_train = np.mean(PETR3_SA_Close_forecasted_nonLinear_all2.values, axis=0)[:-len(PETR3_SA_Close_fractional_difference_Linear_filtred_test)].ravel()
PETR3_SA_Close_forecasted_nonLinear_all2_test =  np.mean(PETR3_SA_Close_forecasted_nonLinear_all2[-len(PETR3_SA_Close_fractional_difference_Linear_filtred_test):].values, axis=1).ravel()

pd.DataFrame(PETR3_SA_Close_forecasted_nonLinear_all2_train).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_nonLinear2_train.csv', index=None)
pd.DataFrame(PETR3_SA_Close_forecasted_nonLinear_all2_test).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_nonLinear2_test.csv', index=None)

PETR3_SA_Close_forecasted_nonLinear2_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear2_train.csv")
PETR3_SA_Close_forecasted_nonLinear2_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear2_test.csv")

#######################
#   In Sample KPI     #
#                     #
#######################                                                                                                                                                            

MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)
RMSE:  0.453078

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel())
print('MAE: %f' % mae)
MAE: 0.400967

coefficient_of_dermination = r2_score(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(),function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: -6.297986  #Pobre peformace  para a amostragem de treino 

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.832636   ,pvalue: 0.000000
#Nenhuma Comrespôndencia a 1% , 3% nem a 5% de confiança


# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values)

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name      score           loc  ... bootstrap_score bootstrap_pass    color
0            t    1.01542       0.41691  ...               0           None  #e41a1c
1     dweibull   1.368804      0.414186  ...               0           None  #e41a1c
2     loggamma   3.571976     -0.378739  ...               0           None  #377eb8
3         beta   3.928113 -162608.69585  ...               0           None  #4daf4a
4      lognorm   4.195181   -160.649762  ...               0           None  #984ea3
5         norm    4.22622      0.409123  ...               0           None  #ff7f00
6        gamma    5.54959     -3.122902  ...               0           None  #ffff33
7   genextreme   8.010925      0.349366  ...               0           None  #a65628
8      uniform  42.363231          -1.0  ...               0           None  #f781bf
9        expon  50.371916          -1.0  ...               0           None  #999999
10      pareto  55.738077     -2.043868  ...               0           None  #999999


# Plot resultados
dist.plot()
plt.show()


####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################

MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
RMSE = math.sqrt(MSE)
print('RMSE: %f'%  RMSE)
RMSE: 0.700240

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
print('MAE: %f' % mae)
MAE: 0.569394

coefficient_of_dermination = r2_score(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: -1.534842 # Pobre peformace para a amostragem de teste

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.200000   ,pvalue: 0.594071
#Comrespôndencia a 1% , 3% e a 5% de confiança

# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel())

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name     score               loc  ... bootstrap_score bootstrap_pass    color
0   genextreme  0.045271         -0.177572  ...               0           None  #e41a1c
1      lognorm  0.049137        -23.959477  ...               0           None  #e41a1c
2        gamma   0.04954        -24.365413  ...               0           None  #377eb8
3            t  0.050359         -0.003202  ...               0           None  #4daf4a
4         norm  0.050365         -0.003222  ...               0           None  #984ea3
5     loggamma  0.050646       -109.630318  ...               0           None  #ff7f00
6     dweibull  0.072965          -0.02713  ...               0           None  #ffff33
7      uniform      0.14              -1.0  ...               0           None  #a65628
8         beta   0.22434              -1.0  ...               0           None  #f781bf
9       pareto  0.570535 -165440650.340521  ...               0           None  #999999
10       expon  0.623426              -1.0  ...               0           None  #999999

# Plot resultados
dist.plot()
plt.show()


##########*/                   *\###########      
#
#   Intervalo de Credibilidade via bootstrap 
#
##########*/                   *\###########

IC valor-z
80   1.282
85   1.440
90   1.645
95   1.960
99   2.576


mean = function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel()
std = np.std(PETR3_SA_Close_forecasted_nonLinear_all2.values, axis=0)[:-len(PETR3_SA_Close_fractional_difference_Linear_filtred_test)]
Credibility_intervals = 1.960 * (std/np.sqrt(len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)))
upper_bands = mean + Credibility_intervals
lower_bands = mean - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1), label = "Amostra observada de treino")
plt.title(' log-retornos a d=  1.0 da PETR3_SA extrapolação da amostragem de treino ')
plt.plot(mean, label = "Amostra estimada")
plt.plot(function_to_scale_data(upper_bands,-1,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands,-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

# Figure_2-1_modelo02_in_sample_train_CI.png


mean = function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel() 
std = np.std(PETR3_SA_Close_forecasted_nonLinear_all2[-len(PETR3_SA_Close_fractional_difference_Linear_filtred_test):].values, axis=1)
Credibility_intervals = 1.960 * (std/np.sqrt(len(PETR3_SA_Close_fractional_difference_Linear_filtred_test)))
upper_bands = mean + Credibility_intervals
lower_bands = mean - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1), label = "Amostra observada de teste")
plt.title(' log-retornos a d=  1.0 da PETR3_SA extrapolação da amostragem de teste ')
plt.plot(mean, label = "Amostra estimada")
plt.plot(function_to_scale_data(upper_bands,-1,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands,-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_2-1_modelo02_out_sample_test_CI.png


sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra observada de treino")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel(),bins=10,color='blue', label = "Amostra estimada de treino")
plt.suptitle('Histograma para amostras estimadas  e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_2-1_modelo02_in_sample_train_Histogram

sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra observado de teste")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel(),bins=10,color='blue', label = "Amostra estimada de teste")
plt.suptitle('Histograma para amostras estimadas de teste e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_2-1_modelo02_out_sample_test_Histogram.png

def qqplot(x, y, quantiles=None, interpolation='nearest', ax=None, rug=False,
           rug_length=0.05, rug_kwargs=None, **kwargs):
    """Draw a quantile-quantile plot for `x` versus `y`.
   
	Implementation : Artem, M. (2017) https://stats.stackexchange.com/questions/403652/two-sample-quantile-quantile-plot-in-python
	
	Buja, A., Cook, D. Hofmann, H., Lawrence, M. Lee, E.-K., Swayne, D.F and Wickham, H. (2009) 
	Statistical Inference for exploratory data analysis and model diagnostics Phil. Trans. R. Soc. A 2009 367, 
	4361-4383 doi: 20.02098/rsta.2009.0120
    
	Parameters
    ----------
    x, y : array-like
        One-dimensional numeric arrays.

    ax : matplotlib.axes.Axes, optional
        Axes on which to plot. If not provided, the current axes will be used.

    quantiles : int or array-like, optional
        Quantiles to include in the plot. This can be an array of quantiles, in
        which case only the specified quantiles of `x` and `y` will be plotted.
        If this is an int `n`, then the quantiles will be `n` evenly spaced
        points between 0 and 1. If this is None, then `min(len(x), len(y))`
        evenly spaced quantiles between 0 and 1 will be computed.

    interpolation : {‘Linear’, ‘lower’, ‘higher’, ‘midpoint’, ‘nearest’}
        Specify the interpolation method used to find quantiles when `quantiles`
        is an int or None. See the documentation for numpy.quantile().

    rug : bool, optional
        If True, draw a rug plot representing both samples on the horizontal and
        vertical axes. If False, no rug plot is drawn.

    rug_length : float in [0, 1], optional
        Specifies the length of the rug plot lines as a fraction of the total
        vertical or horizontal length.

    rug_kwargs : dict of keyword arguments
        Keyword arguments to pass to matplotlib.axes.Axes.axvline() and
        matplotlib.axes.Axes.axhline() when drawing rug plots.

    kwargs : dict of keyword arguments
        Keyword arguments to pass to matplotlib.axes.Axes.scatter() when drawing
        the q-q plot.
    """
    # Get current axes if none are provided
    if ax is None:
        ax = plt.gca()
    if quantiles is None:
        quantiles = min(len(x), len(y))
    # Compute quantiles of the two samples
    if isinstance(quantiles, numbers.Integral):
        quantiles = np.linspace(start=0, stop=1, num=int(quantiles))
    else:
        quantiles = np.atleast_1d(np.sort(quantiles))
    x_quantiles = np.quantile(x, quantiles, interpolation=interpolation)
    y_quantiles = np.quantile(y, quantiles, interpolation=interpolation)
    # Draw the rug plots if requested
    if rug:
        # Default rug plot settings
        rug_x_params = dict(ymin=0, ymax=rug_length, c='gray', alpha=0.1)
        rug_y_params = dict(xmin=0, xmax=rug_length, c='gray', alpha=0.1)
        # Override default setting by any user-specified settings
        if rug_kwargs is not None:
            rug_x_params.update(rug_kwargs)
            rug_y_params.update(rug_kwargs)
        # Draw the rug plots
        for point in x:
            ax.axvline(point, **rug_x_params)
        for point in y:
            ax.axhline(point, **rug_y_params)
    # Draw the q-q plot
    ax.scatter(x_quantiles, y_quantiles, **kwargs)

# Draw quantile-quantile plot
plt.figure()
qqplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel(),function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas de treino')
plt.ylabel('QQplot realidade observada de treino')
plt.title('QQplots para as duas amostras  de treino')
plt.show()

#Figure_2-1_modelo02_in_sample_train_QQplots

qqplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas de teste')
plt.ylabel('QQplot realidade observada de teste')
plt.title('QQplots para as duas amostras de teste')
plt.show()	

#Figure_2-1_modelo02_out_sample_teste_QQplots
	
######################################################################################################
# Para a amostragem de treino Caudas leves                                                           #
# Para a amostragem de teste Bimodal com caudas leves                                                #
######################################################################################################

#######/                               \#######
#                                             #
#    KPI de Risco.Binômio de risco vs retorno #
#                                             #
#######/                               \#######

mean_02_treino = np.mean(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel())
std_02_treino  = np.std(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train.values,-1,1).ravel())
print('retorno_modelo02_treino: %f   ,risco_modelo02_treino: %f' % (mean_02_treino,std_02_treino))
#retorno_modelo02_treino: 0.021994   ,risco_modelo02_treino: 0.168800

mean_obs_treino = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
std_obs_treino  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
print('retorno_obs_treino: %f   ,risco_obs_treino: %f' % (mean_obs_treino,std_obs_treino))
#retorno_obs_treino: 0.409123   ,risco_obs_treino: 0.167715

mean_02_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1))
std_02_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test.values,-1,1))
print('retorno_modelo02_extrapolado: %f   ,risco_modelo02_extrapolado: %f' % (mean_02_extrapolado,std_02_extrapolado))
#retorno_modelo02_extrapolado: -0.003222   ,risco_modelo02_extrapolado: 0.522471

mean_obs_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
std_obs_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
print('retorno_obs_extrapolado: %f   ,risco_obs_extrapolado: %f' % (mean_obs_extrapolado,std_obs_extrapolado))
#retorno_obs_extrapolado: -0.056525   ,risco_obs_extrapolado: 0.438676


#########*/                                              *\###########
#
#      */\*Narx Aproximações Numéricas   
#                  recursivas via ANN  */\*
#                  
#      Non Linear Auto-Regression *Narx* 
# 1DCasualCNN_Attention_Bidirectional_MS_MomentumLSTM_Encoder_Decoder
#
#########*/                                               *\###########


# dividir uma sequência multivariada em amostras (Transformar matriz coluna em tensor)
def split_sequences(sequences, n_steps):
   X, y = [] , []
   for i in range(len(sequences)):
      # encontrar o fim deste padrão
      end_ix = i + n_steps
      # verificar se estamos além do conjunto de dados
      if end_ix > len(sequences)-1:
         break
      # reunir partes de entrada e saída do padrão
      seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]
      X.append(seq_x)
      y.append(seq_y)
   return np.array(X) ,np.array(y)
	

###########################################################################  
#  Para dados de séries temporais é melhor que a divisão                  #
#  seja  mais concentrada para a amostragem de treino devido a            #
#  interdependência temporal                                              # 
#  temporal                                                               #
###########################################################################

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min

volatility_PETR3_SA_Close_SV_MomentumLSTM = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volatility_PETR3_SA_Close_SV_MomentumLSTM.csv")
volume_PETR3_SA_Close_SV_MomentumLSTM = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volume_PETR3_SA_Close_SV_MomentumLSTM.csv")
PETR3_SA_Volume_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Volume_Linear_filtred.csv")
PETR3_SA_Close_fractional_difference_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_fractional_difference_Linear_filtred.csv")
PETR3_SA_Close_fractional_difference_Linear_filtred = PETR3_SA_Close_fractional_difference_Linear_filtred[-len(PETR3_SA_Volume_Linear_filtred):]

for t in range(1, len(volatility_PETR3_SA_Close_SV_MomentumLSTM)):
   PETR3_SA_Close_SV_MomentumLSTM01 = np.random.normal(0, volatility_PETR3_SA_Close_SV_MomentumLSTM.iloc[t], len(volatility_PETR3_SA_Close_SV_MomentumLSTM)) #GJR-GARCH
   PETR3_SA_Close_SV_MomentumLSTM02  = np.random.normal(0,volume_PETR3_SA_Close_SV_MomentumLSTM.iloc[t], len(volume_PETR3_SA_Close_SV_MomentumLSTM))         #GJR-GARCH

PETR3_SA_Close_SV_MomentumLSTM01 = pd.DataFrame(PETR3_SA_Close_SV_MomentumLSTM01)
PETR3_SA_Close_SV_MomentumLSTM02 = pd.DataFrame(PETR3_SA_Close_SV_MomentumLSTM02)

PETR3_SA_Close_and_its_own_volatility_and_volume = np.concatenate((function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred.values,0,1),
function_to_scale_data(PETR3_SA_Close_SV_MomentumLSTM01,0,1),function_to_scale_data(PETR3_SA_Close_SV_MomentumLSTM02.values,0,1),function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred.values,0,1)), axis=1).astype('float16')

PETR3_SA_Close_and_its_own_volatility_and_volume = pd.DataFrame(PETR3_SA_Close_and_its_own_volatility_and_volume)
PETR3_SA_Close_and_its_own_volatility_and_volume_train = PETR3_SA_Close_and_its_own_volatility_and_volume[:-30]
PETR3_SA_Close_and_its_own_volatility_and_volume_test = PETR3_SA_Close_and_its_own_volatility_and_volume[-30:]

PETR3_SA_Close_fractional_difference_Linear_filtred_train  =  PETR3_SA_Close_fractional_difference_Linear_filtred[:-30]
PETR3_SA_Close_fractional_difference_Linear_filtred_test   =  PETR3_SA_Close_fractional_difference_Linear_filtred[-30:]

n_steps = 1
X_train, y_train = split_sequences(PETR3_SA_Close_and_its_own_volatility_and_volume_train.values.astype('float16'), n_steps)
X_test, y_test = split_sequences(PETR3_SA_Close_and_its_own_volatility_and_volume_test.values.astype('float16'), n_steps)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
X_train = torch.FloatTensor(X_train).to(device)
y_train=  torch.FloatTensor(y_train).to(device)  

X_test = torch.FloatTensor(X_test).to(device)
y_test=  torch.FloatTensor(y_test).to(device)

def swish(x):
   return (x * F.sigmoid(x*0.000001)).to(device)

	
class TimeDistributed(nn.Module):
   "Aplica um módulo sobre tdim (time dimension) de forma idêntica para cada etapa" 
   def __init__(self, module, low_mem=False, tdim=1):
      super(TimeDistributed, self).__init__()
      self.module = module
      self.low_mem = low_mem
      self.tdim = tdim   
   def forward(self, *args, **kwargs):
      "matriz x com formato:(batch size(Narx p),seq_len(linhas),channels(colunas),width(eixo x),height(eixo y))"
      if self.low_mem or self.tdim!=1: 
         return self.low_mem_forward(*args)
      else:
         #only support tdim=1
         inp_shape = args[0].shape
         bs, seq_len = inp_shape[0], inp_shape[1]   
         out = self.module(*[x.contiguous().view(bs*seq_len, *x.shape[2:]) for x in args], **kwargs)
         out_shape = out.shape
         return out.view(bs, seq_len,*out_shape[1:])
   def low_mem_forward(self, *args, **kwargs):                                 
      "matriz x com as seguintes dimensões:bs(batch_size/timesteps),seq_len(Numero de linhas),channels(Numero de colunas),width(eixo x de timesteps),height(eixo y de timesteps))"
      tlen = args[0].shape[self.tdim]
      args_split = [torch.unbind(x, dim=self.tdim) for x in args]
      out = []
      for i in range(tlen):
         out.append(self.module(*[args[i] for args in args_split]), **kwargs)
      return torch.stack(out,dim=self.tdim)
   def __repr__(self):
      return f'TimeDistributed({self.module})'


		
class MyDropout(nn.Module):
   def __init__(self, p):
      super(MyDropout, self).__init__()
      self.p = p
      # multiplicador é 1/(1-p). Definir o multiplicador para 0 quando p=1 para evitar erros
      if self.p < 1:
         self.multiplier_ = 1.0 / (1.0-p)
      else:
         self.multiplier_ = 0.0
   def forward(self, input):
      # com model.eval(), não usar dropout
      if not self.training:
         return input
      # A quantidade de ensaios/ensaios de Bernoulli(1-p) é dada pela cardinalidade da matrix de entrada (input.shape)
      selected_ = torch.Tensor(input.shape).uniform_(0,1)>self.p
      # Suporte a  CPU ou GPU.
      if input.is_cuda:
         selected_ = Variable(selected_.type(torch.cuda.FloatTensor), requires_grad=False)
      else:
         selected_ = Variable(selected_.type(torch.FloatTensor), requires_grad=False)         
      # Multiplicando a saída pelo multiplicador conforme descrito no artigo [1]
      return torch.mul(selected_,input) * self.multiplier_



class CausalConv1d(nn.Module):
    """
    Implementação : A causal 1D convolution.An Introduction to Deep Generative
	Tomczak, J. M. (2022). Deep Generative Modeling. Springer Nature
	https://jmtomczak.github.io/blog/2/2_ARM.html
	[SRINIVASAMURTHY, 2018] Srinivasamurthy, Ravisutha Sakrepatna,
	"Understanding 1D Convolutional Neural Networks Using Multiclass Time-Varying Signals"(2018). All Theses. 2911.
    https://tigerprints.clemson.edu/all_theses/2911
	[LeCUM ET AL. 1998]  Yann LeCum , Léon Botton, Yoshua Bengio, Patrick Haffner (1998) Gradient-Based Learning Applied to Documment Recognition , 1998. IEEE , November 1998.
    """
    def __init__(self, in_channels, out_channels, kernel_size, dilation, A=False, bias=True ,**kwargs):
        super(CausalConv1d, self).__init__()
        # A ideia geral é a seguinte: Pegamos o PyTorch Conv1D embutido.
        # Então, devemos escolher um preenchimento adequado, porque devemos garantir que a convolucional seja causal.
        # Eventualmente, devemos remover alguns elementos finais da saída, porque simplesmente não precisamos deles!
        # Como CausalConv1D ainda é uma convolução, devemos definir o tamanho do kernel, dilatação e se é
        # opção A (A=Verdadeiro) ou opção B (A=Falso). Lembre-se que brincando com a dilatação podemos ampliar
        # o tamanho da memória.
        # atributos:
        self.kernel_size = kernel_size
        self.dilation = dilation
        self.bias = bias
        self.A = A
        self.padding = (kernel_size - 1) * dilation + A * 1
        # module:
        self.conv1d = torch.nn.Conv1d(in_channels, out_channels,
                                      kernel_size, stride=1,
                                      padding=0, #notice we will do padding by ourself in the forward pass!
                                      dilation=dilation, bias=True,
                                      **kwargs)
    def forward(self, x):
        # Nós fazemos preenchimento apenas da esquerda! Esta é uma implementação mais eficiente.
        x = torch.nn.functional.pad(x, (self.padding, 0))
        conv1d_out = self.conv1d(x)
        if self.A:
            # Lembre-se, não podemos depender do componente atual, portanto, o último elemento é removido.
            return conv1d_out[:, :, : -1]
        else:
            return conv1d_out

			

		 				  
class Momentum_MS_LSTMCell(nn.Module):
           """
           [HOCHREITER & SCHMIDHUBER, 1997] Hochreiter, Sepp & Schmidhuber, Jürgen. (1997).
           Long Short-term Memory. Neural computation. 9. 1735-80. 20.1162/neco.1997.9.8.1735.
           [GOODFELLOW , BENGIO & COURVILLE, 2016] Goodfellow, I., Y. Bengio & A.Courville (2016),
           Deep Learning, 2016.MIT Press , Cambridge, MA, USA.
           [NGUYEN   ET AL, 2020] Nguyen. Tam M, Baraniuk .. Richard G,   Bertozzi. Andrea L, Osher. Stanley J, Wang, Bao. (2020).
           MomentumRNN: Integrating Momentum into Recurrent Neural Networks: Advances in Neural Information Processing Systems (NeurIPS) 2020
           https://doi.org/20.48550/arXiv.2006.06919
           Implementação vetorizada do modelo LSTM por Pierre Esposito  https://github.com/piEsposito/pytorch-LSTM-by-hand
           Ilhan F, Karaahmetoglu O, Balaban I, Kozat SS. Markovian RNN: An Adaptive Time Series Prediction Network With HMM-Based Switching for Nonstationary Environments. 
           IEEE Trans Neural Netw Learn Syst. 2023 Feb;34(2):715-728. doi: 10.1109/TNNLS.2021.3100528. Epub 2023 Feb 3. PMID: 34370675.
           """
           def __init__(self, input_sz, hidden_sz, mu, epsilon,k,bias=True):
              super().__init__()
              self.input_sz = input_sz
              self.hidden_size = hidden_sz
              self.mu = mu
              self.epsilon = epsilon
              self.k = k     # number of RNN cells
              self.W_x_gates = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4 * self.k))
              self.W_xz = nn.Parameter(torch.FloatTensor(input_sz, self.k))  # affine transform x_t -> (z_1, z_2, , z_K)
              self.U_x_gates = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4 * self.k))
              self.W_hz = nn.Parameter(torch.FloatTensor(hidden_sz, self.k))  # affine transform   h -> (z_1, z_2, , z_K)
              if bias:
               self.U_bias = nn.Parameter(torch.Tensor(hidden_sz * 4* self.k))
               self.W_bias = nn.Parameter(torch.Tensor(hidden_sz * 4* self.k))
               self.b_xz = nn.Parameter(torch.FloatTensor(self.k))  # size = (1, k)
               self.b_hz = nn.Parameter(torch.FloatTensor(self.k))  # size = (1, k)
              else:
               self.register_parameter('U_bias', None)
               self.register_parameter('W_bias', None)
               self.register_parameter('b_xz', None)
               self.register_parameter('b_hz', None)
              self.init_weights()
           def init_weights(self):
              stdv = 1.0 / math.sqrt(self.hidden_size)
              for weight in self.parameters():
                 weight.data.uniform_(-stdv, stdv)
           def forward(self, x, init_states=None):
              """Assume que  x é de forma (batch(timesteps), sequence(linhas), feature(colunas))"""
              bs,seq_sz, _ = x.size()
              hidden_seq = []
              v = torch.zeros(bs, 4*self.hidden_size * self.k).requires_grad_().to(device)
              if init_states is None:
                h_t, c_t   = (Variable(nn.init.kaiming_normal_((torch.zeros(bs, self.hidden_size )), mode='fan_out', nonlinearity='relu')).requires_grad_().to(device),  Variable(nn.init.kaiming_normal_((torch.zeros(bs, self.hidden_size)), mode='fan_out', nonlinearity='relu')).requires_grad_().to(device))
              else:
                h_t, c_t = init_states
              N = h_t.size(0)  # batch size
              # expand (repeat) bias for batch processing
              batch_b_xz = (self.b_xz.unsqueeze(0).expand(N, *self.b_xz.size()))   # size = (N, k)
              batch_b_hz = (self.b_hz.unsqueeze(0).expand(N, *self.b_hz.size()))  # size = (N, k)
              batch_b_4gates = (self.U_bias.unsqueeze(0).expand(N, *self.U_bias.size()))  # size = (N, 4* m* k)
              HS = self.hidden_size
              for t in range(seq_sz):
                 x_t = x[:, t, :]
                 ''' logit encoder: logit_z = (W1 * x_t + b1) + (W2 * h_{t-1} + b2) in R^K '''
                 logit_z = torch.addmm(batch_b_xz, x_t,self.W_xz)  + torch.mm(h_t, self.W_hz)
                 # probability q_z(x_t, h_{t-1}) in R^K
                 q_z = F.softmax(logit_z, dim=1)
                 z = torch.FloatTensor(N, self.k).zero_().to(device) 
                 z.scatter_(1, torch.max(q_z, dim=1)[1].view(N, 1), 1) # find which position is max
                 # agrupar os cálculos em uma única multiplicação de matriz
                 vy = self.mu*v+ self.epsilon*(x_t @ self.W_x_gates + self.W_bias)
                 gates = vy + (h_t @ self.U_x_gates + self.U_bias)
                 i_t, f_t, g_t, o_t = gates.chunk(4, 1)
                 i_t = torch.sigmoid(i_t)
                 f_t = torch.sigmoid(f_t)
                 g_t = torch.tanh(g_t)
                 o_t = torch.sigmoid(o_t)
                 c_t = c_t.repeat(1, self.k)
                 c_t = f_t * c_t + i_t * g_t
                 h_t = o_t * torch.tanh(c_t)
                 # reshape C_t & H_t has dim = (N, k, m)   (hidden_size = m)
                 c_t = c_t.view([N, self.k, self.hidden_size])
                 h_t = h_t.view([N, self.k, self.hidden_size])
				 # sum over K LSTMs (like K ensemble) to become 1 LSTM cell & hidden state: (h_t , c_t)
                 h_t = torch.einsum('nkm,nk->nm', (h_t, z))   # size= (batch, output-dim), no time
                 c_t = torch.einsum('nkm,nk->nm', (c_t, z))   # size= (batch, output-dim), no time
                 hidden_seq.append(h_t.unsqueeze(0))
              hidden_seq = torch.cat(hidden_seq, dim=0)
              # De (linhas, batch(timesteps), colunas) para (batch(timesteps), linhas, colunas)
              hidden_seq = hidden_seq.transpose(0, 1).contiguous()
              return hidden_seq, h_t, c_t


		 
class LinearMomentum(nn.Module):
      """
      [GOODFELLOW , BENGIO & COURVILLE, 2016] Goodfellow, I., Y. Bengio & A.Courville (2016),
      Deep Learning, 2016.MIT Press ,, Cambridge, MA, USA.
      [NGUYEN   ET AL, 2020] Nguyen. Tam M, Baraniuk .. Richard G,   Bertozzi. Andrea L, Osher. Stanley J, Wang, Bao. (2020).
      MomentumRNN: Integrating Momentum into Recurrent Neural Networks: Advances in Neural Information Processing Systems (NeurIPS) 2020
      https://doi.org/20.48550/arXiv.2006.06919
      Implementação vetorizada do modelo LSTM por Pierre Esposito  https://github.com/piEsposito/pytorch-lstm-by-hand
      """
      def __init__(self, input_features, out_features, mu, epsilon, bias=True):
         super().__init__()
         self.input_features = input_features
         self.out_features = out_features
         self.mu = mu
         self.epsilon = epsilon
         self.weight = nn.Parameter(torch.Tensor(out_features,input_features))
         if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
         else :
            self.register_parameter('bias', None)
         self.init_weights()
      def init_weights(self):
         for weight in self.parameters():
            #weight.data.uniform_(-1/math.sqrt(self.input_features), 1/math.sqrt(self.input_features))
            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5),mode='fan_out', nonlinearity='relu').to(device)
      def forward(self, x):
         """
         y[i,j] == x[i,0] * m.weight[j,0] + x[i,1] * m.weight[j,1] + m.bias[j]
            where i is in interval [0, batch_size) and j in [0, out_features) .
         """
         v = torch.zeros(self.out_features).requires_grad_().to(x.device)
         y = self.mu*v + self.epsilon*(x.matmul(self.weight.t()) + self.bias) # y = mu*v + epsilon*(x*W^T + b)
         return y
		 
		 

class CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder(nn.Module):
                               def __init__(self):
                                   super(CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder, self).__init__()
                                   """ ARQUITETURA
                                   Implementação portada/baseada em  Andreas Holm Nielsen :
                                   https://holmdk.github.io/2020/04/02/video_prediction.html
                                   # Encoder 1D CNN (Parametrizar as dependências do tensor no tempo.Sua Volatilidade)

                                   # Encoder (LSTM)
                                   # Encoder Vector (últimos estados latentes do encoder)
                                   # Decoder (LSTM) - assume a matriz produzida pelo encoder como impute
                                   
								   """
                                   self.TimeDistributed = TimeDistributed(nn.Module)
                                   self.qkv = X_train.shape[2]
                                   self.query = TimeDistributed(LinearMomentum(X_train.shape[2],X_train.shape[2],0.9,0.9,True),tdim=1).to(device)
                                   self.key = TimeDistributed(LinearMomentum(X_train.shape[2],X_train.shape[2],0.9,0.9, True),tdim=1).to(device)
                                   self.value = TimeDistributed(LinearMomentum(X_train.shape[2],X_train.shape[2],0.9,0.9,  True),tdim=1).to(device)
                                   self.attn = TimeDistributed(LinearMomentum(X_train.shape[2],X_train.shape[2],0.9,0.9,True),tdim=1).to(device)
                                   self.scale = math.sqrt(self.qkv)
                                   self.bn1 =  nn.BatchNorm2d(1,affine=True).to(device)
                                   self.bn2 =  nn.BatchNorm2d(1,affine=True).to(device)
                                   self.bn3 =  nn.BatchNorm2d(1,affine=True).to(device)
                                   self.bn4 =  nn.BatchNorm2d(1,affine=True).to(device)
                                   self.bn5 =  nn.BatchNorm2d(1,affine=True).to(device)
                                   self.num_layers = X_train.shape[1]
                                   self.encoder_1_CNN =  TimeDistributed(CausalConv1d(X_train.shape[1], X_train.shape[2]  , X_train.shape[1], X_train.shape[1] ,True, True), tdim=1).to(device)
                                   self.encoder_2_CNN =  TimeDistributed(CausalConv1d(X_train.shape[2], X_train.shape[2]  , X_train.shape[1], X_train.shape[1] ,True, True), tdim=1).to(device)
                                   encoder_1_LSTM_forward = []
                                   encoder_2_LSTM_forward = []
                                   decoder_1_LSTM_forward = []
                                   decoder_2_LSTM_forward = []
                                   encoder_1_LSTM_backward =[]
                                   encoder_2_LSTM_backward = []
                                   decoder_1_LSTM_backward = []
                                   decoder_2_LSTM_backward = []
                                   for i in range(0, self.num_layers):
                                     encoder_1_LSTM_forward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2], 0.90, 0.90,X_train.shape[2], True))
                                     encoder_2_LSTM_forward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2], 0.90,0.90,X_train.shape[2], True))
                                     decoder_1_LSTM_forward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2], 0.90,0.90,X_train.shape[2], True))
                                     decoder_2_LSTM_forward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2], 0.90,0.90,X_train.shape[2], True))
                                     encoder_1_LSTM_backward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2],0.90,0.90,X_train.shape[2], True))
                                     encoder_2_LSTM_backward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2],0.90,0.90,X_train.shape[2], True))
                                     decoder_1_LSTM_backward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2],0.90,0.90,X_train.shape[2], True))
                                     decoder_2_LSTM_backward.append(Momentum_MS_LSTMCell(X_train.shape[2],  X_train.shape[2],0.90,0.90,X_train.shape[2], True))
                                   self.encoder_1_LSTM_forward = nn.ModuleList(encoder_1_LSTM_forward)
                                   self.encoder_1_LSTM_forward = nn.ModuleList(encoder_1_LSTM_forward)
                                   self.encoder_2_LSTM_forward = nn.ModuleList(encoder_2_LSTM_forward)
                                   self.decoder_1_LSTM_forward = nn.ModuleList(decoder_1_LSTM_forward)
                                   self.decoder_2_LSTM_forward = nn.ModuleList(decoder_2_LSTM_forward)
                                   self.encoder_1_LSTM_backward = nn.ModuleList(encoder_1_LSTM_backward)
                                   self.encoder_2_LSTM_backward = nn.ModuleList(encoder_2_LSTM_backward)
                                   self.decoder_1_LSTM_backward = nn.ModuleList(decoder_1_LSTM_backward)
                                   self.decoder_2_LSTM_backward = nn.ModuleList(decoder_2_LSTM_backward)
                                   self.fc1 = TimeDistributed(LinearMomentum(X_train.shape[2], 1,0.9,0.9,  True), tdim=1).to(device)
                                   self.fc2 = TimeDistributed(LinearMomentum(X_train.shape[2], 1,0.9,0.9,  True), tdim=1).to(device)
                                   self.dropout = TimeDistributed(MyDropout(p=0.50), tdim=1).to(device)
                               def encoder_decoder_attention(self, x,  h_t_forward, c_t_forward,h_t_backward, c_t_backward):
                                   x = self.bn1(self.encoder_1_CNN(x.unsqueeze(1)))
                                   x = self.bn2(self.encoder_2_CNN(x))
                                   x = torch.nn.functional.max_pool2d(x.squeeze(1), 1)
                                   outputs = []
                                   for layer_idx in range(self.num_layers):
                                   # encoder
                                     yhat_forward, h_t1_forward, c_t1_forward = self.encoder_1_LSTM_forward[layer_idx](x.squeeze(1),
                                                                        (h_t_forward, c_t_forward))
                                     yhat_forward, h_t2_forward, c_t2_forward = self.encoder_2_LSTM_forward[layer_idx](yhat_forward,
                                                                        (h_t1_forward, c_t1_forward))
                                     yhat_backward, h_t1_backward, c_t1_backward = self.encoder_1_LSTM_backward[layer_idx](torch.flip(x,[0,1]).squeeze(1),
                                                                        (h_t_backward, c_t_backward))
                                     yhat_backward, h_t2_backward, c_t2_backward = self.encoder_2_LSTM_backward[layer_idx](yhat_backward,
                                                                        (h_t1_backward, c_t1_backward))
                                   # encoder_vector
                                   encoder_vector_forward  = self.dropout(yhat_forward)
                                   encoder_vector_backward = self.dropout(yhat_backward)
                                   # decoder
                                   for layer_idx in range(self.num_layers):
                                     yhat_forward,  h_t3_forward, c_t3_forward = self.decoder_1_LSTM_forward[layer_idx](encoder_vector_forward, (h_t2_forward,c_t2_forward))
                                     yhat_backward, h_t3_backward, c_t3_backward = self.decoder_1_LSTM_backward[layer_idx](encoder_vector_backward,(h_t2_backward ,c_t2_backward))
                                     yhat_forward,  h_t4_forward, c_t4_forward = self.decoder_2_LSTM_forward[layer_idx](yhat_forward, (h_t3_forward, c_t3_forward))
                                     yhat_backward, h_t4_backward, c_t4_backward = self.decoder_2_LSTM_backward[layer_idx](yhat_backward,(h_t3_backward, c_t3_backward))
                                     encoder_vector_forward  = self.dropout(yhat_forward)
                                     encoder_vector_backward = self.dropout(yhat_backward)
                                     outputs += [(yhat_forward + yhat_backward)/2]  # predictions
                                   x = torch.stack(outputs , dim=1)
                                   Q, K, V = self.query(x), self.key(x), self.value(x)
                                   dot_product = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale
                                   scores = torch.softmax(dot_product, dim=-1)
                                   scaled_x = torch.matmul(scores, V)+ x
                                   x = self.bn3(self.attn(scaled_x))
                                   x = torch.nn.functional.max_pool2d(x, 1)
                                   return x
                               def _init_hidden(self,x):
                                   init_states_forward = []
                                   init_states_backward = []
                                   h0 = Variable(nn.init.kaiming_normal_((torch.zeros(x.size(0),x.size(2))), mode='fan_out', nonlinearity='relu')).requires_grad_().to(device)
                                   c0 = Variable(nn.init.kaiming_normal_((torch.zeros(x.size(0),x.size(2))), mode='fan_out', nonlinearity='relu')).requires_grad_().to(device)
                                   h1 = Variable(nn.init.kaiming_normal_((torch.zeros(x.size(0),x.size(2))), mode='fan_out', nonlinearity='relu')).requires_grad_().to(device)
                                   c1 = Variable(nn.init.kaiming_normal_((torch.zeros(x.size(0),x.size(2))), mode='fan_out', nonlinearity='relu')).requires_grad_().to(device)
                                   for i in range(self.num_layers):
                                     init_states_forward.append((h0,c0))
                                     init_states_backward.append((h1,c1))
                                   return init_states_forward , init_states_backward
                               def forward(self,x):
                                   """
                                   Paramêtros
                                   ----------
                                   input_tensor:
                                   3-D Tensor de seguintes dimensões  (t, b, c)  # time(linhas da matriz),  batch( timesteps), channel( colunas da matriz)
                                   """
                                   # inicializar os estados latentes
                                   hidden_state_forward,hidden_state_backward  = self._init_hidden(x)
                                   for layer_idx in range(self.num_layers):
                                      h_t_forward, c_t_forward = hidden_state_forward[layer_idx]
                                      h_t_backward, c_t_backward = hidden_state_backward[layer_idx]
                                   # autoencoder forward
                                   x  = self.encoder_decoder_attention(x, h_t_forward, c_t_forward,h_t_backward, c_t_backward)
                                   x  = F.max_pool2d(self.bn4(self.fc1(x)),1)
                                   x  = swish(F.max_pool2d(self.bn5(self.fc2(x.permute(0,1,3,2))),1))
                                   return x
								 			 
#########*/                                *\###########
#
#  */\*Narx ~ distribuições a priori de pesos  */\*
#
#########*/                                 *\##########


def initialize_weights(m):
   if type(m) in [LinearMomentum] :
      nn.init.kaiming_uniform_(m.weight ,mode='fan_out', nonlinearity='relu')
      nn.init.uniform_(m.bias, -1.0, 1.0)
   elif type(m) in [Momentum_MS_LSTMCell,CausalConv1d]:
            for name, param in m.named_parameters():
               if 'bias' in name:
                  nn.init.normal_(param ,0,0.1)
               elif 'weight' in name:
                  nn.init.kaiming_normal_(param , mode='fan_out', nonlinearity='relu')
						
						

#########*/    *\###########
#
#    Reprodutibilidade
#
#########*/     *\###########

		   
def seed_everything(seed):
   torch.manual_seed(seed)
   torch.cuda.manual_seed(seed)
   torch.cuda.manual_seed_all(seed)
   np.random.seed(seed)
   torch.backends.cudbenchmark = False
   torch.backends.cuddeterministic = True
	
						
#########*/                       *\###########
#
#           Narx  - Parametric Assembling 
#
#########*/                        *\###########

CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder = CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder().to(device)
CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder.apply(initialize_weights)

CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder(
  (TimeDistributed): TimeDistributed(<class 'torch.nn.modules.module.Module'>)
  (query): TimeDistributed(LinearMomentum())
  (key): TimeDistributed(LinearMomentum())
  (value): TimeDistributed(LinearMomentum())
  (attn): TimeDistributed(LinearMomentum())
  (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (encoder_1_CNN): TimeDistributed(CausalConv1d(
    (conv1d): Conv1d(1, 3, kernel_size=(1,), stride=(1,))
  ))
  (encoder_2_CNN): TimeDistributed(CausalConv1d(
    (conv1d): Conv1d(3, 3, kernel_size=(1,), stride=(1,))
  ))
  (encoder_1_LSTM_forward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (encoder_2_LSTM_forward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (decoder_1_LSTM_forward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (decoder_2_LSTM_forward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (encoder_1_LSTM_backward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (encoder_2_LSTM_backward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (decoder_1_LSTM_backward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (decoder_2_LSTM_backward): ModuleList(
    (0): Momentum_MS_LSTMCell()
  )
  (fc1): TimeDistributed(LinearMomentum())
  (fc2): TimeDistributed(LinearMomentum())
  (dropout): TimeDistributed(MyDropout())
)


  
class GAdam(Optimizer):
   def __init__(self, params, lr=1e-2, betas=(0.1, 0.199), optimism=0.0, avg_sq_mode='weight',
             amsgrad_decay=1, weight_decay=0, l1_decay=0, late_weight_decay=True, eps=1e-8):
      """Implements generalization of Adam, AdaMax, AMSGrad algorithms.
      https://github.com/SSS135/gadam
      Adam and AdaMax has been proposed in `Adam: A Method for Stochastic Optimization`_.
      With `betas` = (beta1, 0) and `amsgrad_decay` = beta2 it will become AdaMax.
      With `amsgrad_decay` = 0 it will become AMSGrad.
      I've found it's better to use something in-between.
         `betas` = (0.1, 0.19) and `amsgrad_decay` = (0.0001) or
         `betas` = (0.1, 0.020) and `amsgrad_decay` = (0.05)
         worked best for me, but I've seen good results with wide range of settings.
      Args:
         params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
         lr (float, optional): learning rate (default: 1e-3)
         betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.1, 0.199))
         optimism (float, optional): Look-ahead factor proposed in `Training GANs with Optimism`_.
            Must be in [0, 1) range. Value of 0.1 corresponds to paper,
            0 disables it, 0.1 is 5x stronger than 0.1 (default: 0)
         avg_sq_mode (str, optional): Specifies how square gradient term should be calculated. Valid values are
            'weight' will calculate it per-weight as in vanilla Adam (default)
            'output' will average it over 0 dim of each tensor,
               i.e. shape[0] average squares will be used for each tensor
            'tensor' will average it over each tensor
            'global' will take average of average over each tensor,
               i.e. only one avg sq value will be used
         amsgrad_decay (float, optional): Decay factor for maximum running square of gradient.
            Should be in [0, 1] range.
            0 will instantly update it to current running mean square (default)
            1 will behave as proposed in `On the Convergence of Adam and Beyond`_
            Values between 0 and 1 will pull maximum mean square closer to current mean square on each step
         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
         l1_decay (float, optional): L1 penalty (default: 0)
         late_weight_decay (boolean, optional): Whether L1 and L2 penalty should be
            applied before (as proposed in 'Fixing Weight Decay Regularization in Adam'_)
            or after (vanilla Adam) normalization with gradient average squares (default: True)
         eps (float, optional): term added to the denominator to improve
            numerical stability (default: 1e-8)
      .. _Adam\: A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
      .. _On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
      .. _Training GANs with Optimism:
         https://arxiv.org/abs/1711.00141
      .. _Fixing Weight Decay Regularization in Adam:
         https://arxiv.org/abs/1711.05201
      """
      defaults = dict(lr=lr, betas=betas, optimism=optimism, amsgrad_decay=amsgrad_decay,
                  weight_decay=weight_decay, l1_decay=l1_decay, late_weight_decay=late_weight_decay, eps=eps)
      self.avg_sq_mode = avg_sq_mode
      super().__init__(params, defaults)
   def step(self, closure=None):
      """Performs a single optimization step.
      Arguments:
         closure (callable, optional): A closure that reevaluates the model
            and returns the loss.
      """
      loss = None
      if closure is not None:
         loss = closure()
      if self.avg_sq_mode == 'global':
         exp_avg_sq_list = []
      for group in self.param_groups:
         for p in group['params']:
            if p.grad is None:
               continue
            grad = p.grad.data
            if grad.is_sparse:
               raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
            state = self.state[p]
            amsgrad_decay = group['amsgrad_decay']
            amsgrad = amsgrad_decay != 1
            # State initialization
            if len(state) == 0:
               state['step'] = 0
               # Exponential moving average of gradient values
               state['exp_avg'] = torch.zeros_like(p.data)
               state['prev_shift'] = torch.zeros_like(p.data)
               # Exponential moving average of squared gradient values
               state['exp_avg_sq'] = torch.zeros_like(p.data)
               if amsgrad:
                  # Maintains max of all exp. moving avg. of sq. grad. values
                  state['max_exp_avg_sq'] = torch.zeros_like(p.data)
            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
            if amsgrad:
               max_exp_avg_sq = state['max_exp_avg_sq']
            beta1, beta2 = group['betas']
            exp_avg.mul_(beta1).add_(1 - beta1, grad)
            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
            if amsgrad:
               torch.max(max_exp_avg_sq * (1 - amsgrad_decay), exp_avg_sq, out=max_exp_avg_sq)
               if self.avg_sq_mode == 'global':
                  exp_avg_sq_list.append(max_exp_avg_sq.mean())
            else:
               if self.avg_sq_mode == 'global':
                  exp_avg_sq_list.append(exp_avg_sq.mean())
      if self.avg_sq_mode == 'global':
         global_exp_avg_sq = np.mean(exp_avg_sq_list)
      for group in self.param_groups:
         for p in group['params']:
            if p.grad is None:
               continue
            state = self.state[p]
            amsgrad_decay = group['amsgrad_decay']
            amsgrad = amsgrad_decay != 1
            exp_avg = state['exp_avg']
            if self.avg_sq_mode == 'weight':
               exp_avg_sq = state['max_exp_avg_sq'] if amsgrad else state['exp_avg_sq']
            elif self.avg_sq_mode == 'tensor':
               exp_avg_sq = (state['max_exp_avg_sq'] if amsgrad else state['exp_avg_sq']).mean()
            elif self.avg_sq_mode == 'output':
               exp_avg_sq = (state['max_exp_avg_sq'] if amsgrad else state['exp_avg_sq'])
               exp_avg_sq = exp_avg_sq.view(exp_avg_sq.shape[0], -1).mean(-1)\
                  .view(exp_avg_sq.shape[0], *((exp_avg_sq.dim() - 1) * [1]))
            elif self.avg_sq_mode == 'global':
               exp_avg_sq = global_exp_avg_sq
            else:
               raise ValueError()
            beta1, beta2 = group['betas']
            state['step'] += 1
            bias_correction1 = 1 - beta1 ** state['step']
            bias_correction2 = 1 - beta2 ** state['step']
            # Decay the first and second moment running average coefficient
            exp_avg = exp_avg / bias_correction1
            exp_avg_sq = exp_avg_sq / bias_correction2
            if self.avg_sq_mode == 'weight' or self.avg_sq_mode == 'output':
               denom = exp_avg_sq.sqrt().add_(group['eps'])
            else:
               denom = math.sqrt(exp_avg_sq) + group['eps']
            late_weight_decay = group['late_weight_decay']
            if late_weight_decay:
               exp_avg = exp_avg.div(denom)
            weight_decay = group['weight_decay']
            l1_decay = group['l1_decay']
            if weight_decay != 0:
               exp_avg.add_(weight_decay, p.data)
            if l1_decay != 0:
               exp_avg.add_(l1_decay, p.data.sign())
            if not late_weight_decay:
               exp_avg = exp_avg.div(denom)
            lr = group['lr']
            optimism = group['optimism']
            if optimism != 0:
               prev_shift = state['prev_shift']
               p.data.sub_(optimism, prev_shift)
               cur_shift = (-lr / (1 - optimism)) * exp_avg
               prev_shift.copy_(cur_shift)
               p.data.add_(cur_shift)
            else:
               grad = exp_avg
               p.data.add_(-lr, grad)
      return loss	

class EarlyStopping:
    """
	 [GOODFELLOW , BENGIO & COURVILLE, 2016] Goodfellow, I., Y. Bengio & A.Courville (2016), Deep Learning, 2016.MIT Press , Cambridge, MA, USA.
	 Early stops interrompe o treinamento se a perda de validação não melhorar após uma determinada paciência.
         https://stackoverflow.com/questions/71891964/how-to-load-early-stopping-counter-in-pytorch
	 """
    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):
       """
       Args:
          patience (int): How long to wait after last time validation loss improved.
                      Default: 7
          verbose (bool): If True, prints a message for each validation loss improvement.
                      Default: False
          delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                      Default: 0
          path (str): Path for the checkpoint to be saved to.
                      Default: 'checkpoint.pt'
          trace_func (function): trace print function.
                      Default: print
       """
       self.patience = patience
       self.verbose = verbose
       self.counter = 0
       self.best_score = None
       self.early_stop = False
       self.val_loss_min = np.Inf
       self.delta = delta
       self.path = path
       self.trace_func = trace_func
    def __call__(self, val_loss, model):
       score = -val_loss
       if self.best_score is None:
          self.best_score = score
          self.save_checkpoint(val_loss, model)
       elif score < self.best_score + self.delta:
          self.counter += 1
          self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')
          if self.counter >= self.patience:
             self.early_stop = True
       else:
          self.best_score = score
          self.save_checkpoint(val_loss, model)
          self.counter = 0
    def save_checkpoint(self, val_loss, model):
       '''Saves model when validation loss decrease.'''
       if self.verbose:
          self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ')
       torch.save(model.state_dict(), self.path)
       self.val_loss_min = val_loss	

	
def train_model(CasualCNN_Bidirectional_MomentumCasualConvLSTMCell_AdittiveAttention_Encoder_Decoder ,patience, epochs):
   model = CasualCNN_Bidirectional_MomentumCasualConvLSTMCell_AdittiveAttention_Encoder_Decoder
   seed_everything(101)
   loss_function = nn.MSELoss()
   optimizer  = GAdam(model.parameters(),lr=1e-2, betas=(0.1, 0.19), optimism=0.1, avg_sq_mode='weight',
                      amsgrad_decay=0.0001, weight_decay=0, l1_decay=0, late_weight_decay=True, eps=1e-12)
   # to track the training loss as the model trains
   train_losses = []
   # to track the learning rate decays along parameters trials estimations
   LR = []
   # initialize the early_stopping object
   early_stopping = EarlyStopping(patience=patience, verbose=True)
   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor =0.1, patience=50,  verbose=True, min_lr  = 1e-12)
   for i in range(epochs):   
      ###################
      # train the model #
      ###################
      #Wont initialize this for Stateful LSTM
      #model.init_hidden(batch_size=20)
      seed_everything(101)
      torch.cuda.empty_cache()
      model.train()
      optimizer.zero_grad()
      decoder_output = model(X_train)
      single_loss = loss_function(decoder_output.view(-1), y_train.view(-1))
      single_loss.requires_grad_(True).backward()
      nn.utils.clip_grad_value_(model.parameters(), clip_value=0.1)      
      optimizer.step()
      scheduler.step(single_loss)
      train_losses.append(single_loss.item())
      del decoder_output
      print(f'epoch: {i:3} Train loss: {single_loss.item():20.8f},  Learning Rate = {optimizer.param_groups[0]["lr"]: 20.8f}')
      # early_stopping needs the validation loss to check if it has decresed, 
      # and if it has, it will make a checkpoint of the current model
      early_stopping(single_loss, model)
      if early_stopping.early_stop:
         print("Early stopping")
         break   
   model.load_state_dict(torch.load('checkpoint.pt'))
   return model, train_losses
   

epochs = 3500
train_loss = []
model, train_loss   = train_model(CasualCNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder,patience=epochs, epochs=epochs)


# visualize the loss as the model has been parametrized 
fig = plt.figure(figsize=(20,8))
plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')
# find position of lowest validation loss
minposs = train_loss.index(min(train_loss))+1 
plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')
plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.xlim(0, len(train_loss)+1) # consistent scale
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

#Epochs-Convergence_model03.

pd.DataFrame(model(X_train).cpu().detach().numpy().ravel()).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_nonLinear3_train.csv', index=None)
PETR3_SA_Close_forecasted_nonLinear3_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear3_train.csv")
 

#######################
#   In Sample KPI     #
#                     #
#######################                                                                                                                                                            

MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values ,-1,1).ravel(), function_to_scale_data((y_train.detach().cpu().numpy()),-1,+1).ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)
RMSE: 0.151545

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel(),function_to_scale_data((y_train.detach().cpu().numpy()),-1,+1).ravel())
print('MAE: %f' % mae)
MAE: 0.148532

coefficient_of_dermination = r2_score(function_to_scale_data((y_train.detach().cpu().numpy()),-1,+1).ravel(), function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train,-1,1))
print('R2: %f' % coefficient_of_dermination)
R2: 0.184504  #Baixa peformace  para a amostragem de treino 

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(y_train.detach().cpu().numpy(),-1,1).ravel() ,function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.405028   ,pvalue: 0.000000
# Nenhuma Comrespôndencia a 1% , 3% e a 5% de confiança

# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel())

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name      score             loc  ... bootstrap_score bootstrap_pass    color
0            t   0.563274        0.272278  ...               0           None  #e41a1c
1     dweibull   1.045123        0.268298  ...               0           None  #e41a1c
2     loggamma   1.492854        -0.92435  ...               0           None  #377eb8
3         beta   1.660737  -130394.498404  ...               0           None  #4daf4a
4      lognorm   1.988415     -123.461989  ...               0           None  #984ea3
5         norm   1.997325        0.260793  ...               0           None  #ff7f00
6        gamma     2.6358       -3.684715  ...               0           None  #ffff33
7   genextreme   4.135657        0.188809  ...               0           None  #a65628
8      uniform  27.041076            -1.0  ...               0           None  #f781bf
9        expon  33.257751            -1.0  ...               0           None  #999999
10      pareto  33.824558 -5840626.539765  ...               0           None  #999999


# Plot resultados
dist.plot()
plt.show()

#Figure_3-1_epdf_modelo03_in_sample_train.png

####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################

def CNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder_predictions(model,data ,regressors, extrapolations_leght):
      n_input = extrapolations_leght
      pred_list = []
      batch = data
      pred_list.append(torch.cat(( model(batch)[-1:],  torch.FloatTensor(regressors.iloc[0,[1]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).to(device) , torch.FloatTensor(regressors.iloc[0,[2]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).to(device)),1))
      batch =  torch.cat((batch,  pred_list[-1].squeeze(3).permute(0,2,1)),0)   
      for i in range(n_input-1):
        pred_list.append(torch.cat((model(batch)[-1:], torch.FloatTensor(regressors.iloc[i+1,[1]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).to(device), torch.FloatTensor(regressors.iloc[i+1,[2]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).to(device)),1))
        batch =  torch.cat((batch, pred_list[-1].squeeze(3).permute(0,2,1)),0)
      y_pred = np.array([pred_list[j].cpu().detach().numpy() for j in range(n_input)])[:,:, 0]
      y_pred = pd.DataFrame(np.array(y_pred.ravel()))
      return  y_pred
	  
y_hat = np.array([CNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder_predictions(model, X_train ,pd.DataFrame(PETR3_SA_Close_and_its_own_volatility_and_volume_test), len(PETR3_SA_Close_and_its_own_volatility_and_volume_test)) for _ in range(7500)])
y_hat = y_hat.reshape(30,7500)
yhat = np.mean(y_hat, 1)

pd.DataFrame(yhat.ravel()).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_nonLinear3_test.csv', index=None)
PETR3_SA_Close_forecasted_nonLinear3_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear3_test.csv")
 
# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(PETR3_SA_Close_forecasted_nonLinear3_test.values)

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name          score       loc  ... bootstrap_score bootstrap_pass    color
0         beta   75384.548775  0.746049  ...               0           None  #e41a1c
1   genextreme  156659.176342  0.746805  ...               0           None  #e41a1c
2     dweibull  160787.645131  0.746965  ...               0           None  #377eb8
3        gamma  176008.577558  0.743721  ...               0           None  #4daf4a
4      lognorm  179922.723088  0.741686  ...               0           None  #984ea3
5     loggamma  198896.281474  0.644644  ...               0           None  #ff7f00
6         norm  199430.888418  0.746955  ...               0           None  #ffff33
7            t  201284.950317  0.746955  ...               0           None  #a65628
8      uniform   327287.12691  0.746176  ...               0           None  #f781bf
9        expon  824989.654402  0.746176  ...               0           None  #999999
10      pareto  840093.488405  0.722491  ...               0           None  #999999


# Plot resultados
dist.plot()
plt.show()
#Figure_3-1_epdf_modelo03_out_sample_test.png


####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################


MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
RMSE = math.sqrt(MSE)
print('RMSE: %f'%  RMSE)
RMSE: 0.633240

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
print('MAE: %f' % mae)
MAE: 0.492868

coefficient_of_dermination = r2_score(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: -1.072970 # Muito pobre peformace para a amostragem de teste

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.166667   ,pvalue: 0.807963
# conrespôndencia a 1% ,3% e 5% de confiança de que a distribuição extrapolada vem da observada

##########*/                   *\###########      
#
#   Intervalo de Credibilidade via bootstrap 
#
##########*/                   *\###########

IC valor-z
80   1.282
85   1.440
90   1.645
95   1.960
99   2.576

model.eval()
mean = function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel()
std = np.array([model(X_train).cpu().detach().numpy() for _ in range(7500)])
std = np.std(std, axis=0)
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1), label = "Amostra observada")
plt.title(' log-retornos a d=  1.0 da PETR3_SA estimação da amostragem de treino e extrapolação para o perído de teste ')
plt.plot(function_to_scale_data(mean,-1,1), label = "Amostra estimada")
plt.plot(function_to_scale_data(upper_bands.ravel(),-1,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()
#modelo03_obs_vs_est.png

mean = function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel()
std = np.array([CNN_Attention_Bidirectional_MS_LSTM_Encoder_Decoder_predictions(model.eval(), X_train ,pd.DataFrame(PETR3_SA_Close_and_its_own_volatility_and_volume_test), len(PETR3_SA_Close_and_its_own_volatility_and_volume_test)) for _ in range(7500)])
std = np.std(std, axis=0)
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1), label = "Amostra observada de teste")
plt.title(' log-retornos a d=  1.0 da PETR3_SA extrapolação da amostragem de teste ')
plt.plot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1), label = "Amostra extrapolada de teste")
plt.plot(function_to_scale_data(upper_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#modelo03_obs_vs_est_test..png

sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra observada de treino")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel(),bins=10,color='blue', label = "Amostra  estimada de treino ")
plt.suptitle('Histograma para amostras estimadas  e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_3-1_modelo03_in_sample_train_Histogram.png


sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra de teste observada")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel(),bins=10,color='blue', label = "Amostra de teste estimada/extrapolada")
plt.suptitle('Histograma para amostras estimadas de teste e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_3-1_modelo03_out_sample_test_Histogram.png

#######/                               \#######
#                                             #
#    KPI de Risco.Binômio de risco vs retorno #
#                                             #
#######/                               \#######

mean_03_treino = np.mean(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel())
std_03_treino  = np.std(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel())
print('retorno_modelo03_treino: %f   ,risco_modelo03_treino: %f' % (mean_03_treino,std_03_treino))
#retorno_modelo03_treino: 0.260793   ,risco_modelo03_treino: 0.192381

mean_obs_treino = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
std_obs_treino  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
print('retorno_obs_treino: %f   ,risco_obs_treino: %f' % (mean_obs_treino,std_obs_treino))
#retorno_obs_treino: 0.409123   ,risco_obs_treino: 0.167715

mean_03_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel())
std_03_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel())
print('retorno_modelo03_extrapolado: %f   ,risco_modelo03_extrapolado: %f' % (mean_03_extrapolado,std_03_extrapolado))
#retorno_modelo03_extrapolado: -0.090398   ,risco_modelo03_extrapolado: 0.468272

mean_obs_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
std_obs_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
print('retorno_obs_extrapolado: %f   ,risco_obs_extrapolado: %f' % (mean_obs_extrapolado,std_obs_extrapolado))
#retorno_obs_extrapolado: -0.056525   ,risco_obs_extrapolado: 0.438676

# Draw quantile-quantile plot
plt.figure()
qqplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas')
plt.ylabel('QQplot realidade observada ')
plt.title('QQplots para as duas amostras ')
plt.show()

#Figure_3-1_modelo03_in_sample_train_QQplots.png

qqplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas de teste')
plt.ylabel('QQplot realidade observada de teste')
plt.title('QQplots para as duas amostras de teste')
plt.show()	

#
	

######################################################################################################
# Para a amostragem de treino Caudas leves                                                           #
# Para a amostragem de teste  com caudas leves                                                       #
######################################################################################################


#########*/                                                 *\###########
#
#      */\*Narx Aproximações Numéricas   
#                  recursivas via ANN  */\*
#      Non Linear Auto-Regression *Narx* 
# 2DCasualCNN_Attention_Bidirectional_MomentumConvLSTM_Encoder_Decoder
#
#########*/                                                 *\###########


###########################################################################  
#  Para dados de séries temporais é melhor que a divisão                  #
#  seja  mais concentrada para a amostragem de treino devido a            #
#  interdependência temporal                                              # 
#  temporal                                                               #
###########################################################################

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min

volatility_PETR3_SA_Close_SV_MomentumLSTM = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volatility_PETR3_SA_Close_SV_MomentumLSTM.csv")
volume_PETR3_SA_Close_SV_MomentumLSTM = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/volume_PETR3_SA_Close_SV_MomentumLSTM.csv")
PETR3_SA_Volume_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Volume_Linear_filtred.csv")
PETR3_SA_Close_fractional_difference_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_fractional_difference_Linear_filtred.csv")
PETR3_SA_Close_fractional_difference_Linear_filtred = PETR3_SA_Close_fractional_difference_Linear_filtred[-len(PETR3_SA_Volume_Linear_filtred):]

for t in range(1, len(volatility_PETR3_SA_Close_SV_MomentumLSTM)):
   PETR3_SA_Close_SV_MomentumLSTM01  = np.random.normal(0, volatility_PETR3_SA_Close_SV_MomentumLSTM.iloc[t], len(volatility_PETR3_SA_Close_SV_MomentumLSTM))
   PETR3_SA_Close_SV_MomentumLSTM02  = np.random.normal(0, np.exp(volume_PETR3_SA_Close_SV_MomentumLSTM.iloc[t]/2), len(volume_PETR3_SA_Close_SV_MomentumLSTM))

PETR3_SA_Close_SV_MomentumLSTM01 = pd.DataFrame(PETR3_SA_Close_SV_MomentumLSTM01)
PETR3_SA_Close_SV_MomentumLSTM02 = pd.DataFrame(PETR3_SA_Close_SV_MomentumLSTM02)

PETR3_SA_Close_and_its_own_volatility_and_volume = np.concatenate((function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred.values,0,1),
function_to_scale_data(PETR3_SA_Close_SV_MomentumLSTM01,0,1),function_to_scale_data(PETR3_SA_Close_SV_MomentumLSTM02.values,0,1),function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred.values,0,1)), axis=1)

PETR3_SA_Close_and_its_own_volatility_and_volume = pd.DataFrame(PETR3_SA_Close_and_its_own_volatility_and_volume)
PETR3_SA_Close_and_its_own_volatility_and_volume_train = PETR3_SA_Close_and_its_own_volatility_and_volume[:-30]
PETR3_SA_Close_and_its_own_volatility_and_volume_test = PETR3_SA_Close_and_its_own_volatility_and_volume[-30:]

# dividir uma sequência multivariada em amostras (Transformar matriz coluna em tensor)
def split_sequences(sequences, n_steps):
   X, y = [] , []
   for i in range(len(sequences)):
      # encontrar o fim deste padrão
      end_ix = i + n_steps
      # verificar se estamos além do conjunto de dados
      if end_ix > len(sequences)-1:
         break
      # reunir partes de entrada e saída do padrão
      seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]
      X.append(seq_x)
      y.append(seq_y)
   return np.array(X) ,np.array(y)
   

n_steps = 1
X_train, y_train = split_sequences(PETR3_SA_Close_and_its_own_volatility_and_volume_train.values.astype('float16'), n_steps)
X_test, y_test = split_sequences(PETR3_SA_Close_and_its_own_volatility_and_volume_test.values.astype('float16'), n_steps)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
X_train = torch.FloatTensor(X_train).to(device)

X_train = X_train.reshape((X_train.shape[0],1,1,X_train.shape[1], X_train.shape[2]))
y_train=  torch.FloatTensor(y_train).to(device)

X_test = torch.FloatTensor(X_test).to(device)
X_test = X_test.reshape((X_test.shape[0],1,1,X_test.shape[1], X_test.shape[2]))
y_test=  torch.FloatTensor(y_test).to(device)


class TimeDistributed(nn.Module):
   "Aplica um módulo sobre tdim (time dimension) de forma idêntica para cada etapa
    https://github.com/pytorch/pytorch/issues/1927   Salim Khazem"
   def __init__(self, module, low_mem=False, tdim=1):
      super(TimeDistributed, self).__init__()
      self.module = module
      self.low_mem = low_mem  
      self.tdim = tdim   
   def forward(self, *args, **kwargs):
      "matriz x com formato:(batch size(Narx p),seq_len(linhas),channels(colunas),width(eixo x),height(eixo y))"
      if self.low_mem or self.tdim!=1: 
         return self.low_mem_forward(*args)
      else:
         #only support tdim=1
         inp_shape = args[0].shape
         bs, seq_len = inp_shape[0], inp_shape[1]   
         out = self.module(*[x.contiguous().view(bs*seq_len, *x.shape[2:]) for x in args], **kwargs)
         out_shape = out.shape
         return out.view(bs, seq_len,*out_shape[1:])
   def low_mem_forward(self, *args, **kwargs):                                 
      "matriz x com as seguintes dimensões:bs(batch_size/timesteps),seq_len(Numero de linhas),channels(Numero de colunas),width(eixo x de timesteps),height(eixo y de timesteps))"
      tlen = args[0].shape[self.tdim]
      args_split = [torch.unbind(x, dim=self.tdim) for x in args]
      out = []
      for i in range(tlen):
         out.append(self.module(*[args[i] for args in args_split]), **kwargs)
      return torch.stack(out,dim=self.tdim)
   def __repr__(self):
      return f'TimeDistributed({self.module})'
	  
	  
	
class CausalConv2d(nn.Conv2d):
   "https://gist.github.com/wassname/7eb4095a4f3d3b5eea8adaaf4419c822"
   def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=None, dilation=1, groups=1, bias=True):
      kernel_size = _pair(kernel_size)
      stride = _pair(stride)
      dilation = _pair(dilation)
      if padding is None:
         padding = [int((kernel_size[i] -1) * dilation[i]) for i in range(len(kernel_size))]
      self.left_padding = _pair(padding)
      super().__init__(in_channels, out_channels, kernel_size,
                                 stride=stride, padding=0, dilation=dilation,
                                 groups=groups, bias=bias)
   def forward(self, inputs):
      inputs = F.pad(inputs, (self.left_padding[1], 0, self.left_padding[0], 0))
      output = super().forward(inputs)
      return output	

		
class MyDropout(nn.Module):
   def __init__(self, p):
      "https://gist.github.com/mohcinemadkour/0ae0fff3f81c82d83b9d9615f13a2861"
      super(MyDropout, self).__init__()
      self.p = p
      # multiplicador é 1/(1-p). Definir o multiplicador para 0 quando p=1 para evitar erros
      if self.p < 1:
         self.multiplier_ = 1.0 / (1.0-p)
      else:
         self.multiplier_ = 0.0
   def forward(self, input):
      # com model.eval(), não usar dropout
      if not self.training:
         return input
      # A quantidade de ensaios/ensaios de Bernoulli(1-p) é dada pela cardinalidade da matrix de entrada (input.shape)
      selected_ = torch.Tensor(input.shape).uniform_(0,1)>self.p
      # Suporte a  CPU ou GPU.
      if input.is_cuda:
         selected_ = Variable(selected_.type(torch.cuda.FloatTensor), requires_grad=False)
      else:
         selected_ = Variable(selected_.type(torch.FloatTensor), requires_grad=False)         
      # Multiplicando a saída pelo multiplicador conforme descrito no artigo [1]
      return torch.mul(selected_,input) * self.multiplier_
		

def swish(x):
   return (x * F.sigmoid(x*0.000001)).to(device)		


class MomentumConvLSTMCell(nn.Module):
    def __init__(self, input_dim, hidden_dim,mu,epsilon, kernel_size, bias):
       """
       [SAINATH ET AL., 2020]  Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks.  Conference: ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . DOI:20.1209/ICASSP.2020.7178838.
       Implementação por Andreas Holm Nielsen : https://holmdk.github.io/2020/04/02/video_prediction.html

       Initialize MomentumConvLSTM cell.
       Parametros
       ----------
       input_dim: int
          Numero de colunas da matriz de entrada.
       hidden_dim: int
          Numero de canais dos estados latentes.
       kernel_size: (int, int)
          Tamanho da matriz filtro Kernel.
       bias: bool
          usar coeficiente de regressão ou não. bias.
	    Can't use Markov Switching transitions between hidden states, because
		 there is no covariance matrix (weight matrix).In ConvLSTM, the weights
		 are given by 2D convolutions linear transformations.
       """
       super(MomentumConvLSTMCell, self).__init__()
       self.input_dim = input_dim
       self.hidden_dim = hidden_dim
       self.kernel_size = kernel_size
       self.padding = kernel_size[0] // 2, kernel_size[1] // 2
       self.bias = bias
       self.mu = mu
       self.epsilon = epsilon
       self.conv = CausalConv2d(in_channels=self.input_dim + self.hidden_dim,
                        out_channels=4 * self.hidden_dim,
                        kernel_size=self.kernel_size,
                        padding=self.padding,
                        bias=self.bias)
    def forward(self, input_tensor, cur_state):
       h_cur, c_cur = cur_state
       combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis
       v = torch.rand(combined.shape).requires_grad_().to(device)
       vy = self.mu*v+ self.epsilon*combined
       combined_conv =  self.conv(combined)  + self.conv(vy)
       cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)
       i = torch.sigmoid(cc_i)
       f = torch.sigmoid(cc_f)
       o = torch.sigmoid(cc_o)
       g = torch.tanh(cc_g)
       c_next = f * c_cur + i * g
       h_next = o * torch.tanh(c_next)
       return h_next, c_next
    def init_hidden(self, batch_size, image_size):
       height, width = image_size
       return (Variable(nn.init.kaiming_normal_((torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)), mode='fan_out', nonlinearity='relu')),
             Variable(nn.init.kaiming_normal_((torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device)), mode='fan_out', nonlinearity='relu')))


			 
class LinearMomentum(nn.Module):
      """
      [GOODFELLOW , BENGIO & COURVILLE, 2016] Goodfellow, I., Y. Bengio & A.Courville (2016),
      Deep Learning, 2016.MIT Press ,, Cambridge, MA, USA.
      [NGUYEN   ET AL, 2020] Nguyen. Tam M, Baraniuk .. Richard G,   Bertozzi. Andrea L, Osher. Stanley J, Wang, Bao. (2020).
      MomentumRNN: Integrating Momentum into Recurrent Neural Networks: Advances in Neural Information Processing Systems (NeurIPS) 2020
      https://doi.org/20.48550/arXiv.2006.06919
      Implementação vetorizada do modelo LSTM por Pierre Esposito  https://github.com/piEsposito/pytorch-lstm-by-hand
      """
      def __init__(self, input_features, out_features, mu, epsilon, bias=True):
         super().__init__()
         self.input_features = input_features
         self.out_features = out_features
         self.mu = mu
         self.epsilon = epsilon
         self.weight = nn.Parameter(torch.Tensor(out_features,input_features))
         if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
         else :
            self.register_parameter('bias', None)
         self.init_weights()
      def init_weights(self):
         for weight in self.parameters():
            #weight.data.uniform_(-1/math.sqrt(self.input_features), 1/math.sqrt(self.input_features))
            nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5),mode='fan_out', nonlinearity='relu').to(device)
      def forward(self, x):
         """
         y[i,j] == x[i,0] * m.weight[j,0] + x[i,1] * m.weight[j,1] + m.bias[j]
            where i is in interval [0, batch_size) and j in [0, out_features) .
         """
         v = torch.zeros(self.out_features).requires_grad_().to(x.device)
         y = self.mu*v + self.epsilon*(x.matmul(self.weight.t()) + self.bias) # y = mu*v + epsilon*(x*W^T + b)
         return y
			

class CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder(nn.Module):
                      def __init__(self, nf=3, in_chan=4):
                         super(CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder, self).__init__()
                         """ ARQUITETURA
                         Implementação por Andreas Holm Nielsen : https://holmdk.github.io/2020/04/02/video_prediction.html
                         # Encoder 2D CNN (Parametrizar as dependências do tensor tempo-espacial.Sua Volatilidade)
                         # Encoder (ConvLSTM)
                         # Encoder Vector (últimos estados latentes do encoder)
                         # Decoder (ConvLSTM) - assume a matriz produzida pelo encoder como impute

                         """
                         self.TimeDistributed = TimeDistributed(nn.Module)
                         self.qkv = 3
                         self.query = TimeDistributed(LinearMomentum(3, 3,0.9,0.9,True),tdim=1).to(device)
                         self.key = TimeDistributed(LinearMomentum(3, 3,0.9,0.9,   True),tdim=1).to(device)
                         self.value = TimeDistributed(LinearMomentum(3, 3,0.9,0.9,   True),tdim=1).to(device)
                         self.attn = TimeDistributed(LinearMomentum(3, 3,0.9,0.9,   True),tdim=1).to(device)
                         self.scale = math.sqrt(self.qkv)
                         self.bn1 =  nn.BatchNorm3d(1,affine=True).to(device)
                         self.bn2 =  nn.BatchNorm3d(1,affine=True).to(device)
                         self.bn3 =  nn.BatchNorm3d(1,affine=True).to(device)
                         self.bn4 =  nn.BatchNorm3d(1,affine=True).to(device)
                         self.bn5 =  nn.BatchNorm3d(1,affine=True).to(device)
                         self.num_layers = 1
                         self.encoder_1_CNN =  TimeDistributed(CausalConv2d(1, 2, (1,1),
                                        stride=(1, 1), padding=(1, 1),
                                        dilation=(1, 1), bias =True),tdim=1).to(device)
                         self.encoder_2_CNN =  TimeDistributed(CausalConv2d(2, 4, (1,1),
                                         stride=(1, 1), padding=(1, 1),
                                         dilation=(1, 1), bias =True),tdim=1).to(device)
                         encoder_1_convlstm_forward = []
                         encoder_2_convlstm_forward = []
                         decoder_1_convlstm_forward = []
                         decoder_2_convlstm_forward = []
                         encoder_1_convlstm_backward =[]
                         encoder_2_convlstm_backward = []
                         decoder_1_convlstm_backward = []
                         decoder_2_convlstm_backward = []
                         for i in range(0, self.num_layers):
                           encoder_1_convlstm_forward.append(MomentumConvLSTMCell(input_dim= in_chan,
                                                       hidden_dim=nf, mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           encoder_2_convlstm_forward.append(MomentumConvLSTMCell(input_dim=nf,
                                                       hidden_dim=nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           decoder_1_convlstm_forward.append(MomentumConvLSTMCell(input_dim=nf,
                                                       hidden_dim= nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           decoder_2_convlstm_forward.append(MomentumConvLSTMCell(input_dim=nf,
                                                       hidden_dim=nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           encoder_1_convlstm_backward.append(MomentumConvLSTMCell(input_dim= in_chan,
                                                       hidden_dim=nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           encoder_2_convlstm_backward.append(MomentumConvLSTMCell(input_dim=nf,
                                                       hidden_dim=nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           decoder_1_convlstm_backward.append(MomentumConvLSTMCell(input_dim=nf,
                                                       hidden_dim=nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                           decoder_2_convlstm_backward.append(MomentumConvLSTMCell(input_dim=nf,
                                                       hidden_dim=nf,mu = 0.9, epsilon =0.9,
                                                       kernel_size=(1,1),
                                                       bias=True).to(device))
                         self.encoder_1_convlstm_forward = nn.ModuleList(encoder_1_convlstm_forward)
                         self.encoder_1_convlstm_forward = nn.ModuleList(encoder_1_convlstm_forward)
                         self.encoder_2_convlstm_forward = nn.ModuleList(encoder_2_convlstm_forward)
                         self.decoder_1_convlstm_forward = nn.ModuleList(decoder_1_convlstm_forward)
                         self.decoder_2_convlstm_forward = nn.ModuleList(decoder_2_convlstm_forward)
                         self.encoder_1_convlstm_backward = nn.ModuleList(encoder_1_convlstm_backward)
                         self.encoder_2_convlstm_backward = nn.ModuleList(encoder_2_convlstm_backward)
                         self.decoder_1_convlstm_backward = nn.ModuleList(decoder_1_convlstm_backward)
                         self.decoder_2_convlstm_backward = nn.ModuleList(decoder_2_convlstm_backward)
                         self.fc1 = TimeDistributed(LinearMomentum(3, 1,0.9,0.9,  True), tdim=1).to(device)
                         self.fc2 = TimeDistributed(LinearMomentum(5, 1,0.9,0.9,  True), tdim=1).to(device)
                         self.fc3 = TimeDistributed(LinearMomentum(3, 1,0.9,0.9,  True), tdim=1).to(device)
                         self.dropout = TimeDistributed(MyDropout(p=0.50), tdim=1).to(device)
                      def encoder_decoder_attention(self, x, seq_len, future_step, h_t_forward, c_t_forward , h_t_backward, c_t_backward):
                         x = self.encoder_1_CNN(x)
                         x = self.encoder_2_CNN(x)
                         x = self.bn1(x)
                         x = F.max_pool3d(x, 1)
                         outputs = []
                         for layer_idx in range(self.num_layers):
                         # encoder
                            h_t1_forward, c_t1_forward = self.encoder_1_convlstm_forward[layer_idx](input_tensor=x.squeeze(1).squeeze(1),
                                                     cur_state=[h_t_forward, c_t_forward])
                            h_t2_forward, c_t2_forward = self.encoder_2_convlstm_forward[layer_idx](input_tensor=h_t1_forward,
                                                       cur_state=[h_t1_forward, c_t1_forward])
                            h_t1_backward, c_t1_backward = self.encoder_1_convlstm_backward[layer_idx](input_tensor= torch.flip(x.squeeze(1),[0,1]),
                                                    cur_state=[h_t_backward, c_t_backward])
                            h_t2_backward, c_t2_backward = self.encoder_2_convlstm_backward[layer_idx](input_tensor=h_t1_backward,
                                                      cur_state=[h_t1_backward, c_t1_backward])
                         # encoder_vector
                         encoder_vector_forward = self.dropout(h_t2_forward)
                         encoder_vector_backward = self.dropout(h_t2_backward)
                         # decoder
                         for layer_idx in range(self.num_layers):
                            h_t3_forward, c_t3_forward = self.decoder_1_convlstm_forward[layer_idx](input_tensor=encoder_vector_forward,
                                                       cur_state=[h_t2_forward,c_t2_forward])
                            h_t3_backward, c_t3_backward = self.decoder_1_convlstm_backward[layer_idx](input_tensor=encoder_vector_backward,
                                                       cur_state=[h_t2_backward ,c_t2_backward])
                            h_t4_forward, c_t4_forward = self.decoder_2_convlstm_forward[layer_idx](input_tensor=h_t3_forward,
                                                       cur_state=[h_t3_forward, c_t3_forward])
                            h_t4_backward, c_t4_backward = self.decoder_2_convlstm_backward[layer_idx](input_tensor=h_t3_backward,
                                                     cur_state=[h_t3_backward, c_t3_backward])
                            encoder_vector_forward  = self.dropout(h_t4_forward)
                            encoder_vector_backward = self.dropout(h_t4_backward)
                            outputs += [(h_t4_forward + h_t4_backward)/2]  # predictions
                         outputs =torch.stack(outputs , dim=1)
                         x = outputs.permute(0,1,2,4,3)
                         Q, K, V = self.query(x), self.key(x), self.value(x)
                         dot_product = torch.matmul(Q, K.permute(0,1,2,4,3)) / self.scale
                         scores = torch.softmax(dot_product, dim=-1)
                         scaled_x = torch.matmul(scores, V)+ x
                         x = self.bn3(self.attn(scaled_x))
                         x = torch.nn.functional.max_pool3d(x, 1)
                         return x
                      def _init_hidden(self,batch_size, image_size):
                         init_states_forward = []
                         init_states_backward = []
                         for i in range(self.num_layers):
                           init_states_forward.append(self.encoder_1_convlstm_forward[i].init_hidden(batch_size, image_size))
                           init_states_backward.append(self.encoder_1_convlstm_backward[i].init_hidden(batch_size, image_size))
                         return init_states_forward , init_states_backward
                      def forward(self, x ):
                         """
                         Parametros
                         ----------
                         input_tensor:
                         5-D Tensor de seguintes dimensões  (b, t, c, h, w)      #   batch( timesteps), time(linhas da matriz) , channel( colunas da matriz), height (subeixo x) , width( subeixo y)
                         """
                         # observar o numero das dimensões da matriz
                         b, _, _, h, w = x.size()
                         seq_len = x.size(1)
                         # inicializar os estados latentes
                         hidden_state_forward,hidden_state_backward  = self._init_hidden(batch_size=b, image_size=(3, 5))
                         for layer_idx in range(self.num_layers):
                            h_t_forward, c_t_forward = hidden_state_forward[layer_idx]
                            h_t_backward, c_t_backward = hidden_state_backward[layer_idx]
                         # autoencoder forward
                         x   = self.encoder_decoder_attention(x, seq_len, seq_len, h_t_forward, c_t_forward,h_t_backward, c_t_backward)
                         x   = F.max_pool3d(self.bn3(self.fc1(x)),1)
                         x   = F.max_pool3d(self.bn4(self.fc2(x.permute(0,1,2,4,3))),1)
                         x   = swish(F.max_pool3d(self.bn5(self.fc3(x.permute(0,1,3,4,2))),1))
                         return x



#########*/                               *\###########
#
#       */\*Narx ~ distribuições a priori de pesos  */\*
#
#########*/                                *\##########


def initialize_weights(m):
   if type(m) in [LinearMomentum] :
      nn.init.kaiming_uniform_(m.weight ,mode='fan_in', nonlinearity='relu')
      nn.init.uniform_(m.bias, -1.0, 1.0)
   elif type(m) in [MomentumConvLSTMCell,CausalConv2d]:
            for name, param in m.named_parameters():
               if 'bias' in name:
                  nn.init.normal_(param ,0,0.1)
               elif 'weight' in name:
                  nn.init.kaiming_normal_(param , mode='fan_in', nonlinearity='relu')
						
												
#########*/    *\###########
#
#    Reprodutibilidade
#
#########*/     *\###########

		   
def seed_everything(seed):
   torch.manual_seed(seed)
   torch.cuda.manual_seed(seed)
   torch.cuda.manual_seed_all(seed)
   np.random.seed(seed)
   torch.backends.cudbenchmark = False
   torch.backends.cuddeterministic = True

		   
#########*/                       *\###########
#
#           Narx  - Parametric Assembling 
#
#########*/                        *\###########

CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder  = CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder().to(device)
CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder.apply(initialize_weights)

class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):
       """
       Args:
          patience (int): How long to wait after last time validation loss improved.
                      Default: 7
          verbose (bool): If True, prints a message for each validation loss improvement.
                      Default: False
          delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                      Default: 0
          path (str): Path for the checkpoint to be saved to.
                      Default: 'checkpoint.pt'
          trace_func (function): trace print function.
                      Default: print
       """
       self.patience = patience
       self.verbose = verbose
       self.counter = 0
       self.best_score = None
       self.early_stop = False
       self.val_loss_min = np.Inf
       self.delta = delta
       self.path = path
       self.trace_func = trace_func
    def __call__(self, val_loss, model):
       score = -val_loss
       if self.best_score is None:
          self.best_score = score
          self.save_checkpoint(val_loss, model)
       elif score < self.best_score + self.delta:
          self.counter += 1
          self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')
          if self.counter >= self.patience:
             self.early_stop = True
       else:
          self.best_score = score
          self.save_checkpoint(val_loss, model)
          self.counter = 0
    def save_checkpoint(self, val_loss, model):
       '''Saves model when validation loss decrease.'''
       if self.verbose:
          self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ')
       torch.save(model.state_dict(), self.path)
       self.val_loss_min = val_loss


class GAdam(Optimizer):
   def __init__(self, params, lr=1e-3, betas=(0.1, 0.199), optimism=0.0, avg_sq_mode='weight',
             amsgrad_decay=1, weight_decay=0, l1_decay=0, late_weight_decay=True, eps=1e-8):
      """Implements generalization of Adam, AdaMax, AMSGrad algorithms.
      Adam and AdaMax has been proposed in `Adam: A Method for Stochastic Optimization`_.
      With `betas` = (beta1, 0) and `amsgrad_decay` = beta2 it will become AdaMax.
      With `amsgrad_decay` = 0 it will become AMSGrad.
      I've found it's better to use something in-between.
         `betas` = (0.1, 0.19) and `amsgrad_decay` = (0.0001) or
         `betas` = (0.1, 0.020) and `amsgrad_decay` = (0.05)
         worked best for me, but I've seen good results with wide range of settings.
      Args:
         params (iterable): iterable of parameters to optimize or dicts defining
            parameter groups
         lr (float, optional): learning rate (default: 1e-3)
         betas (Tuple[float, float], optional): coefficients used for computing
            running averages of gradient and its square (default: (0.1, 0.199))
         optimism (float, optional): Look-ahead factor proposed in `Training GANs with Optimism`_.
            Must be in [0, 1) range. Value of 0.1 corresponds to paper,
            0 disables it, 0.1 is 5x stronger than 0.1 (default: 0)
         avg_sq_mode (str, optional): Specifies how square gradient term should be calculated. Valid values are
            'weight' will calculate it per-weight as in vanilla Adam (default)
            'output' will average it over 0 dim of each tensor,
               i.e. shape[0] average squares will be used for each tensor
            'tensor' will average it over each tensor
            'global' will take average of average over each tensor,
               i.e. only one avg sq value will be used
         amsgrad_decay (float, optional): Decay factor for maximum running square of gradient.
            Should be in [0, 1] range.
            0 will instantly update it to current running mean square (default)
            1 will behave as proposed in `On the Convergence of Adam and Beyond`_
            Values between 0 and 1 will pull maximum mean square closer to current mean square on each step
         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
         l1_decay (float, optional): L1 penalty (default: 0)
         late_weight_decay (boolean, optional): Whether L1 and L2 penalty should be
            applied before (as proposed in 'Fixing Weight Decay Regularization in Adam'_)
            or after (vanilla Adam) normalization with gradient average squares (default: True)
         eps (float, optional): term added to the denominator to improve
            numerical stability (default: 1e-8)
      .. _Adam\: A Method for Stochastic Optimization:
         https://arxiv.org/abs/1412.6980
      .. _On the Convergence of Adam and Beyond:
         https://openreview.net/forum?id=ryQu7f-RZ
      .. _Training GANs with Optimism:
         https://arxiv.org/abs/1711.00141
      .. _Fixing Weight Decay Regularization in Adam:
         https://arxiv.org/abs/1711.05201
      """
      defaults = dict(lr=lr, betas=betas, optimism=optimism, amsgrad_decay=amsgrad_decay,
                  weight_decay=weight_decay, l1_decay=l1_decay, late_weight_decay=late_weight_decay, eps=eps)
      self.avg_sq_mode = avg_sq_mode
      super().__init__(params, defaults)
   def step(self, closure=None):
      """Performs a single optimization step.
      Arguments:
         closure (callable, optional): A closure that reevaluates the model
            and returns the loss.
      """
      loss = None
      if closure is not None:
         loss = closure()
      if self.avg_sq_mode == 'global':
         exp_avg_sq_list = []
      for group in self.param_groups:
         for p in group['params']:
            if p.grad is None:
               continue
            grad = p.grad.data
            if grad.is_sparse:
               raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
            state = self.state[p]
            amsgrad_decay = group['amsgrad_decay']
            amsgrad = amsgrad_decay != 1
            # State initialization
            if len(state) == 0:
               state['step'] = 0
               # Exponential moving average of gradient values
               state['exp_avg'] = torch.zeros_like(p.data)
               state['prev_shift'] = torch.zeros_like(p.data)
               # Exponential moving average of squared gradient values
               state['exp_avg_sq'] = torch.zeros_like(p.data)
               if amsgrad:
                  # Maintains max of all exp. moving avg. of sq. grad. values
                  state['max_exp_avg_sq'] = torch.zeros_like(p.data)
            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
            if amsgrad:
               max_exp_avg_sq = state['max_exp_avg_sq']
            beta1, beta2 = group['betas']
            exp_avg.mul_(beta1).add_(1 - beta1, grad)
            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
            if amsgrad:
               torch.max(max_exp_avg_sq * (1 - amsgrad_decay), exp_avg_sq, out=max_exp_avg_sq)
               if self.avg_sq_mode == 'global':
                  exp_avg_sq_list.append(max_exp_avg_sq.mean())
            else:
               if self.avg_sq_mode == 'global':
                  exp_avg_sq_list.append(exp_avg_sq.mean())
      if self.avg_sq_mode == 'global':
         global_exp_avg_sq = np.mean(exp_avg_sq_list)
      for group in self.param_groups:
         for p in group['params']:
            if p.grad is None:
               continue
            state = self.state[p]
            amsgrad_decay = group['amsgrad_decay']
            amsgrad = amsgrad_decay != 1
            exp_avg = state['exp_avg']
            if self.avg_sq_mode == 'weight':
               exp_avg_sq = state['max_exp_avg_sq'] if amsgrad else state['exp_avg_sq']
            elif self.avg_sq_mode == 'tensor':
               exp_avg_sq = (state['max_exp_avg_sq'] if amsgrad else state['exp_avg_sq']).mean()
            elif self.avg_sq_mode == 'output':
               exp_avg_sq = (state['max_exp_avg_sq'] if amsgrad else state['exp_avg_sq'])
               exp_avg_sq = exp_avg_sq.view(exp_avg_sq.shape[0], -1).mean(-1)\
                  .view(exp_avg_sq.shape[0], *((exp_avg_sq.dim() - 1) * [1]))
            elif self.avg_sq_mode == 'global':
               exp_avg_sq = global_exp_avg_sq
            else:
               raise ValueError()
            beta1, beta2 = group['betas']
            state['step'] += 1
            bias_correction1 = 1 - beta1 ** state['step']
            bias_correction2 = 1 - beta2 ** state['step']
            # Decay the first and second moment running average coefficient
            exp_avg = exp_avg / bias_correction1
            exp_avg_sq = exp_avg_sq / bias_correction2
            if self.avg_sq_mode == 'weight' or self.avg_sq_mode == 'output':
               denom = exp_avg_sq.sqrt().add_(group['eps'])
            else:
               denom = math.sqrt(exp_avg_sq) + group['eps']
            late_weight_decay = group['late_weight_decay']
            if late_weight_decay:
               exp_avg = exp_avg.div(denom)
            weight_decay = group['weight_decay']
            l1_decay = group['l1_decay']
            if weight_decay != 0:
               exp_avg.add_(weight_decay, p.data)
            if l1_decay != 0:
               exp_avg.add_(l1_decay, p.data.sign())
            if not late_weight_decay:
               exp_avg = exp_avg.div(denom)
            lr = group['lr']
            optimism = group['optimism']
            if optimism != 0:
               prev_shift = state['prev_shift']
               p.data.sub_(optimism, prev_shift)
               cur_shift = (-lr / (1 - optimism)) * exp_avg
               prev_shift.copy_(cur_shift)
               p.data.add_(cur_shift)
            else:
               grad = exp_avg
               p.data.add_(-lr, grad)
      return loss

	  

def train_model(CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder ,patience, epochs):
   model = CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder
   seed_everything(101)
   loss_function = nn.MSELoss()
   optimizer  = GAdam(model.parameters(),lr=1e-2, betas=(0.1, 0.19), optimism=0.1, avg_sq_mode='weight',
             amsgrad_decay=0.0001, weight_decay=0, l1_decay=0, late_weight_decay=True, eps=1e-8)
	# to track the training loss as the model trains
   train_losses = []
   # to track the learning rate decays along parameters trials estimations
   LR = []
   # initialize the early_stopping object
   early_stopping = EarlyStopping(patience=patience, verbose=True)
   scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor =0.1, patience=50,  verbose=True, min_lr  = 1e-20)
   for i in range(epochs):   
      ###################
      # train the model #
      ###################
      #Wont initialize this for Stateful LSTM
      #model.init_hidden(batch_size=20)
      torch.cuda.empty_cache()
      model.train()
      optimizer.zero_grad()
      decoder_output = model(X_train)
      single_loss = loss_function(decoder_output.view(-1), y_train.view(-1))
      single_loss.requires_grad_(True).backward()
      nn.utils.clip_grad_value_(model.parameters(), clip_value=0.1)      
      optimizer.step()
      scheduler.step(single_loss)
      train_losses.append(single_loss.item())
      del decoder_output
      print(f'epoch: {i:3} Train loss: {single_loss.item():20.8f},  Learning Rate = {optimizer.param_groups[0]["lr"]: 20.8f}')
      # early_stopping needs the validation loss to check if it has decresed, 
      # and if it has, it will make a checkpoint of the current model
      early_stopping(single_loss, model)
      if early_stopping.early_stop:
         print("Early stopping")
         break   
   model.load_state_dict(torch.load('checkpoint.pt'))
   return model, train_losses


epochs = 3500  	  
train_loss = []
model, train_loss   = train_model(CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder,patience=epochs, epochs=epochs)

# visualize the loss as the model has been parametrized 
fig = plt.figure(figsize=(20,8))
plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')
# find position of lowest validation loss
minposs = train_loss.index(min(train_loss))+1 
plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')
plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.xlim(0, len(train_loss)+1) # consistent scale
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

#Epochs-Convergence_model04.png

pd.DataFrame(function_to_scale_data(pd.DataFrame(model(X_train).cpu().detach().numpy().ravel()),-1,1)).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_nonLinear4_train.csv', index=None)
PETR3_SA_Close_forecasted_nonLinear4_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear4_train.csv")

# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel())

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name      score              loc  ... bootstrap_score bootstrap_pass    color
0            t   0.980264          0.45054  ...               0           None  #e41a1c
1     dweibull   1.159157          0.44462  ...               0           None  #e41a1c
2     loggamma   3.076137        -0.214735  ...               0           None  #377eb8
3         beta    3.47734   -296118.912372  ...               0           None  #4daf4a
4         norm   3.803039         0.441879  ...               0           None  #984ea3
5      lognorm   3.807941      -151.611389  ...               0           None  #ff7f00
6        gamma   5.169312        -2.959167  ...               0           None  #ffff33
7   genextreme   7.176642         0.383002  ...               0           None  #a65628
8      uniform  41.303533             -1.0  ...               0           None  #f781bf
9       pareto  49.328099 -17471712.535891  ...               0           None  #999999
10       expon  49.393358             -1.0  ...               0           None  #999999


# Plot resultados
dist.plot()
plt.show()

#Figure_4-1_epdf_modelo04_in_sample_train.png

#######################
#   In Sample KPI     #
#                     #
#######################                                                                                                                                                            

MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel(), function_to_scale_data((y_train.detach().cpu().numpy()),-1,+1).ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)
RMSE: 0.035337

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel(),function_to_scale_data((y_train.detach().cpu().numpy()),-1,+1).ravel())
print('MAE: %f' % mae)
MAE:  0.032694

coefficient_of_dermination = r2_score(function_to_scale_data((y_train.detach().cpu().numpy()),-1,+1).ravel(), function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: 0.955660   #Alta peformace  para a amostragem de treino 

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(y_train.detach().cpu().numpy(),-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.113128   ,pvalue: 0.000207
# Nenhuma Conrespôndencia a 1% , 3% nem a 5% 

####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################	


def predictions_CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder(model,data ,regressors, extrapolations_leght):
     n_input = extrapolations_leght
     pred_list = []  
     batch = data	 
     pred_list.append(torch.cat(( model(batch)[-1].unsqueeze(1),  torch.FloatTensor(regressors.iloc[0,[1]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).unsqueeze(3).to(device) , torch.FloatTensor(regressors.iloc[0,[2]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).unsqueeze(3).to(device)),1))
     batch =  torch.cat((batch, pred_list[-1].permute(0,2,3,4,1)) ,0)
     for l in range(n_input-1):
      pred_list.append(torch.cat(( model(batch)[-1].unsqueeze(1),  torch.FloatTensor(regressors.iloc[l+1,[1]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).unsqueeze(3).to(device) , torch.FloatTensor(regressors.iloc[l+1,[2]].values).unsqueeze(0).unsqueeze(1).unsqueeze(2).unsqueeze(3).to(device)),1))
      batch =  torch.cat((batch, pred_list[-1].permute(0,2,3,4,1)) ,0)
     y_pred = np.array([pred_list[j].cpu().detach().numpy() for j in range(n_input)])[:,:, 0]
     y_pred = pd.DataFrame(y_pred.ravel())
     return  y_pred 
	 

y_hat = np.array([predictions_CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder(model, X_train ,pd.DataFrame(PETR3_SA_Close_and_its_own_volatility_and_volume_test), len(PETR3_SA_Close_and_its_own_volatility_and_volume_test)) for _ in range(7500)])
y_hat = y_hat.reshape(30,7500)
yhat = np.mean(y_hat, 1)

pd.DataFrame(yhat).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_forecasted_nonLinear4_test.csv', index=None)
PETR3_SA_Close_forecasted_nonLinear4_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear4_test.csv")

# Determinar a melhor distribuição de probabilidade adequada para os dados
dist.fit_transform(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel())

# Imprimir o sumário das distribuições elencadas 
print(dist.summary)

          name     score          loc  ... bootstrap_score bootstrap_pass    color
0     loggamma  0.349934   -92.731526  ...               0           None  #e41a1c
1   genextreme  0.353248    -0.255608  ...               0           None  #e41a1c
2            t  0.353786    -0.074383  ...               0           None  #377eb8
3         norm  0.353797    -0.074394  ...               0           None  #4daf4a
4        gamma  0.356655   -35.646754  ...               0           None  #984ea3
5      lognorm  0.360059   -25.062355  ...               0           None  #ff7f00
6     dweibull   0.38605    -0.080146  ...               0           None  #ffff33
7      uniform      0.44         -1.0  ...               0           None  #a65628
8         beta  0.474314         -1.0  ...               0           None  #f781bf
9        expon  0.730011         -1.0  ...               0           None  #999999
10      pareto   0.73892 -10116.29849  ...               0           None  #999999

# Plot resultados
dist.plot()
plt.show()

#Figure_4-1_modelo04_out_sample_test_pdf.png


####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################

MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
RMSE = math.sqrt(MSE)
print('RMSE: %f'%  RMSE)
RMSE: 0.666444

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
print('MAE: %f' % mae)
MAE: 0.562622

coefficient_of_dermination = r2_score(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: -1.296068 # Muito pobre peformace para a amostragem de teste

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.166667   ,pvalue: 0.807963
# Conrespôndencia a 1% ,3% e a 5% de Confiança

##########*/                   *\###########      
#
#   Intervalo de Credibilidade via bootstrap 
#
##########*/                   *\###########

IC valor-z
80   1.282
85   1.440
90   1.645
95   1.960
99   2.576

model.eval()
mean = function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel()
std = np.array([model(X_train).cpu().detach().numpy() for _ in range(3000)])
std = np.std(std, axis=0)
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1), label = "Amostra observada de treino")
plt.title(' log-retornos a d=  1.0 da PETR3_SA estimação da amostragem de treino ')
plt.plot(mean, label = "Amostra estimada de treino")
plt.plot(function_to_scale_data(upper_bands.ravel(),-1,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#modelo04_obs_vs_est.png

mean = function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel()
std = np.array([predictions_CasualCNN_Bidirectional_MomentumConvLSTM_AdittiveAttention_Encoder_Decoder(model.eval(), X_train ,pd.DataFrame(PETR3_SA_Close_and_its_own_volatility_and_volume_test), len(PETR3_SA_Close_and_its_own_volatility_and_volume_test)) for _ in range(7500)])
std = np.std(std, axis=0)
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1), label = "Amostra observada de teste")
plt.title(' log-retornos a d=  1.0 da PETR3_SA extrapolação da amostragem de teste ')
plt.plot(mean, label = "Amostra extrapolada de teste")
plt.plot(function_to_scale_data(upper_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#modelo04_obs_vs_est_test..png


sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra observada")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel(),bins=10,color='blue', label = "Amostra  estimada ")
plt.suptitle('Histograma para amostras estimadas  e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_4-1_modelo03_in_sample_train_Histogram.png

sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra de teste observada")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel(),bins=10,color='blue', label = "Amostra de teste estimada/extrapolada")
plt.suptitle('Histograma para amostras estimadas de teste e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_4-1_modelo03_out_sample_test_Histogram.png
#######/                               \#######
#                                             #
#    KPI de Risco.Binômio de risco vs retorno #
#                                             #
#######/                               \#######

mean_04_treino = np.mean(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel())
std_04_treino  = np.std(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel())
print('retorno_modelo04_treino: %f   ,risco_modelo04_treino: %f' % (mean_04_treino,std_04_treino))
#retorno_modelo04_treino: 0.430126   ,risco_modelo04_treino: 0.166140

mean_obs_treino = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
std_obs_treino  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
print('retorno_obs_treino: %f   ,risco_obs_treino: %f' % (mean_obs_treino,std_obs_treino))
#retorno_obs_treino: 0.409123   ,risco_obs_treino: 0.167715

mean_04_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel())
std_04_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel())
print('retorno_modelo04_extrapolado: %f   ,risco_modelo04_extrapolado: %f' % (mean_04_extrapolado,std_04_extrapolado))
#retorno_modelo04_extrapolado: -0.074394   ,risco_modelo04_extrapolado: 0.536220

mean_obs_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
std_obs_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
print('retorno_obs_extrapolado: %f   ,risco_obs_extrapolado: %f' % (mean_obs_extrapolado,std_obs_extrapolado))
#retorno_obs_extrapolado: -0.056525   ,risco_obs_extrapolado: 0.438676

# Draw quantile-quantile plot
plt.figure()
qqplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train.values,-1,1).ravel(),function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas')
plt.ylabel('QQplot realidade observada ')
plt.title('QQplots para as duas amostras ')
plt.show()

#Figure_4-1_modelo04_in_sample_train_QQplots.png

qqplot(function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test.values,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas de teste')
plt.ylabel('QQplot realidade observada de teste')
plt.title('QQplots para as duas amostras de teste')
plt.show()	

#Figure_4-1_modelo04_out_sample_teste_QQplots.png


######################################################################################################
# Para a amostragem de treino Distribuição Normal                                                    #
# Para a amostragem de teste  com caudas leves                                                       #
######################################################################################################

PETR3_SA_Close_forecasted_nonLinear2_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear2_train.csv")
PETR3_SA_Close_forecasted_nonLinear3_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear3_train.csv")
PETR3_SA_Close_forecasted_nonLinear4_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear4_train.csv")

PETR3_SA_Close_all_together_forecasted_train = np.concatenate((function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_train[:-1],-1,1),
function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_train,-1,1),function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_train,-1,1)), axis=1)

pd.DataFrame(PETR3_SA_Close_all_together_forecasted_train).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_all_together_forecasted_train.csv', index=None)
PETR3_SA_Close_all_together_forecasted_train = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_all_together_forecasted_train.csv")

PETR3_SA_Close_all_together_forecasted_train_mean= np.mean(PETR3_SA_Close_all_together_forecasted_train, axis=1)
PETR3_SA_Close_all_together_forecasted_train_std= np.std(PETR3_SA_Close_all_together_forecasted_train, axis=1)  


##########*/                   *\###########      
#
#   Intervalo de Credibilidade via bootstrap 
#
##########*/                   *\###########

IC valor-z
80   1.282
85   1.440
90   1.645
95   1.960
99   2.576

mean = PETR3_SA_Close_all_together_forecasted_train_mean
std = PETR3_SA_Close_all_together_forecasted_train_std
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1), label = "Amostra observada de treino")
plt.title(' log-retornos a d=  1.0 da PETR3_SA estimação da amostragem de treino e extrapolação para o período de treino ')
plt.plot(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1), label = "Amostra estimada para os três modelos juntos  de treino")
plt.plot(function_to_scale_data(upper_bands.ravel(),-1,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#modelo04_obs_vs_est.png
PETR3_SA_Close_forecasted_nonLinear2_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear2_test.csv")
PETR3_SA_Close_forecasted_nonLinear3_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear3_test.csv")
PETR3_SA_Close_forecasted_nonLinear4_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_forecasted_nonLinear4_test.csv")

PETR3_SA_Close_all_together_forecasted_test = np.concatenate((function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear2_test,-1,1),
function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear3_test,-1,1),function_to_scale_data(PETR3_SA_Close_forecasted_nonLinear4_test,-1,1)),axis=1)

pd.DataFrame(PETR3_SA_Close_all_together_forecasted_test).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3_SA_Close_all_together_forecasted_test.csv', index=None)
PETR3_SA_Close_all_together_forecasted_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_all_together_forecasted_test.csv")

PETR3_SA_Close_all_together_forecasted_test_mean= np.mean(PETR3_SA_Close_all_together_forecasted_test, axis=1)
PETR3_SA_Close_all_together_forecasted_test_std= np.std(PETR3_SA_Close_all_together_forecasted_test, axis=1) 


##########*/                   *\###########      
#
#   Intervalo de Credibilidade via bootstrap 
#
##########*/                   *\###########

IC valor-z
80   1.282
85   1.440
90   1.645
95   1.960
99   2.576

mean = PETR3_SA_Close_all_together_forecasted_test_mean
std = PETR3_SA_Close_all_together_forecasted_test_std
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

plt.plot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1), label = "Amostra observada de teste")
plt.title(' log-retornos a d=  1.0 da PETR3_SA estimação da amostragem de treino e extrapolação para o período de teste ')
plt.plot(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1), label = "Amostra estimada para os três modelos juntos  de teste")
plt.plot(function_to_scale_data(upper_bands.ravel(),-1,1) , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(function_to_scale_data(lower_bands.ravel(),-1,1)  , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#modelo04_obs_vs_est_test..png

mean_forecasted_train = np.mean(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel())
std_forecasted_train  = np.std(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel())
print('retorno_forecasted_train: %f   ,risco_forecasted_train: %f' % (mean_forecasted_train,std_forecasted_train))
#retorno_forecasted_train: 0.314371   ,risco_forecasted_train: 0.169517

mean_obs_treino = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
std_obs_treino  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1))
print('retorno_obs_treino: %f   ,risco_obs_treino: %f' % (mean_obs_treino,std_obs_treino))
#retorno_obs_treino: 0.409123   ,risco_obs_treino: 0.167715

mean_forecasted_test = np.mean(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1)).ravel()
std_forecasted_test  = np.std(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1)).ravel()
print('retorno_forecasted_test: %f   ,risco_forecasted_test: %f' % (mean_forecasted_test,std_forecasted_test))
#retorno_forecasted_test: -0.086063   ,risco_forecasted_test: 0.457189

mean_obs_extrapolado = np.mean(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
std_obs_extrapolado  = np.std(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1))
print('retorno_obs_extrapolado: %f   ,risco_obs_extrapolado: %f' % (mean_obs_extrapolado,std_obs_extrapolado))
#retorno_obs_extrapolado: -0.056525   ,risco_obs_extrapolado: 0.438676

Var_Estimado_95_treino = mean_forecasted_train - 1.960*std_forecasted_train
Var_Observado_95_treino = mean_obs_treino - 1.960*std_obs_treino

Var_Extrapolado_95_teste = mean_forecasted_test-1.960*std_forecasted_test
Var_Observado_95_teste = mean_obs_extrapolado-1.960*std_obs_extrapolado


print('var_obs_treino: %f   ,var_estimado_treino: %f ,var_obs_teste: %f, var_estimado_teste: %f ' % (Var_Observado_95_treino,Var_Estimado_95_treino,Var_Observado_95_teste,Var_Extrapolado_95_teste))
#var_obs_treino: 0.080402   ,var_estimado_treino: -0.017882 ,var_obs_teste: -0.915952, var_estimado_teste: -0.982154

sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra de treino observada")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel(),bins=10,color='blue', label = "Amostra  estimada de treino para todos os modelos juntos")
plt.suptitle('Histograma para amostras estimadas  e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_4-1_modelo03_in_sample_train_Histogram.png

sns.distplot(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(),bins=10,color='red',  label = "Amostra de teste observada")  
sns.distplot(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1).ravel(),bins=10,color='blue', label = "Amostra  estimada de teste para todos os modelos juntos")
plt.suptitle('Histograma para amostras estimadas de teste e sua respectiva  realidade observada ' ,fontsize=12, color='black')
plt.grid(True)
plt.legend(loc = "upper left")
plt.plot(0.0, 1.0)
plt.show()

#Figure_4-1_modelo03_out_sample_test_Histogram.png

#######################
#   In Sample KPI     #
#                     #
#######################                                                                                                                                                            

MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel(), function_to_scale_data((PETR3_SA_Close_fractional_difference_Linear_filtred_train[:-1].values),-1,+1).ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)
RMSE:  0.120586

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel(),function_to_scale_data((PETR3_SA_Close_fractional_difference_Linear_filtred_train[:-1].values),-1,+1).ravel())
print('MAE: %f' % mae)
MAE: 0.101421

coefficient_of_dermination = r2_score(function_to_scale_data((PETR3_SA_Close_fractional_difference_Linear_filtred_train[:-1].values),-1,+1).ravel(), function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: 0.483697   #Alta peformace  para a amostragem de treino 

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train[:-1].values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.297486   ,pvalue: 0.000000
#Nenhuma  Comrespôndencia a 1% , 3% e nem 5% de Confiança

####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################


MSE = mean_squared_error(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
RMSE = math.sqrt(MSE)
print('RMSE: %f'%  RMSE)
RMSE: 0.624640

mae = mean_absolute_error(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel())
print('MAE: %f' % mae)
MAE: 0.508310

coefficient_of_dermination = r2_score(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1).ravel())
print('R2: %f' % coefficient_of_dermination)
R2: -1.017051 # Muito pobre peformace para a amostragem de teste

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel() , function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1).ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.133333   ,pvalue: 0.957846
#Correnspondência a 1%,  3% e a 5% de Intervalos de Confiança

# Draw quantile-quantile plot
plt.figure()
qqplot(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_train_mean,-1,1).ravel(),function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_train[:-1].values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas')
plt.ylabel('QQplot realidade observada ')
plt.title('QQplots para as duas amostras ')
plt.show()

#Figure_4-1_modelo04_in_sample_train_QQplots.png

qqplot(function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1).ravel(), function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred_test.values,-1,1).ravel(), c='r', alpha=0.1, edgecolor='k')
plt.xlabel('QQplot amostras estimadas de teste')
plt.ylabel('QQplot realidade observada de teste')
plt.title('QQplots para as duas amostras de teste')
plt.show()

#Figure_4-1_modelo04_out_sample_teste_QQplots.png


######################################################################################################
# Para a amostragem de treino com caudas leves                                                       #
# Para a amostragem de teste  com caudas normal                                                      #
######################################################################################################

# Parte 3 Juntando e analisando os resultados 


##########*/                   *\###########      
#
#            Reintegrando a Série 
#                  para a amostragem de teste
#
##########*/                   *\###########

PETR3_SA = pd.read_csv(r"C:\Users\Rafael Xavier Botelho\Documents\Desktop\Trabalho-Github\PETR3.SA.csv", parse_dates=['Date'])
PETR3_SA_Close = PETR3_SA.loc[:,[ 'Close']]

PETR3_SA_Close_fractional_difference = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_fractional_difference.csv")
PETR3_SA_Close_fractional_difference_train = PETR3_SA_Close_fractional_difference[:-30]
PETR3_SA_Close_fractional_difference_test  = PETR3_SA_Close_fractional_difference[-30:]

PETR3_SA_Close_all_together_forecasted_test = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Desktop/Trabalho-Github/PETR3_SA_Close_all_together_forecasted_test.csv")
PETR3_SA_Close_all_together_forecasted_test_mean= np.mean(PETR3_SA_Close_all_together_forecasted_test, axis=1)
PETR3_SA_Close_all_together_forecasted_test_std= np.std(PETR3_SA_Close_all_together_forecasted_test, axis=1) 

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min


	
PETR3_SA_Close_test_estimated=PETR3_SA_Close[:-30].values[-1].ravel()*np.exp(np.cumsum(PETR3_SA_Close_fractional_difference_train.values[-1].ravel()*function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_mean,-1,1)))
PETR3_SA_Close_test_estimated_std = PETR3_SA_Close[:-30].values[-1].ravel()*np.exp(np.cumsum(PETR3_SA_Close_fractional_difference_train.values[-1].ravel()*function_to_scale_data(PETR3_SA_Close_all_together_forecasted_test_std,-1,1)))
PETR3_SA_Close_test_observed= PETR3_SA_Close[:-30].values[-1].ravel()*np.exp(np.cumsum(PETR3_SA_Close_fractional_difference_test))
#All_together_model_obs_vs_estimated_train..png
#All_together_model_obs_vs_estimated_test..png
#All_together_model_histograma_treino_estimado_vs_observado.png
#All_together_model_histograma_teste_estimado_vs_observado.png
#All_together_model_histograma_treino_estimado_vs_observado.png
#Figure_5-1_All_together_in_sample_train_QQplots.png
#igure_5-1_All_together_out_sample_teste_QQplots.png

####################################
#  Out of Sample                   #
#        Peformace preditiva KPI   #
#                                  #
####################################


MSE = mean_squared_error(PETR3_SA_Close_test_estimated.ravel(), PETR3_SA_Close_test_observed.values.ravel())
RMSE = math.sqrt(MSE)
print('RMSE: %f'%  RMSE)
RMSE: 1.368446 

mae = mean_absolute_error(PETR3_SA_Close_test_estimated.ravel(), PETR3_SA_Close_test_observed.values.ravel())
print('MAE: %f' % mae)
MAE: 1.155684

coefficient_of_dermination = r2_score( PETR3_SA_Close_test_observed.values.ravel(), PETR3_SA_Close_test_estimated.ravel())
print('R2: %f' % coefficient_of_dermination)
R2:  -1.384018  # Muito pobre peformace para a amostragem de teste

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(PETR3_SA_Close_test_observed.values.ravel(), PETR3_SA_Close_test_estimated.ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
statistic: 0.766667   ,pvalue: 0.000000
#Nenhuma Correnspondência a 1%,  3% nem a  5%  de Confiança

mean = PETR3_SA_Close_test_estimated.values.ravel()
std =PETR3_SA_Close_test_estimated_std.values.ravel()
Credibility_intervals = 1.960 * (std.ravel()/np.sqrt(len(mean)))
upper_bands = mean.ravel() + Credibility_intervals
lower_bands = mean.ravel() - Credibility_intervals

# Create some mock data
t = PETR3_SA[-30:].index
fig, ax1 = plt.subplots()
color = 'tab:red'
ax1.set_xlabel('datas (s)')
ax1.set_ylabel('Amostra estimada de teste  para os três modelos juntos', color=color)
ax1.plot(t, PETR3_SA_Close_test_estimated, color=color)
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()  # Colocando um segundo eixo juntamente com o mesmotammaho do eixo X 

color = 'tab:blue'
ax2.set_ylabel('Amostra observada de teste para os Preços de Fechamentos ', color=color)  
ax2.plot(t, PETR3_SA_Close_test_observed.values, color=color)
ax2.tick_params(axis='y', labelcolor=color)
fig.tight_layout() 
plt.show()

#Figure_5-1_Resultado_Final_estimado_vs_observado.png

plt.title('PETR3_SA estimação da amostragem de teste e extrapolação para o período de teste ')
plt.plot(PETR3_SA_Close_test_estimated, label = "Amostra estimada para os três modelos juntos  de teste")
plt.plot(upper_bands , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade superior a 95")
plt.plot(lower_bands , linestyle= '--', linewidth =0.7, label = "Intervalo de Credibilidade inferior a 95")
plt.legend(loc = "upper left")
plt.show()

#Figure_5-3_PETR3.SA_estimated_IC.png

Initial_Price = PETR3_SA_Close_test_estimated.values[0]
final_Price = PETR3_SA_Close_test_estimated.values[-1]
return_interest = (final_Price - Initial_Price)/Initial_Price
Initial_Price_obs = PETR3_SA_Close_test_observed.values[0]
final_Price_obs = PETR3_SA_Close_test_observed.values[-1]
return_interest_obs = (final_Price_obs - Initial_Price_obs)/Initial_Price_obs
Take_Profit = upper_bands[-1]	
Stop_Loss = lower_bands[-1]

print('retorno_extrapolção_teste: %f  ,retorno_teste_obs: %f , Preço_final_extrapolado_teste: %f , Preço_final_teste_obs: %f  , Stop_Loss: %f, Take_Profit: %f  ' % (return_interest,  return_interest_obs, final_Price ,final_Price_obs, Stop_Loss,Take_Profit))

retorno_extrapolção_teste: 0.008377  ,retorno_teste_obs: 0.037957 , Preço_final_extrapolado_teste: 35.552840 , 
Preço_final_teste_obs: 36.369999  , Stop_Loss: 22.587589, Take_Profit: 48.518091

for i in range(len(possible_d)):
    log_adf_stat_holder[i]=adfuller(ts_differencing_tau(np.log(pd.DataFrame(PETR3_SA_Close_test_estimated)) ,possible_d[i],tau))[1]

d=1.00
PETR3_SA_Close_fractional_difference_test_estimated  = (ts_differencing_tau(np.log(PETR3_SA_Close_test_estimated), d,tau )) 
PETR3_SA_Close_fractional_difference_test_estimated = PETR3_SA_Close_fractional_difference_test_estimated.fillna(method='ffill')  

Var_Estimado_95_teste = np.mean(PETR3_SA_Close_fractional_difference_test_estimated) - 1.960*np.std(PETR3_SA_Close_fractional_difference_test_estimated)
Var_Observado_95_teste = np.mean(PETR3_SA_Close_fractional_difference_test) - 1.960*np.std(PETR3_SA_Close_fractional_difference_test)

print('Var_Estimado_95_teste: %f  ,Var_Observado_95_teste:%f  ' % (Var_Estimado_95_teste, Var_Observado_95_teste))
Var_Estimado_95_teste: -0.001943  ,Var_Observado_95_teste:-0.035614

#######*/       *\#########
#                         #
#  Possível Hedge         #
#                         #
#######*/       *\#########


Amplitude_total = upper_bands[-1] - lower_bands[-1]
Amplitude_Long = (upper_bands[-1]-PETR3_SA_Close_test_estimated.values.ravel()[-1])/Amplitude_total
Amplitude_Short = (PETR3_SA_Close_test_estimated.values.ravel()[-1]-lower_bands[-1])/Amplitude_total

print('Peso_Long: %f  , Peso_Short:%f  ' % (Amplitude_Long, Amplitude_Short))
#Peso_Long: 0.500000  , Peso_Short:0.500000


#Parte 4 Conclusão sobre o assunto aboradado

###########################/* Considerações Finais *\ ########################################
#      Ou seja, a prórpia percentagem de hedge , metade para short e para long, vem confirmar as KPIs 
#      de erro que a estimativa para a amostragem de teste não tem nemhuma correspondência com a o
#      que realmente aconteceu, com a amostragem de teste observada.Confirmando aquilo que encomtramos 
#      na parte de analíse exploratória dos dados de que o sinal da série histórica da PETR3.SA é um ruído branco, 
#      imprevisível justamente porque sua natureza é aleatóriedade pura, caos.Hipotese Fraca de Mercado.
#      Concorrência Perfeitamente Elastíca, pelo menos para os modelos  aqui apresentados.
#      Concerteza existem sim modelos mais recentes que os clássicos apresentados aqui, LSTM(1997) ,CNN(1998),ARMA(1980),SV AR(1982),Garch(1982) e afins, tais como
#      Autoencoders Transformers, Bert e variantes , bem como o prório Chat GPT ,em suas versões mais recentes, para serem aboradados em pesquisas futuras.
#      Entretanto , a abordagem mais precisa é a passiva Long 'n Hold das principais Blue Chips infraestruturais do país ,
#      tais como Bancos, Energia ,Saneamento/Seguradoras ,Telecon ( metódo BEST do Luis Carlos Barsi)
###########################/ *                    *\ ########################################                
